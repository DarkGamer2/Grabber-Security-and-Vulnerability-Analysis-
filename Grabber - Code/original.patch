Binary files grabber-original/.DS_Store and grabber-new/.DS_Store differ
diff -rupN grabber-original/.gitattributes grabber-new/.gitattributes
--- grabber-original/.gitattributes	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.gitattributes	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1 @@
+* text=auto
diff -rupN grabber-original/.gitignore grabber-new/.gitignore
--- grabber-original/.gitignore	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.gitignore	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,3 @@
+results/*
+*.pyc
+*.log
diff -rupN grabber-original/.idea/.name grabber-new/.idea/.name
--- grabber-original/.idea/.name	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/.name	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1 @@
+grabber
\ No newline at end of file
diff -rupN grabber-original/.idea/encodings.xml grabber-new/.idea/encodings.xml
--- grabber-original/.idea/encodings.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/encodings.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,4 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="Encoding" useUTFGuessing="true" native2AsciiForPropertiesFiles="false" />
+</project>
\ No newline at end of file
diff -rupN grabber-original/.idea/grabber.iml grabber-new/.idea/grabber.iml
--- grabber-original/.idea/grabber.iml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/grabber.iml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="PYTHON_MODULE" version="4">
+  <component name="NewModuleRootManager">
+    <content url="file://$MODULE_DIR$" />
+    <orderEntry type="jdk" jdkName="Python 2.7.9 (/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7)" jdkType="Python SDK" />
+    <orderEntry type="sourceFolder" forTests="false" />
+  </component>
+</module>
\ No newline at end of file
diff -rupN grabber-original/.idea/misc.xml grabber-new/.idea/misc.xml
--- grabber-original/.idea/misc.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/misc.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,27 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="DaemonCodeAnalyzer">
+    <disable_hints />
+  </component>
+  <component name="DependencyValidationManager">
+    <option name="SKIP_IMPORT_STATEMENTS" value="false" />
+  </component>
+  <component name="Encoding" useUTFGuessing="true" native2AsciiForPropertiesFiles="false" />
+  <component name="ProjectLevelVcsManager" settingsEditedManually="false">
+    <OptionsSetting value="true" id="Add" />
+    <OptionsSetting value="true" id="Remove" />
+    <OptionsSetting value="true" id="Checkout" />
+    <OptionsSetting value="true" id="Update" />
+    <OptionsSetting value="true" id="Status" />
+    <OptionsSetting value="true" id="Edit" />
+    <ConfirmationsSetting value="0" id="Add" />
+    <ConfirmationsSetting value="0" id="Remove" />
+  </component>
+  <component name="ProjectModuleManager">
+    <modules />
+  </component>
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 2.7.9 (/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7)" project-jdk-type="Python SDK" />
+  <component name="RunManager">
+    <list size="0" />
+  </component>
+</project>
\ No newline at end of file
diff -rupN grabber-original/.idea/modules.xml grabber-new/.idea/modules.xml
--- grabber-original/.idea/modules.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/modules.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,8 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ProjectModuleManager">
+    <modules>
+      <module fileurl="file://$PROJECT_DIR$/.idea/grabber.iml" filepath="$PROJECT_DIR$/.idea/grabber.iml" />
+    </modules>
+  </component>
+</project>
\ No newline at end of file
diff -rupN grabber-original/.idea/scopes/scope_settings.xml grabber-new/.idea/scopes/scope_settings.xml
--- grabber-original/.idea/scopes/scope_settings.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/scopes/scope_settings.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,5 @@
+<component name="DependencyValidationManager">
+  <state>
+    <option name="SKIP_IMPORT_STATEMENTS" value="false" />
+  </state>
+</component>
\ No newline at end of file
diff -rupN grabber-original/.idea/vcs.xml grabber-new/.idea/vcs.xml
--- grabber-original/.idea/vcs.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/vcs.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,6 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="VcsDirectoryMappings">
+    <mapping directory="$PROJECT_DIR$" vcs="Git" />
+  </component>
+</project>
\ No newline at end of file
diff -rupN grabber-original/.idea/workspace.xml grabber-new/.idea/workspace.xml
--- grabber-original/.idea/workspace.xml	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/.idea/workspace.xml	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,488 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<project version="4">
+  <component name="ChangeListManager">
+    <list default="true" id="6e3024c6-b861-403b-bb15-75d4eebf611b" name="Default" comment="">
+      <change type="MODIFICATION" beforePath="$PROJECT_DIR$/cookies.lwp" afterPath="$PROJECT_DIR$/cookies.lwp" />
+      <change type="MODIFICATION" beforePath="$PROJECT_DIR$/grabber.conf.xml" afterPath="$PROJECT_DIR$/grabber.conf.xml" />
+      <change type="MODIFICATION" beforePath="$PROJECT_DIR$/spider.py" afterPath="$PROJECT_DIR$/spider.py" />
+    </list>
+    <ignored path="grabber.iws" />
+    <ignored path=".idea/workspace.xml" />
+    <option name="EXCLUDED_CONVERTED_TO_IGNORED" value="true" />
+    <option name="TRACKING_ENABLED" value="true" />
+    <option name="SHOW_DIALOG" value="false" />
+    <option name="HIGHLIGHT_CONFLICTS" value="true" />
+    <option name="HIGHLIGHT_NON_ACTIVE_CHANGELIST" value="false" />
+    <option name="LAST_RESOLUTION" value="IGNORE" />
+  </component>
+  <component name="ChangesViewManager" flattened_view="true" show_ignored="false" />
+  <component name="CreatePatchCommitExecutor">
+    <option name="PATCH_PATH" value="" />
+  </component>
+  <component name="DaemonCodeAnalyzer">
+    <disable_hints />
+  </component>
+  <component name="ExecutionTargetManager" SELECTED_TARGET="default_target" />
+  <component name="FavoritesManager">
+    <favorites_list name="grabber" />
+  </component>
+  <component name="FileEditorManager">
+    <leaf>
+      <file leaf-file-name="grabber.py" pinned="false" current-in-tab="true">
+        <entry file="file://$PROJECT_DIR$/grabber.py">
+          <provider selected="true" editor-type-id="text-editor">
+            <state vertical-scroll-proportion="0.0" vertical-offset="315" max-vertical-offset="8415">
+              <caret line="28" column="0" selection-start-line="28" selection-start-column="0" selection-end-line="28" selection-end-column="0" />
+              <folding />
+            </state>
+          </provider>
+        </entry>
+      </file>
+      <file leaf-file-name="spider.py" pinned="false" current-in-tab="false">
+        <entry file="file://$PROJECT_DIR$/spider.py">
+          <provider selected="true" editor-type-id="text-editor">
+            <state vertical-scroll-proportion="0.0" vertical-offset="1845" max-vertical-offset="9255">
+              <caret line="130" column="0" selection-start-line="130" selection-start-column="0" selection-end-line="130" selection-end-column="0" />
+              <folding />
+            </state>
+          </provider>
+        </entry>
+      </file>
+    </leaf>
+  </component>
+  <component name="Git.Settings">
+    <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
+  </component>
+  <component name="IdeDocumentHistory">
+    <option name="CHANGED_PATHS">
+      <list>
+        <option value="$PROJECT_DIR$/grabber.conf.xml" />
+      </list>
+    </option>
+  </component>
+  <component name="ProjectFrameBounds">
+    <option name="y" value="22" />
+    <option name="width" value="1280" />
+    <option name="height" value="774" />
+  </component>
+  <component name="ProjectLevelVcsManager" settingsEditedManually="false">
+    <OptionsSetting value="true" id="Add" />
+    <OptionsSetting value="true" id="Remove" />
+    <OptionsSetting value="true" id="Checkout" />
+    <OptionsSetting value="true" id="Update" />
+    <OptionsSetting value="true" id="Status" />
+    <OptionsSetting value="true" id="Edit" />
+    <ConfirmationsSetting value="0" id="Add" />
+    <ConfirmationsSetting value="0" id="Remove" />
+  </component>
+  <component name="ProjectView">
+    <navigator currentView="ProjectPane" proportions="" version="1">
+      <flattenPackages />
+      <showMembers />
+      <showModules />
+      <showLibraryContents />
+      <hideEmptyPackages />
+      <abbreviatePackageNames />
+      <autoscrollToSource />
+      <autoscrollFromSource />
+      <sortByType />
+    </navigator>
+    <panes>
+      <pane id="ProjectPane">
+        <subPane>
+          <PATH>
+            <PATH_ELEMENT>
+              <option name="myItemId" value="grabber" />
+              <option name="myItemType" value="com.intellij.ide.projectView.impl.nodes.ProjectViewProjectNode" />
+            </PATH_ELEMENT>
+          </PATH>
+          <PATH>
+            <PATH_ELEMENT>
+              <option name="myItemId" value="grabber" />
+              <option name="myItemType" value="com.intellij.ide.projectView.impl.nodes.ProjectViewProjectNode" />
+            </PATH_ELEMENT>
+            <PATH_ELEMENT>
+              <option name="myItemId" value="grabber" />
+              <option name="myItemType" value="com.intellij.ide.projectView.impl.nodes.PsiDirectoryNode" />
+            </PATH_ELEMENT>
+          </PATH>
+        </subPane>
+      </pane>
+      <pane id="Scope" />
+    </panes>
+  </component>
+  <component name="PropertiesComponent">
+    <property name="last_opened_file_path" value="$PROJECT_DIR$" />
+    <property name="recentsLimit" value="5" />
+    <property name="options.lastSelected" value="com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable" />
+    <property name="options.splitter.main.proportions" value="0.3" />
+    <property name="options.splitter.details.proportions" value="0.2" />
+    <property name="restartRequiresConfirmation" value="true" />
+    <property name="FullScreen" value="false" />
+  </component>
+  <component name="PyConsoleOptionsProvider">
+    <option name="myPythonConsoleState">
+      <console-settings custom-start-script="" module-name="grabber" sdk-home="/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7">
+        <option name="myCustomStartScript" value="" />
+        <option name="mySdkHome" value="/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7" />
+        <option name="myModuleName" value="grabber" />
+      </console-settings>
+    </option>
+  </component>
+  <component name="RunManager" selected="Python.grabber">
+    <configuration default="false" name="grabber" type="PythonConfigurationType" factoryName="Python" temporary="true">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/grabber.py" />
+      <option name="PARAMETERS" value="--url https://129.219.253.30:80/~level03/cgi-bin/login.php --spider 0 --sql --xss" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <RunnerSettings RunnerId="PyDebugRunner" />
+      <RunnerSettings RunnerId="PythonRunner" />
+      <ConfigurationWrapper RunnerId="PyDebugRunner" />
+      <ConfigurationWrapper RunnerId="PythonRunner" />
+      <method />
+    </configuration>
+    <configuration default="false" name="Unittests in grabber" type="tests" factoryName="Unittests" temporary="true" nameIsGenerated="true">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
+      <option name="IS_MODULE_SDK" value="true" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="$PROJECT_DIR$" />
+      <option name="TEST_TYPE" value="TEST_FOLDER" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <option name="PUREUNITTEST" value="true" />
+      <option name="PARAMS" value="" />
+      <option name="USE_PARAM" value="false" />
+      <RunnerSettings RunnerId="PyDebugRunner" />
+      <RunnerSettings RunnerId="PythonRunner" />
+      <ConfigurationWrapper RunnerId="PyDebugRunner" />
+      <ConfigurationWrapper RunnerId="PythonRunner" />
+      <method />
+    </configuration>
+    <configuration default="true" type="tests" factoryName="py.test">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="" />
+      <option name="TEST_TYPE" value="TEST_SCRIPT" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <option name="testToRun" value="" />
+      <option name="keywords" value="" />
+      <option name="params" value="" />
+      <option name="USE_PARAM" value="false" />
+      <option name="USE_KEYWORD" value="false" />
+      <method />
+    </configuration>
+    <configuration default="true" type="tests" factoryName="Nosetests">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="" />
+      <option name="TEST_TYPE" value="TEST_SCRIPT" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <option name="PARAMS" value="" />
+      <option name="USE_PARAM" value="false" />
+      <method />
+    </configuration>
+    <configuration default="true" type="PythonConfigurationType" factoryName="Python">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <method />
+    </configuration>
+    <configuration default="true" type="tests" factoryName="Unittests">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="" />
+      <option name="TEST_TYPE" value="TEST_SCRIPT" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <option name="PUREUNITTEST" value="true" />
+      <option name="PARAMS" value="" />
+      <option name="USE_PARAM" value="false" />
+      <method />
+    </configuration>
+    <configuration default="true" type="tests" factoryName="Doctests">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="" />
+      <option name="TEST_TYPE" value="TEST_SCRIPT" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <method />
+    </configuration>
+    <configuration default="true" type="tests" factoryName="Attests">
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs />
+      <option name="SDK_HOME" value="" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <module name="grabber" />
+      <option name="SCRIPT_NAME" value="" />
+      <option name="CLASS_NAME" value="" />
+      <option name="METHOD_NAME" value="" />
+      <option name="FOLDER_NAME" value="" />
+      <option name="TEST_TYPE" value="TEST_SCRIPT" />
+      <option name="PATTERN" value="" />
+      <option name="USE_PATTERN" value="false" />
+      <method />
+    </configuration>
+    <list size="2">
+      <item index="0" class="java.lang.String" itemvalue="Python.grabber" />
+      <item index="1" class="java.lang.String" itemvalue="Python tests.Unittests in grabber" />
+    </list>
+    <recent_temporary>
+      <list size="2">
+        <item index="0" class="java.lang.String" itemvalue="Python.grabber" />
+        <item index="1" class="java.lang.String" itemvalue="Python tests.Unittests in grabber" />
+      </list>
+    </recent_temporary>
+  </component>
+  <component name="ShelveChangesManager" show_recycled="false" />
+  <component name="SvnConfiguration">
+    <configuration />
+  </component>
+  <component name="TaskManager">
+    <task active="true" id="Default" summary="Default task">
+      <changelist id="6e3024c6-b861-403b-bb15-75d4eebf611b" name="Default" comment="" />
+      <created>1428800294292</created>
+      <option name="number" value="Default" />
+      <updated>1428800294292</updated>
+    </task>
+    <servers />
+  </component>
+  <component name="ToolWindowManager">
+    <frame x="0" y="22" width="1280" height="774" extended-state="6" />
+    <editor active="false" />
+    <layout>
+      <window_info id="Changes" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="false" content_ui="tabs" />
+      <window_info id="Terminal" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="false" content_ui="tabs" />
+      <window_info id="TODO" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="6" side_tool="false" content_ui="tabs" />
+      <window_info id="Structure" active="false" anchor="left" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.25" sideWeight="0.5" order="1" side_tool="false" content_ui="tabs" />
+      <window_info id="Application Servers" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="false" content_ui="tabs" />
+      <window_info id="Project" active="false" anchor="left" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="true" weight="0.24609375" sideWeight="0.5" order="0" side_tool="false" content_ui="combo" />
+      <window_info id="Python Console" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="false" content_ui="tabs" />
+      <window_info id="Favorites" active="false" anchor="left" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="2" side_tool="true" content_ui="tabs" />
+      <window_info id="Event Log" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="true" content_ui="tabs" />
+      <window_info id="Version Control" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="7" side_tool="false" content_ui="tabs" />
+      <window_info id="Cvs" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.25" sideWeight="0.5" order="4" side_tool="false" content_ui="tabs" />
+      <window_info id="Message" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="0" side_tool="false" content_ui="tabs" />
+      <window_info id="Ant Build" active="false" anchor="right" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.25" sideWeight="0.5" order="1" side_tool="false" content_ui="tabs" />
+      <window_info id="Find" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.33" sideWeight="0.5" order="1" side_tool="false" content_ui="tabs" />
+      <window_info id="Debug" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.4" sideWeight="0.5" order="3" side_tool="false" content_ui="tabs" />
+      <window_info id="Commander" active="false" anchor="right" auto_hide="false" internal_type="SLIDING" type="SLIDING" visible="false" weight="0.4" sideWeight="0.5" order="0" side_tool="false" content_ui="tabs" />
+      <window_info id="Hierarchy" active="false" anchor="right" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.25" sideWeight="0.5" order="2" side_tool="false" content_ui="combo" />
+      <window_info id="Inspection" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.4" sideWeight="0.5" order="5" side_tool="false" content_ui="tabs" />
+      <window_info id="Run" active="false" anchor="bottom" auto_hide="false" internal_type="DOCKED" type="DOCKED" visible="false" weight="0.32907802" sideWeight="0.5" order="2" side_tool="false" content_ui="tabs" />
+    </layout>
+  </component>
+  <component name="Vcs.Log.UiProperties">
+    <option name="RECENTLY_FILTERED_USER_GROUPS">
+      <collection />
+    </option>
+    <option name="RECENTLY_FILTERED_BRANCH_GROUPS">
+      <collection />
+    </option>
+  </component>
+  <component name="VcsContentAnnotationSettings">
+    <option name="myLimit" value="2678400000" />
+  </component>
+  <component name="VcsManagerConfiguration">
+    <option name="myTodoPanelSettings">
+      <TodoPanelSettings />
+    </option>
+  </component>
+  <component name="XDebuggerManager">
+    <breakpoint-manager>
+      <default-breakpoints>
+        <breakpoint type="python-exception">
+          <properties notifyOnTerminate="true" exception="BaseException">
+            <option name="notifyOnTerminate" value="true" />
+          </properties>
+        </breakpoint>
+      </default-breakpoints>
+      <option name="time" value="7" />
+    </breakpoint-manager>
+    <watches-manager />
+  </component>
+  <component name="editorHistoryManager">
+    <entry file="file://$PROJECT_DIR$/grabber.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="8415">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/spider.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="1845" max-vertical-offset="9255">
+          <caret line="130" column="0" selection-start-line="130" selection-start-column="0" selection-end-line="130" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/grabber.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="8415">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/spider.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="1065" max-vertical-offset="8880">
+          <caret line="128" column="0" selection-start-line="128" selection-start-column="0" selection-end-line="128" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/files.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="1815">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/javascript.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="2790">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/crystal.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.56962025" vertical-offset="2190" max-vertical-offset="7965">
+          <caret line="168" column="0" selection-start-line="168" selection-start-column="0" selection-end-line="168" selection-end-column="0" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/grabber.conf.xml">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.3323486" vertical-offset="0" max-vertical-offset="677">
+          <caret line="15" column="27" selection-start-line="15" selection-start-column="20" selection-end-line="15" selection-end-column="27" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/results/xss_GrabberAttacks.xml">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="435">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/results/sql_GrabberAttacks.xml">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="0" max-vertical-offset="395">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/sqlAttacks.xml">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="-0.3072378" vertical-offset="208" max-vertical-offset="885">
+          <caret line="0" column="0" selection-start-line="0" selection-start-column="0" selection-end-line="0" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/sql.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="1.7503693" vertical-offset="0" max-vertical-offset="1755">
+          <caret line="82" column="29" selection-start-line="82" selection-start-column="29" selection-end-line="82" selection-end-column="29" />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/spider.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="1095" max-vertical-offset="9270">
+          <caret line="130" column="0" selection-start-line="130" selection-start-column="0" selection-end-line="130" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+    <entry file="file://$PROJECT_DIR$/grabber.py">
+      <provider selected="true" editor-type-id="text-editor">
+        <state vertical-scroll-proportion="0.0" vertical-offset="315" max-vertical-offset="8415">
+          <caret line="28" column="0" selection-start-line="28" selection-start-column="0" selection-end-line="28" selection-end-column="0" />
+          <folding />
+        </state>
+      </provider>
+    </entry>
+  </component>
+</project>
\ No newline at end of file
diff -rupN grabber-original/BeautifulSoup/BeautifulSoup.py grabber-new/BeautifulSoup/BeautifulSoup.py
--- grabber-original/BeautifulSoup/BeautifulSoup.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoup/BeautifulSoup.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,2017 @@
+"""Beautiful Soup
+Elixir and Tonic
+"The Screen-Scraper's Friend"
+http://www.crummy.com/software/BeautifulSoup/
+
+Beautiful Soup parses a (possibly invalid) XML or HTML document into a
+tree representation. It provides methods and Pythonic idioms that make
+it easy to navigate, search, and modify the tree.
+
+A well-formed XML/HTML document yields a well-formed data
+structure. An ill-formed XML/HTML document yields a correspondingly
+ill-formed data structure. If your document is only locally
+well-formed, you can use this library to find and process the
+well-formed part of it.
+
+Beautiful Soup works with Python 2.2 and up. It has no external
+dependencies, but you'll have more success at converting data to UTF-8
+if you also install these three packages:
+
+* chardet, for auto-detecting character encodings
+  http://chardet.feedparser.org/
+* cjkcodecs and iconv_codec, which add more encodings to the ones supported
+  by stock Python.
+  http://cjkpython.i18n.org/
+
+Beautiful Soup defines classes for two main parsing strategies:
+
+ * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific
+   language that kind of looks like XML.
+
+ * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid
+   or invalid. This class has web browser-like heuristics for
+   obtaining a sensible parse tree in the face of common HTML errors.
+
+Beautiful Soup also defines a class (UnicodeDammit) for autodetecting
+the encoding of an HTML or XML document, and converting it to
+Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.
+
+For more than you ever wanted to know about Beautiful Soup, see the
+documentation:
+http://www.crummy.com/software/BeautifulSoup/documentation.html
+
+Here, have some legalese:
+
+Copyright (c) 2004-2010, Leonard Richardson
+
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+  * Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+
+  * Redistributions in binary form must reproduce the above
+    copyright notice, this list of conditions and the following
+    disclaimer in the documentation and/or other materials provided
+    with the distribution.
+
+  * Neither the name of the the Beautiful Soup Consortium and All
+    Night Kosher Bakery nor the names of its contributors may be
+    used to endorse or promote products derived from this software
+    without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.
+
+"""
+from __future__ import generators
+
+__author__ = "Leonard Richardson (leonardr@segfault.org)"
+__version__ = "3.2.1"
+__copyright__ = "Copyright (c) 2004-2012 Leonard Richardson"
+__license__ = "New-style BSD"
+
+from sgmllib import SGMLParser, SGMLParseError
+import codecs
+import markupbase
+import types
+import re
+import sgmllib
+try:
+  from htmlentitydefs import name2codepoint
+except ImportError:
+  name2codepoint = {}
+try:
+    set
+except NameError:
+    from sets import Set as set
+
+#These hacks make Beautiful Soup able to parse XML with namespaces
+sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')
+markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\s*').match
+
+DEFAULT_OUTPUT_ENCODING = "utf-8"
+
+def _match_css_class(str):
+    """Build a RE to match the given CSS class."""
+    return re.compile(r"(^|.*\s)%s($|\s)" % str)
+
+# First, the classes that represent markup elements.
+
+class PageElement(object):
+    """Contains the navigational information for some part of the page
+    (either a tag or a piece of text)"""
+
+    def _invert(h):
+        "Cheap function to invert a hash."
+        i = {}
+        for k,v in h.items():
+            i[v] = k
+        return i
+
+    XML_ENTITIES_TO_SPECIAL_CHARS = { "apos" : "'",
+                                      "quot" : '"',
+                                      "amp" : "&",
+                                      "lt" : "<",
+                                      "gt" : ">" }
+
+    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)
+
+    def setup(self, parent=None, previous=None):
+        """Sets up the initial relations between this element and
+        other elements."""
+        self.parent = parent
+        self.previous = previous
+        self.next = None
+        self.previousSibling = None
+        self.nextSibling = None
+        if self.parent and self.parent.contents:
+            self.previousSibling = self.parent.contents[-1]
+            self.previousSibling.nextSibling = self
+
+    def replaceWith(self, replaceWith):
+        oldParent = self.parent
+        myIndex = self.parent.index(self)
+        if hasattr(replaceWith, "parent")\
+                  and replaceWith.parent is self.parent:
+            # We're replacing this element with one of its siblings.
+            index = replaceWith.parent.index(replaceWith)
+            if index and index < myIndex:
+                # Furthermore, it comes before this element. That
+                # means that when we extract it, the index of this
+                # element will change.
+                myIndex = myIndex - 1
+        self.extract()
+        oldParent.insert(myIndex, replaceWith)
+
+    def replaceWithChildren(self):
+        myParent = self.parent
+        myIndex = self.parent.index(self)
+        self.extract()
+        reversedChildren = list(self.contents)
+        reversedChildren.reverse()
+        for child in reversedChildren:
+            myParent.insert(myIndex, child)
+
+    def extract(self):
+        """Destructively rips this element out of the tree."""
+        if self.parent:
+            try:
+                del self.parent.contents[self.parent.index(self)]
+            except ValueError:
+                pass
+
+        #Find the two elements that would be next to each other if
+        #this element (and any children) hadn't been parsed. Connect
+        #the two.
+        lastChild = self._lastRecursiveChild()
+        nextElement = lastChild.next
+
+        if self.previous:
+            self.previous.next = nextElement
+        if nextElement:
+            nextElement.previous = self.previous
+        self.previous = None
+        lastChild.next = None
+
+        self.parent = None
+        if self.previousSibling:
+            self.previousSibling.nextSibling = self.nextSibling
+        if self.nextSibling:
+            self.nextSibling.previousSibling = self.previousSibling
+        self.previousSibling = self.nextSibling = None
+        return self
+
+    def _lastRecursiveChild(self):
+        "Finds the last element beneath this object to be parsed."
+        lastChild = self
+        while hasattr(lastChild, 'contents') and lastChild.contents:
+            lastChild = lastChild.contents[-1]
+        return lastChild
+
+    def insert(self, position, newChild):
+        if isinstance(newChild, basestring) \
+            and not isinstance(newChild, NavigableString):
+            newChild = NavigableString(newChild)
+
+        position =  min(position, len(self.contents))
+        if hasattr(newChild, 'parent') and newChild.parent is not None:
+            # We're 'inserting' an element that's already one
+            # of this object's children.
+            if newChild.parent is self:
+                index = self.index(newChild)
+                if index > position:
+                    # Furthermore we're moving it further down the
+                    # list of this object's children. That means that
+                    # when we extract this element, our target index
+                    # will jump down one.
+                    position = position - 1
+            newChild.extract()
+
+        newChild.parent = self
+        previousChild = None
+        if position == 0:
+            newChild.previousSibling = None
+            newChild.previous = self
+        else:
+            previousChild = self.contents[position-1]
+            newChild.previousSibling = previousChild
+            newChild.previousSibling.nextSibling = newChild
+            newChild.previous = previousChild._lastRecursiveChild()
+        if newChild.previous:
+            newChild.previous.next = newChild
+
+        newChildsLastElement = newChild._lastRecursiveChild()
+
+        if position >= len(self.contents):
+            newChild.nextSibling = None
+
+            parent = self
+            parentsNextSibling = None
+            while not parentsNextSibling:
+                parentsNextSibling = parent.nextSibling
+                parent = parent.parent
+                if not parent: # This is the last element in the document.
+                    break
+            if parentsNextSibling:
+                newChildsLastElement.next = parentsNextSibling
+            else:
+                newChildsLastElement.next = None
+        else:
+            nextChild = self.contents[position]
+            newChild.nextSibling = nextChild
+            if newChild.nextSibling:
+                newChild.nextSibling.previousSibling = newChild
+            newChildsLastElement.next = nextChild
+
+        if newChildsLastElement.next:
+            newChildsLastElement.next.previous = newChildsLastElement
+        self.contents.insert(position, newChild)
+
+    def append(self, tag):
+        """Appends the given tag to the contents of this tag."""
+        self.insert(len(self.contents), tag)
+
+    def findNext(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the first item that matches the given criteria and
+        appears after this Tag in the document."""
+        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)
+
+    def findAllNext(self, name=None, attrs={}, text=None, limit=None,
+                    **kwargs):
+        """Returns all items that match the given criteria and appear
+        after this Tag in the document."""
+        return self._findAll(name, attrs, text, limit, self.nextGenerator,
+                             **kwargs)
+
+    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the closest sibling to this Tag that matches the
+        given criteria and appears after this Tag in the document."""
+        return self._findOne(self.findNextSiblings, name, attrs, text,
+                             **kwargs)
+
+    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,
+                         **kwargs):
+        """Returns the siblings of this Tag that match the given
+        criteria and appear after this Tag in the document."""
+        return self._findAll(name, attrs, text, limit,
+                             self.nextSiblingGenerator, **kwargs)
+    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x
+
+    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the first item that matches the given criteria and
+        appears before this Tag in the document."""
+        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)
+
+    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,
+                        **kwargs):
+        """Returns all items that match the given criteria and appear
+        before this Tag in the document."""
+        return self._findAll(name, attrs, text, limit, self.previousGenerator,
+                           **kwargs)
+    fetchPrevious = findAllPrevious # Compatibility with pre-3.x
+
+    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the closest sibling to this Tag that matches the
+        given criteria and appears before this Tag in the document."""
+        return self._findOne(self.findPreviousSiblings, name, attrs, text,
+                             **kwargs)
+
+    def findPreviousSiblings(self, name=None, attrs={}, text=None,
+                             limit=None, **kwargs):
+        """Returns the siblings of this Tag that match the given
+        criteria and appear before this Tag in the document."""
+        return self._findAll(name, attrs, text, limit,
+                             self.previousSiblingGenerator, **kwargs)
+    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x
+
+    def findParent(self, name=None, attrs={}, **kwargs):
+        """Returns the closest parent of this Tag that matches the given
+        criteria."""
+        # NOTE: We can't use _findOne because findParents takes a different
+        # set of arguments.
+        r = None
+        l = self.findParents(name, attrs, 1)
+        if l:
+            r = l[0]
+        return r
+
+    def findParents(self, name=None, attrs={}, limit=None, **kwargs):
+        """Returns the parents of this Tag that match the given
+        criteria."""
+
+        return self._findAll(name, attrs, None, limit, self.parentGenerator,
+                             **kwargs)
+    fetchParents = findParents # Compatibility with pre-3.x
+
+    #These methods do the real heavy lifting.
+
+    def _findOne(self, method, name, attrs, text, **kwargs):
+        r = None
+        l = method(name, attrs, text, 1, **kwargs)
+        if l:
+            r = l[0]
+        return r
+
+    def _findAll(self, name, attrs, text, limit, generator, **kwargs):
+        "Iterates over a generator looking for things that match."
+
+        if isinstance(name, SoupStrainer):
+            strainer = name
+        # (Possibly) special case some findAll*(...) searches
+        elif text is None and not limit and not attrs and not kwargs:
+            # findAll*(True)
+            if name is True:
+                return [element for element in generator()
+                        if isinstance(element, Tag)]
+            # findAll*('tag-name')
+            elif isinstance(name, basestring):
+                return [element for element in generator()
+                        if isinstance(element, Tag) and
+                        element.name == name]
+            else:
+                strainer = SoupStrainer(name, attrs, text, **kwargs)
+        # Build a SoupStrainer
+        else:
+            strainer = SoupStrainer(name, attrs, text, **kwargs)
+        results = ResultSet(strainer)
+        g = generator()
+        while True:
+            try:
+                i = g.next()
+            except StopIteration:
+                break
+            if i:
+                found = strainer.search(i)
+                if found:
+                    results.append(found)
+                    if limit and len(results) >= limit:
+                        break
+        return results
+
+    #These Generators can be used to navigate starting from both
+    #NavigableStrings and Tags.
+    def nextGenerator(self):
+        i = self
+        while i is not None:
+            i = i.next
+            yield i
+
+    def nextSiblingGenerator(self):
+        i = self
+        while i is not None:
+            i = i.nextSibling
+            yield i
+
+    def previousGenerator(self):
+        i = self
+        while i is not None:
+            i = i.previous
+            yield i
+
+    def previousSiblingGenerator(self):
+        i = self
+        while i is not None:
+            i = i.previousSibling
+            yield i
+
+    def parentGenerator(self):
+        i = self
+        while i is not None:
+            i = i.parent
+            yield i
+
+    # Utility methods
+    def substituteEncoding(self, str, encoding=None):
+        encoding = encoding or "utf-8"
+        return str.replace("%SOUP-ENCODING%", encoding)
+
+    def toEncoding(self, s, encoding=None):
+        """Encodes an object to a string in some encoding, or to Unicode.
+        ."""
+        if isinstance(s, unicode):
+            if encoding:
+                s = s.encode(encoding)
+        elif isinstance(s, str):
+            if encoding:
+                s = s.encode(encoding)
+            else:
+                s = unicode(s)
+        else:
+            if encoding:
+                s  = self.toEncoding(str(s), encoding)
+            else:
+                s = unicode(s)
+        return s
+
+    BARE_AMPERSAND_OR_BRACKET = re.compile("([<>]|"
+                                           + "&(?!#\d+;|#x[0-9a-fA-F]+;|\w+;)"
+                                           + ")")
+
+    def _sub_entity(self, x):
+        """Used with a regular expression to substitute the
+        appropriate XML entity for an XML special character."""
+        return "&" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + ";"
+
+
+class NavigableString(unicode, PageElement):
+
+    def __new__(cls, value):
+        """Create a new NavigableString.
+
+        When unpickling a NavigableString, this method is called with
+        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be
+        passed in to the superclass's __new__ or the superclass won't know
+        how to handle non-ASCII characters.
+        """
+        if isinstance(value, unicode):
+            return unicode.__new__(cls, value)
+        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)
+
+    def __getnewargs__(self):
+        return (NavigableString.__str__(self),)
+
+    def __getattr__(self, attr):
+        """text.string gives you text. This is for backwards
+        compatibility for Navigable*String, but for CData* it lets you
+        get the string without the CData wrapper."""
+        if attr == 'string':
+            return self
+        else:
+            raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__.__name__, attr)
+
+    def __unicode__(self):
+        return str(self).decode(DEFAULT_OUTPUT_ENCODING)
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        # Substitute outgoing XML entities.
+        data = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, self)
+        if encoding:
+            return data.encode(encoding)
+        else:
+            return data
+
+class CData(NavigableString):
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<![CDATA[%s]]>" % NavigableString.__str__(self, encoding)
+
+class ProcessingInstruction(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        output = self
+        if "%SOUP-ENCODING%" in output:
+            output = self.substituteEncoding(output, encoding)
+        return "<?%s?>" % self.toEncoding(output, encoding)
+
+class Comment(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<!--%s-->" % NavigableString.__str__(self, encoding)
+
+class Declaration(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<!%s>" % NavigableString.__str__(self, encoding)
+
+class Tag(PageElement):
+
+    """Represents a found HTML tag with its attributes and contents."""
+
+    def _convertEntities(self, match):
+        """Used in a call to re.sub to replace HTML, XML, and numeric
+        entities with the appropriate Unicode characters. If HTML
+        entities are being converted, any unrecognized entities are
+        escaped."""
+        x = match.group(1)
+        if self.convertHTMLEntities and x in name2codepoint:
+            return unichr(name2codepoint[x])
+        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:
+            if self.convertXMLEntities:
+                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]
+            else:
+                return u'&%s;' % x
+        elif len(x) > 0 and x[0] == '#':
+            # Handle numeric entities
+            if len(x) > 1 and x[1] == 'x':
+                return unichr(int(x[2:], 16))
+            else:
+                return unichr(int(x[1:]))
+
+        elif self.escapeUnrecognizedEntities:
+            return u'&amp;%s;' % x
+        else:
+            return u'&%s;' % x
+
+    def __init__(self, parser, name, attrs=None, parent=None,
+                 previous=None):
+        "Basic constructor."
+
+        # We don't actually store the parser object: that lets extracted
+        # chunks be garbage-collected
+        self.parserClass = parser.__class__
+        self.isSelfClosing = parser.isSelfClosingTag(name)
+        self.name = name
+        if attrs is None:
+            attrs = []
+        elif isinstance(attrs, dict):
+            attrs = attrs.items()
+        self.attrs = attrs
+        self.contents = []
+        self.setup(parent, previous)
+        self.hidden = False
+        self.containsSubstitutions = False
+        self.convertHTMLEntities = parser.convertHTMLEntities
+        self.convertXMLEntities = parser.convertXMLEntities
+        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities
+
+        # Convert any HTML, XML, or numeric entities in the attribute values.
+        convert = lambda(k, val): (k,
+                                   re.sub("&(#\d+|#x[0-9a-fA-F]+|\w+);",
+                                          self._convertEntities,
+                                          val))
+        self.attrs = map(convert, self.attrs)
+
+    def getString(self):
+        if (len(self.contents) == 1
+            and isinstance(self.contents[0], NavigableString)):
+            return self.contents[0]
+
+    def setString(self, string):
+        """Replace the contents of the tag with a string"""
+        self.clear()
+        self.append(string)
+
+    string = property(getString, setString)
+
+    def getText(self, separator=u""):
+        if not len(self.contents):
+            return u""
+        stopNode = self._lastRecursiveChild().next
+        strings = []
+        current = self.contents[0]
+        while current is not stopNode:
+            if isinstance(current, NavigableString):
+                strings.append(current.strip())
+            current = current.next
+        return separator.join(strings)
+
+    text = property(getText)
+
+    def get(self, key, default=None):
+        """Returns the value of the 'key' attribute for the tag, or
+        the value given for 'default' if it doesn't have that
+        attribute."""
+        return self._getAttrMap().get(key, default)
+
+    def clear(self):
+        """Extract all children."""
+        for child in self.contents[:]:
+            child.extract()
+
+    def index(self, element):
+        for i, child in enumerate(self.contents):
+            if child is element:
+                return i
+        raise ValueError("Tag.index: element not in tag")
+
+    def has_key(self, key):
+        return self._getAttrMap().has_key(key)
+
+    def __getitem__(self, key):
+        """tag[key] returns the value of the 'key' attribute for the tag,
+        and throws an exception if it's not there."""
+        return self._getAttrMap()[key]
+
+    def __iter__(self):
+        "Iterating over a tag iterates over its contents."
+        return iter(self.contents)
+
+    def __len__(self):
+        "The length of a tag is the length of its list of contents."
+        return len(self.contents)
+
+    def __contains__(self, x):
+        return x in self.contents
+
+    def __nonzero__(self):
+        "A tag is non-None even if it has no contents."
+        return True
+
+    def __setitem__(self, key, value):
+        """Setting tag[key] sets the value of the 'key' attribute for the
+        tag."""
+        self._getAttrMap()
+        self.attrMap[key] = value
+        found = False
+        for i in range(0, len(self.attrs)):
+            if self.attrs[i][0] == key:
+                self.attrs[i] = (key, value)
+                found = True
+        if not found:
+            self.attrs.append((key, value))
+        self._getAttrMap()[key] = value
+
+    def __delitem__(self, key):
+        "Deleting tag[key] deletes all 'key' attributes for the tag."
+        for item in self.attrs:
+            if item[0] == key:
+                self.attrs.remove(item)
+                #We don't break because bad HTML can define the same
+                #attribute multiple times.
+            self._getAttrMap()
+            if self.attrMap.has_key(key):
+                del self.attrMap[key]
+
+    def __call__(self, *args, **kwargs):
+        """Calling a tag like a function is the same as calling its
+        findAll() method. Eg. tag('a') returns a list of all the A tags
+        found within this tag."""
+        return apply(self.findAll, args, kwargs)
+
+    def __getattr__(self, tag):
+        #print "Getattr %s.%s" % (self.__class__, tag)
+        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:
+            return self.find(tag[:-3])
+        elif tag.find('__') != 0:
+            return self.find(tag)
+        raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__, tag)
+
+    def __eq__(self, other):
+        """Returns true iff this tag has the same name, the same attributes,
+        and the same contents (recursively) as the given tag.
+
+        NOTE: right now this will return false if two tags have the
+        same attributes in a different order. Should this be fixed?"""
+        if other is self:
+            return True
+        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):
+            return False
+        for i in range(0, len(self.contents)):
+            if self.contents[i] != other.contents[i]:
+                return False
+        return True
+
+    def __ne__(self, other):
+        """Returns true iff this tag is not identical to the other tag,
+        as defined in __eq__."""
+        return not self == other
+
+    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        """Renders this tag as a string."""
+        return self.__str__(encoding)
+
+    def __unicode__(self):
+        return self.__str__(None)
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,
+                prettyPrint=False, indentLevel=0):
+        """Returns a string or Unicode representation of this tag and
+        its contents. To get Unicode, pass None for encoding.
+
+        NOTE: since Python's HTML parser consumes whitespace, this
+        method is not certain to reproduce the whitespace present in
+        the original string."""
+
+        encodedName = self.toEncoding(self.name, encoding)
+
+        attrs = []
+        if self.attrs:
+            for key, val in self.attrs:
+                fmt = '%s="%s"'
+                if isinstance(val, basestring):
+                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:
+                        val = self.substituteEncoding(val, encoding)
+
+                    # The attribute value either:
+                    #
+                    # * Contains no embedded double quotes or single quotes.
+                    #   No problem: we enclose it in double quotes.
+                    # * Contains embedded single quotes. No problem:
+                    #   double quotes work here too.
+                    # * Contains embedded double quotes. No problem:
+                    #   we enclose it in single quotes.
+                    # * Embeds both single _and_ double quotes. This
+                    #   can't happen naturally, but it can happen if
+                    #   you modify an attribute value after parsing
+                    #   the document. Now we have a bit of a
+                    #   problem. We solve it by enclosing the
+                    #   attribute in single quotes, and escaping any
+                    #   embedded single quotes to XML entities.
+                    if '"' in val:
+                        fmt = "%s='%s'"
+                        if "'" in val:
+                            # TODO: replace with apos when
+                            # appropriate.
+                            val = val.replace("'", "&squot;")
+
+                    # Now we're okay w/r/t quotes. But the attribute
+                    # value might also contain angle brackets, or
+                    # ampersands that aren't part of entities. We need
+                    # to escape those to XML entities too.
+                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)
+
+                attrs.append(fmt % (self.toEncoding(key, encoding),
+                                    self.toEncoding(val, encoding)))
+        close = ''
+        closeTag = ''
+        if self.isSelfClosing:
+            close = ' /'
+        else:
+            closeTag = '</%s>' % encodedName
+
+        indentTag, indentContents = 0, 0
+        if prettyPrint:
+            indentTag = indentLevel
+            space = (' ' * (indentTag-1))
+            indentContents = indentTag + 1
+        contents = self.renderContents(encoding, prettyPrint, indentContents)
+        if self.hidden:
+            s = contents
+        else:
+            s = []
+            attributeString = ''
+            if attrs:
+                attributeString = ' ' + ' '.join(attrs)
+            if prettyPrint:
+                s.append(space)
+            s.append('<%s%s%s>' % (encodedName, attributeString, close))
+            if prettyPrint:
+                s.append("\n")
+            s.append(contents)
+            if prettyPrint and contents and contents[-1] != "\n":
+                s.append("\n")
+            if prettyPrint and closeTag:
+                s.append(space)
+            s.append(closeTag)
+            if prettyPrint and closeTag and self.nextSibling:
+                s.append("\n")
+            s = ''.join(s)
+        return s
+
+    def decompose(self):
+        """Recursively destroys the contents of this tree."""
+        self.extract()
+        if len(self.contents) == 0:
+            return
+        current = self.contents[0]
+        while current is not None:
+            next = current.next
+            if isinstance(current, Tag):
+                del current.contents[:]
+            current.parent = None
+            current.previous = None
+            current.previousSibling = None
+            current.next = None
+            current.nextSibling = None
+            current = next
+
+    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return self.__str__(encoding, True)
+
+    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,
+                       prettyPrint=False, indentLevel=0):
+        """Renders the contents of this tag as a string in the given
+        encoding. If encoding is None, returns a Unicode string.."""
+        s=[]
+        for c in self:
+            text = None
+            if isinstance(c, NavigableString):
+                text = c.__str__(encoding)
+            elif isinstance(c, Tag):
+                s.append(c.__str__(encoding, prettyPrint, indentLevel))
+            if text and prettyPrint:
+                text = text.strip()
+            if text:
+                if prettyPrint:
+                    s.append(" " * (indentLevel-1))
+                s.append(text)
+                if prettyPrint:
+                    s.append("\n")
+        return ''.join(s)
+
+    #Soup methods
+
+    def find(self, name=None, attrs={}, recursive=True, text=None,
+             **kwargs):
+        """Return only the first child of this Tag matching the given
+        criteria."""
+        r = None
+        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
+        if l:
+            r = l[0]
+        return r
+    findChild = find
+
+    def findAll(self, name=None, attrs={}, recursive=True, text=None,
+                limit=None, **kwargs):
+        """Extracts a list of Tag objects that match the given
+        criteria.  You can specify the name of the Tag and any
+        attributes you want the Tag to have.
+
+        The value of a key-value pair in the 'attrs' map can be a
+        string, a list of strings, a regular expression object, or a
+        callable that takes a string and returns whether or not the
+        string matches for some custom definition of 'matches'. The
+        same is true of the tag name."""
+        generator = self.recursiveChildGenerator
+        if not recursive:
+            generator = self.childGenerator
+        return self._findAll(name, attrs, text, limit, generator, **kwargs)
+    findChildren = findAll
+
+    # Pre-3.x compatibility methods
+    first = find
+    fetch = findAll
+
+    def fetchText(self, text=None, recursive=True, limit=None):
+        return self.findAll(text=text, recursive=recursive, limit=limit)
+
+    def firstText(self, text=None, recursive=True):
+        return self.find(text=text, recursive=recursive)
+
+    #Private methods
+
+    def _getAttrMap(self):
+        """Initializes a map representation of this tag's attributes,
+        if not already initialized."""
+        if not getattr(self, 'attrMap'):
+            self.attrMap = {}
+            for (key, value) in self.attrs:
+                self.attrMap[key] = value
+        return self.attrMap
+
+    #Generator methods
+    def childGenerator(self):
+        # Just use the iterator from the contents
+        return iter(self.contents)
+
+    def recursiveChildGenerator(self):
+        if not len(self.contents):
+            raise StopIteration
+        stopNode = self._lastRecursiveChild().next
+        current = self.contents[0]
+        while current is not stopNode:
+            yield current
+            current = current.next
+
+
+# Next, a couple classes to represent queries and their results.
+class SoupStrainer:
+    """Encapsulates a number of ways of matching a markup element (tag or
+    text)."""
+
+    def __init__(self, name=None, attrs={}, text=None, **kwargs):
+        self.name = name
+        if isinstance(attrs, basestring):
+            kwargs['class'] = _match_css_class(attrs)
+            attrs = None
+        if kwargs:
+            if attrs:
+                attrs = attrs.copy()
+                attrs.update(kwargs)
+            else:
+                attrs = kwargs
+        self.attrs = attrs
+        self.text = text
+
+    def __str__(self):
+        if self.text:
+            return self.text
+        else:
+            return "%s|%s" % (self.name, self.attrs)
+
+    def searchTag(self, markupName=None, markupAttrs={}):
+        found = None
+        markup = None
+        if isinstance(markupName, Tag):
+            markup = markupName
+            markupAttrs = markup
+        callFunctionWithTagData = callable(self.name) \
+                                and not isinstance(markupName, Tag)
+
+        if (not self.name) \
+               or callFunctionWithTagData \
+               or (markup and self._matches(markup, self.name)) \
+               or (not markup and self._matches(markupName, self.name)):
+            if callFunctionWithTagData:
+                match = self.name(markupName, markupAttrs)
+            else:
+                match = True
+                markupAttrMap = None
+                for attr, matchAgainst in self.attrs.items():
+                    if not markupAttrMap:
+                         if hasattr(markupAttrs, 'get'):
+                            markupAttrMap = markupAttrs
+                         else:
+                            markupAttrMap = {}
+                            for k,v in markupAttrs:
+                                markupAttrMap[k] = v
+                    attrValue = markupAttrMap.get(attr)
+                    if not self._matches(attrValue, matchAgainst):
+                        match = False
+                        break
+            if match:
+                if markup:
+                    found = markup
+                else:
+                    found = markupName
+        return found
+
+    def search(self, markup):
+        #print 'looking for %s in %s' % (self, markup)
+        found = None
+        # If given a list of items, scan it for a text element that
+        # matches.
+        if hasattr(markup, "__iter__") \
+                and not isinstance(markup, Tag):
+            for element in markup:
+                if isinstance(element, NavigableString) \
+                       and self.search(element):
+                    found = element
+                    break
+        # If it's a Tag, make sure its name or attributes match.
+        # Don't bother with Tags if we're searching for text.
+        elif isinstance(markup, Tag):
+            if not self.text:
+                found = self.searchTag(markup)
+        # If it's text, make sure the text matches.
+        elif isinstance(markup, NavigableString) or \
+                 isinstance(markup, basestring):
+            if self._matches(markup, self.text):
+                found = markup
+        else:
+            raise Exception, "I don't know how to match against a %s" \
+                  % markup.__class__
+        return found
+
+    def _matches(self, markup, matchAgainst):
+        #print "Matching %s against %s" % (markup, matchAgainst)
+        result = False
+        if matchAgainst is True:
+            result = markup is not None
+        elif callable(matchAgainst):
+            result = matchAgainst(markup)
+        else:
+            #Custom match methods take the tag as an argument, but all
+            #other ways of matching match the tag name as a string.
+            if isinstance(markup, Tag):
+                markup = markup.name
+            if markup and not isinstance(markup, basestring):
+                markup = unicode(markup)
+            #Now we know that chunk is either a string, or None.
+            if hasattr(matchAgainst, 'match'):
+                # It's a regexp object.
+                result = markup and matchAgainst.search(markup)
+            elif hasattr(matchAgainst, '__iter__'): # list-like
+                result = markup in matchAgainst
+            elif hasattr(matchAgainst, 'items'):
+                result = markup.has_key(matchAgainst)
+            elif matchAgainst and isinstance(markup, basestring):
+                if isinstance(markup, unicode):
+                    matchAgainst = unicode(matchAgainst)
+                else:
+                    matchAgainst = str(matchAgainst)
+
+            if not result:
+                result = matchAgainst == markup
+        return result
+
+class ResultSet(list):
+    """A ResultSet is just a list that keeps track of the SoupStrainer
+    that created it."""
+    def __init__(self, source):
+        list.__init__([])
+        self.source = source
+
+# Now, some helper functions.
+
+def buildTagMap(default, *args):
+    """Turns a list of maps, lists, or scalars into a single map.
+    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and
+    NESTING_RESET_TAGS maps out of lists and partial maps."""
+    built = {}
+    for portion in args:
+        if hasattr(portion, 'items'):
+            #It's a map. Merge it.
+            for k,v in portion.items():
+                built[k] = v
+        elif hasattr(portion, '__iter__'): # is a list
+            #It's a list. Map each item to the default.
+            for k in portion:
+                built[k] = default
+        else:
+            #It's a scalar. Map it to the default.
+            built[portion] = default
+    return built
+
+# Now, the parser classes.
+
+class BeautifulStoneSoup(Tag, SGMLParser):
+
+    """This class contains the basic parser and search code. It defines
+    a parser that knows nothing about tag behavior except for the
+    following:
+
+      You can't close a tag without closing all the tags it encloses.
+      That is, "<foo><bar></foo>" actually means
+      "<foo><bar></bar></foo>".
+
+    [Another possible explanation is "<foo><bar /></foo>", but since
+    this class defines no SELF_CLOSING_TAGS, it will never use that
+    explanation.]
+
+    This class is useful for parsing XML or made-up markup languages,
+    or when BeautifulSoup makes an assumption counter to what you were
+    expecting."""
+
+    SELF_CLOSING_TAGS = {}
+    NESTABLE_TAGS = {}
+    RESET_NESTING_TAGS = {}
+    QUOTE_TAGS = {}
+    PRESERVE_WHITESPACE_TAGS = []
+
+    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),
+                       lambda x: x.group(1) + ' />'),
+                      (re.compile('<!\s+([^<>]*)>'),
+                       lambda x: '<!' + x.group(1) + '>')
+                      ]
+
+    ROOT_TAG_NAME = u'[document]'
+
+    HTML_ENTITIES = "html"
+    XML_ENTITIES = "xml"
+    XHTML_ENTITIES = "xhtml"
+    # TODO: This only exists for backwards-compatibility
+    ALL_ENTITIES = XHTML_ENTITIES
+
+    # Used when determining whether a text node is all whitespace and
+    # can be replaced with a single space. A text node that contains
+    # fancy Unicode spaces (usually non-breaking) should be left
+    # alone.
+    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }
+
+    def __init__(self, markup="", parseOnlyThese=None, fromEncoding=None,
+                 markupMassage=True, smartQuotesTo=XML_ENTITIES,
+                 convertEntities=None, selfClosingTags=None, isHTML=False):
+        """The Soup object is initialized as the 'root tag', and the
+        provided markup (which can be a string or a file-like object)
+        is fed into the underlying parser.
+
+        sgmllib will process most bad HTML, and the BeautifulSoup
+        class has some tricks for dealing with some HTML that kills
+        sgmllib, but Beautiful Soup can nonetheless choke or lose data
+        if your data uses self-closing tags or declarations
+        incorrectly.
+
+        By default, Beautiful Soup uses regexes to sanitize input,
+        avoiding the vast majority of these problems. If the problems
+        don't apply to you, pass in False for markupMassage, and
+        you'll get better performance.
+
+        The default parser massage techniques fix the two most common
+        instances of invalid HTML that choke sgmllib:
+
+         <br/> (No space between name of closing tag and tag close)
+         <! --Comment--> (Extraneous whitespace in declaration)
+
+        You can pass in a custom list of (RE object, replace method)
+        tuples to get Beautiful Soup to scrub your input the way you
+        want."""
+
+        self.parseOnlyThese = parseOnlyThese
+        self.fromEncoding = fromEncoding
+        self.smartQuotesTo = smartQuotesTo
+        self.convertEntities = convertEntities
+        # Set the rules for how we'll deal with the entities we
+        # encounter
+        if self.convertEntities:
+            # It doesn't make sense to convert encoded characters to
+            # entities even while you're converting entities to Unicode.
+            # Just convert it all to Unicode.
+            self.smartQuotesTo = None
+            if convertEntities == self.HTML_ENTITIES:
+                self.convertXMLEntities = False
+                self.convertHTMLEntities = True
+                self.escapeUnrecognizedEntities = True
+            elif convertEntities == self.XHTML_ENTITIES:
+                self.convertXMLEntities = True
+                self.convertHTMLEntities = True
+                self.escapeUnrecognizedEntities = False
+            elif convertEntities == self.XML_ENTITIES:
+                self.convertXMLEntities = True
+                self.convertHTMLEntities = False
+                self.escapeUnrecognizedEntities = False
+        else:
+            self.convertXMLEntities = False
+            self.convertHTMLEntities = False
+            self.escapeUnrecognizedEntities = False
+
+        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)
+        SGMLParser.__init__(self)
+
+        if hasattr(markup, 'read'):        # It's a file-type object.
+            markup = markup.read()
+        self.markup = markup
+        self.markupMassage = markupMassage
+        try:
+            self._feed(isHTML=isHTML)
+        except StopParsing:
+            pass
+        self.markup = None                 # The markup can now be GCed
+
+    def convert_charref(self, name):
+        """This method fixes a bug in Python's SGMLParser."""
+        try:
+            n = int(name)
+        except ValueError:
+            return
+        if not 0 <= n <= 127 : # ASCII ends at 127, not 255
+            return
+        return self.convert_codepoint(n)
+
+    def _feed(self, inDocumentEncoding=None, isHTML=False):
+        # Convert the document to Unicode.
+        markup = self.markup
+        if isinstance(markup, unicode):
+            if not hasattr(self, 'originalEncoding'):
+                self.originalEncoding = None
+        else:
+            dammit = UnicodeDammit\
+                     (markup, [self.fromEncoding, inDocumentEncoding],
+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)
+            markup = dammit.unicode
+            self.originalEncoding = dammit.originalEncoding
+            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding
+        if markup:
+            if self.markupMassage:
+                if not hasattr(self.markupMassage, "__iter__"):
+                    self.markupMassage = self.MARKUP_MASSAGE
+                for fix, m in self.markupMassage:
+                    markup = fix.sub(m, markup)
+                # TODO: We get rid of markupMassage so that the
+                # soup object can be deepcopied later on. Some
+                # Python installations can't copy regexes. If anyone
+                # was relying on the existence of markupMassage, this
+                # might cause problems.
+                del(self.markupMassage)
+        self.reset()
+
+        SGMLParser.feed(self, markup)
+        # Close out any unfinished strings and close all the open tags.
+        self.endData()
+        while self.currentTag.name != self.ROOT_TAG_NAME:
+            self.popTag()
+
+    def __getattr__(self, methodName):
+        """This method routes method call requests to either the SGMLParser
+        superclass or the Tag superclass, depending on the method name."""
+        #print "__getattr__ called on %s.%s" % (self.__class__, methodName)
+
+        if methodName.startswith('start_') or methodName.startswith('end_') \
+               or methodName.startswith('do_'):
+            return SGMLParser.__getattr__(self, methodName)
+        elif not methodName.startswith('__'):
+            return Tag.__getattr__(self, methodName)
+        else:
+            raise AttributeError
+
+    def isSelfClosingTag(self, name):
+        """Returns true iff the given string is the name of a
+        self-closing tag according to this parser."""
+        return self.SELF_CLOSING_TAGS.has_key(name) \
+               or self.instanceSelfClosingTags.has_key(name)
+
+    def reset(self):
+        Tag.__init__(self, self, self.ROOT_TAG_NAME)
+        self.hidden = 1
+        SGMLParser.reset(self)
+        self.currentData = []
+        self.currentTag = None
+        self.tagStack = []
+        self.quoteStack = []
+        self.pushTag(self)
+
+    def popTag(self):
+        tag = self.tagStack.pop()
+
+        #print "Pop", tag.name
+        if self.tagStack:
+            self.currentTag = self.tagStack[-1]
+        return self.currentTag
+
+    def pushTag(self, tag):
+        #print "Push", tag.name
+        if self.currentTag:
+            self.currentTag.contents.append(tag)
+        self.tagStack.append(tag)
+        self.currentTag = self.tagStack[-1]
+
+    def endData(self, containerClass=NavigableString):
+        if self.currentData:
+            currentData = u''.join(self.currentData)
+            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and
+                not set([tag.name for tag in self.tagStack]).intersection(
+                    self.PRESERVE_WHITESPACE_TAGS)):
+                if '\n' in currentData:
+                    currentData = '\n'
+                else:
+                    currentData = ' '
+            self.currentData = []
+            if self.parseOnlyThese and len(self.tagStack) <= 1 and \
+                   (not self.parseOnlyThese.text or \
+                    not self.parseOnlyThese.search(currentData)):
+                return
+            o = containerClass(currentData)
+            o.setup(self.currentTag, self.previous)
+            if self.previous:
+                self.previous.next = o
+            self.previous = o
+            self.currentTag.contents.append(o)
+
+
+    def _popToTag(self, name, inclusivePop=True):
+        """Pops the tag stack up to and including the most recent
+        instance of the given tag. If inclusivePop is false, pops the tag
+        stack up to but *not* including the most recent instqance of
+        the given tag."""
+        #print "Popping to %s" % name
+        if name == self.ROOT_TAG_NAME:
+            return
+
+        numPops = 0
+        mostRecentTag = None
+        for i in range(len(self.tagStack)-1, 0, -1):
+            if name == self.tagStack[i].name:
+                numPops = len(self.tagStack)-i
+                break
+        if not inclusivePop:
+            numPops = numPops - 1
+
+        for i in range(0, numPops):
+            mostRecentTag = self.popTag()
+        return mostRecentTag
+
+    def _smartPop(self, name):
+
+        """We need to pop up to the previous tag of this type, unless
+        one of this tag's nesting reset triggers comes between this
+        tag and the previous tag of this type, OR unless this tag is a
+        generic nesting trigger and another generic nesting trigger
+        comes between this tag and the previous tag of this type.
+
+        Examples:
+         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.
+         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.
+         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.
+
+         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.
+         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'
+         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'
+        """
+
+        nestingResetTriggers = self.NESTABLE_TAGS.get(name)
+        isNestable = nestingResetTriggers != None
+        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)
+        popTo = None
+        inclusive = True
+        for i in range(len(self.tagStack)-1, 0, -1):
+            p = self.tagStack[i]
+            if (not p or p.name == name) and not isNestable:
+                #Non-nestable tags get popped to the top or to their
+                #last occurance.
+                popTo = name
+                break
+            if (nestingResetTriggers is not None
+                and p.name in nestingResetTriggers) \
+                or (nestingResetTriggers is None and isResetNesting
+                    and self.RESET_NESTING_TAGS.has_key(p.name)):
+
+                #If we encounter one of the nesting reset triggers
+                #peculiar to this tag, or we encounter another tag
+                #that causes nesting to reset, pop up to but not
+                #including that tag.
+                popTo = p.name
+                inclusive = False
+                break
+            p = p.parent
+        if popTo:
+            self._popToTag(popTo, inclusive)
+
+    def unknown_starttag(self, name, attrs, selfClosing=0):
+        #print "Start tag %s: %s" % (name, attrs)
+        if self.quoteStack:
+            #This is not a real tag.
+            #print "<%s> is not real!" % name
+            attrs = ''.join([' %s="%s"' % (x, y) for x, y in attrs])
+            self.handle_data('<%s%s>' % (name, attrs))
+            return
+        self.endData()
+
+        if not self.isSelfClosingTag(name) and not selfClosing:
+            self._smartPop(name)
+
+        if self.parseOnlyThese and len(self.tagStack) <= 1 \
+               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):
+            return
+
+        tag = Tag(self, name, attrs, self.currentTag, self.previous)
+        if self.previous:
+            self.previous.next = tag
+        self.previous = tag
+        self.pushTag(tag)
+        if selfClosing or self.isSelfClosingTag(name):
+            self.popTag()
+        if name in self.QUOTE_TAGS:
+            #print "Beginning quote (%s)" % name
+            self.quoteStack.append(name)
+            self.literal = 1
+        return tag
+
+    def unknown_endtag(self, name):
+        #print "End tag %s" % name
+        if self.quoteStack and self.quoteStack[-1] != name:
+            #This is not a real end tag.
+            #print "</%s> is not real!" % name
+            self.handle_data('</%s>' % name)
+            return
+        self.endData()
+        self._popToTag(name)
+        if self.quoteStack and self.quoteStack[-1] == name:
+            self.quoteStack.pop()
+            self.literal = (len(self.quoteStack) > 0)
+
+    def handle_data(self, data):
+        self.currentData.append(data)
+
+    def _toStringSubclass(self, text, subclass):
+        """Adds a certain piece of text to the tree as a NavigableString
+        subclass."""
+        self.endData()
+        self.handle_data(text)
+        self.endData(subclass)
+
+    def handle_pi(self, text):
+        """Handle a processing instruction as a ProcessingInstruction
+        object, possibly one with a %SOUP-ENCODING% slot into which an
+        encoding will be plugged later."""
+        if text[:3] == "xml":
+            text = u"xml version='1.0' encoding='%SOUP-ENCODING%'"
+        self._toStringSubclass(text, ProcessingInstruction)
+
+    def handle_comment(self, text):
+        "Handle comments as Comment objects."
+        self._toStringSubclass(text, Comment)
+
+    def handle_charref(self, ref):
+        "Handle character references as data."
+        if self.convertEntities:
+            data = unichr(int(ref))
+        else:
+            data = '&#%s;' % ref
+        self.handle_data(data)
+
+    def handle_entityref(self, ref):
+        """Handle entity references as data, possibly converting known
+        HTML and/or XML entity references to the corresponding Unicode
+        characters."""
+        data = None
+        if self.convertHTMLEntities:
+            try:
+                data = unichr(name2codepoint[ref])
+            except KeyError:
+                pass
+
+        if not data and self.convertXMLEntities:
+                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)
+
+        if not data and self.convertHTMLEntities and \
+            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):
+                # TODO: We've got a problem here. We're told this is
+                # an entity reference, but it's not an XML entity
+                # reference or an HTML entity reference. Nonetheless,
+                # the logical thing to do is to pass it through as an
+                # unrecognized entity reference.
+                #
+                # Except: when the input is "&carol;" this function
+                # will be called with input "carol". When the input is
+                # "AT&T", this function will be called with input
+                # "T". We have no way of knowing whether a semicolon
+                # was present originally, so we don't know whether
+                # this is an unknown entity or just a misplaced
+                # ampersand.
+                #
+                # The more common case is a misplaced ampersand, so I
+                # escape the ampersand and omit the trailing semicolon.
+                data = "&amp;%s" % ref
+        if not data:
+            # This case is different from the one above, because we
+            # haven't already gone through a supposedly comprehensive
+            # mapping of entities to Unicode characters. We might not
+            # have gone through any mapping at all. So the chances are
+            # very high that this is a real entity, and not a
+            # misplaced ampersand.
+            data = "&%s;" % ref
+        self.handle_data(data)
+
+    def handle_decl(self, data):
+        "Handle DOCTYPEs and the like as Declaration objects."
+        self._toStringSubclass(data, Declaration)
+
+    def parse_declaration(self, i):
+        """Treat a bogus SGML declaration as raw data. Treat a CDATA
+        declaration as a CData object."""
+        j = None
+        if self.rawdata[i:i+9] == '<![CDATA[':
+             k = self.rawdata.find(']]>', i)
+             if k == -1:
+                 k = len(self.rawdata)
+             data = self.rawdata[i+9:k]
+             j = k+3
+             self._toStringSubclass(data, CData)
+        else:
+            try:
+                j = SGMLParser.parse_declaration(self, i)
+            except SGMLParseError:
+                toHandle = self.rawdata[i:]
+                self.handle_data(toHandle)
+                j = i + len(toHandle)
+        return j
+
+class BeautifulSoup(BeautifulStoneSoup):
+
+    """This parser knows the following facts about HTML:
+
+    * Some tags have no closing tag and should be interpreted as being
+      closed as soon as they are encountered.
+
+    * The text inside some tags (ie. 'script') may contain tags which
+      are not really part of the document and which should be parsed
+      as text, not tags. If you want to parse the text as tags, you can
+      always fetch it and parse it explicitly.
+
+    * Tag nesting rules:
+
+      Most tags can't be nested at all. For instance, the occurance of
+      a <p> tag should implicitly close the previous <p> tag.
+
+       <p>Para1<p>Para2
+        should be transformed into:
+       <p>Para1</p><p>Para2
+
+      Some tags can be nested arbitrarily. For instance, the occurance
+      of a <blockquote> tag should _not_ implicitly close the previous
+      <blockquote> tag.
+
+       Alice said: <blockquote>Bob said: <blockquote>Blah
+        should NOT be transformed into:
+       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah
+
+      Some tags can be nested, but the nesting is reset by the
+      interposition of other tags. For instance, a <tr> tag should
+      implicitly close the previous <tr> tag within the same <table>,
+      but not close a <tr> tag in another table.
+
+       <table><tr>Blah<tr>Blah
+        should be transformed into:
+       <table><tr>Blah</tr><tr>Blah
+        but,
+       <tr>Blah<table><tr>Blah
+        should NOT be transformed into
+       <tr>Blah<table></tr><tr>Blah
+
+    Differing assumptions about tag nesting rules are a major source
+    of problems with the BeautifulSoup class. If BeautifulSoup is not
+    treating as nestable a tag your page author treats as nestable,
+    try ICantBelieveItsBeautifulSoup, MinimalSoup, or
+    BeautifulStoneSoup before writing your own subclass."""
+
+    def __init__(self, *args, **kwargs):
+        if not kwargs.has_key('smartQuotesTo'):
+            kwargs['smartQuotesTo'] = self.HTML_ENTITIES
+        kwargs['isHTML'] = True
+        BeautifulStoneSoup.__init__(self, *args, **kwargs)
+
+    SELF_CLOSING_TAGS = buildTagMap(None,
+                                    ('br' , 'hr', 'input', 'img', 'meta',
+                                    'spacer', 'link', 'frame', 'base', 'col'))
+
+    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])
+
+    QUOTE_TAGS = {'script' : None, 'textarea' : None}
+
+    #According to the HTML standard, each of these inline tags can
+    #contain another tag of the same type. Furthermore, it's common
+    #to actually use these tags this way.
+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',
+                            'center')
+
+    #According to the HTML standard, these block tags can contain
+    #another tag of the same type. Furthermore, it's common
+    #to actually use these tags this way.
+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')
+
+    #Lists can contain other lists, but there are restrictions.
+    NESTABLE_LIST_TAGS = { 'ol' : [],
+                           'ul' : [],
+                           'li' : ['ul', 'ol'],
+                           'dl' : [],
+                           'dd' : ['dl'],
+                           'dt' : ['dl'] }
+
+    #Tables can contain other tables, but there are restrictions.
+    NESTABLE_TABLE_TAGS = {'table' : [],
+                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],
+                           'td' : ['tr'],
+                           'th' : ['tr'],
+                           'thead' : ['table'],
+                           'tbody' : ['table'],
+                           'tfoot' : ['table'],
+                           }
+
+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')
+
+    #If one of these tags is encountered, all tags up to the next tag of
+    #this type are popped.
+    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',
+                                     NON_NESTABLE_BLOCK_TAGS,
+                                     NESTABLE_LIST_TAGS,
+                                     NESTABLE_TABLE_TAGS)
+
+    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,
+                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)
+
+    # Used to detect the charset in a META tag; see start_meta
+    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)", re.M)
+
+    def start_meta(self, attrs):
+        """Beautiful Soup can detect a charset included in a META tag,
+        try to convert the document to that charset, and re-parse the
+        document from the beginning."""
+        httpEquiv = None
+        contentType = None
+        contentTypeIndex = None
+        tagNeedsEncodingSubstitution = False
+
+        for i in range(0, len(attrs)):
+            key, value = attrs[i]
+            key = key.lower()
+            if key == 'http-equiv':
+                httpEquiv = value
+            elif key == 'content':
+                contentType = value
+                contentTypeIndex = i
+
+        if httpEquiv and contentType: # It's an interesting meta tag.
+            match = self.CHARSET_RE.search(contentType)
+            if match:
+                if (self.declaredHTMLEncoding is not None or
+                    self.originalEncoding == self.fromEncoding):
+                    # An HTML encoding was sniffed while converting
+                    # the document to Unicode, or an HTML encoding was
+                    # sniffed during a previous pass through the
+                    # document, or an encoding was specified
+                    # explicitly and it worked. Rewrite the meta tag.
+                    def rewrite(match):
+                        return match.group(1) + "%SOUP-ENCODING%"
+                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)
+                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],
+                                               newAttr)
+                    tagNeedsEncodingSubstitution = True
+                else:
+                    # This is our first pass through the document.
+                    # Go through it again with the encoding information.
+                    newCharset = match.group(3)
+                    if newCharset and newCharset != self.originalEncoding:
+                        self.declaredHTMLEncoding = newCharset
+                        self._feed(self.declaredHTMLEncoding)
+                        raise StopParsing
+                    pass
+        tag = self.unknown_starttag("meta", attrs)
+        if tag and tagNeedsEncodingSubstitution:
+            tag.containsSubstitutions = True
+
+class StopParsing(Exception):
+    pass
+
+class ICantBelieveItsBeautifulSoup(BeautifulSoup):
+
+    """The BeautifulSoup class is oriented towards skipping over
+    common HTML errors like unclosed tags. However, sometimes it makes
+    errors of its own. For instance, consider this fragment:
+
+     <b>Foo<b>Bar</b></b>
+
+    This is perfectly valid (if bizarre) HTML. However, the
+    BeautifulSoup class will implicitly close the first b tag when it
+    encounters the second 'b'. It will think the author wrote
+    "<b>Foo<b>Bar", and didn't close the first 'b' tag, because
+    there's no real-world reason to bold something that's already
+    bold. When it encounters '</b></b>' it will close two more 'b'
+    tags, for a grand total of three tags closed instead of two. This
+    can throw off the rest of your document structure. The same is
+    true of a number of other tags, listed below.
+
+    It's much more common for someone to forget to close a 'b' tag
+    than to actually use nested 'b' tags, and the BeautifulSoup class
+    handles the common case. This class handles the not-co-common
+    case: where you can't believe someone wrote what they did, but
+    it's valid HTML and BeautifulSoup screwed up by assuming it
+    wouldn't be."""
+
+    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \
+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
+      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',
+      'big')
+
+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)
+
+    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,
+                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,
+                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)
+
+class MinimalSoup(BeautifulSoup):
+    """The MinimalSoup class is for parsing HTML that contains
+    pathologically bad markup. It makes no assumptions about tag
+    nesting, but it does know which tags are self-closing, that
+    <script> tags contain Javascript and should not be parsed, that
+    META tags may contain encoding information, and so on.
+
+    This also makes it better for subclassing than BeautifulStoneSoup
+    or BeautifulSoup."""
+
+    RESET_NESTING_TAGS = buildTagMap('noscript')
+    NESTABLE_TAGS = {}
+
+class BeautifulSOAP(BeautifulStoneSoup):
+    """This class will push a tag with only a single string child into
+    the tag's parent as an attribute. The attribute's name is the tag
+    name, and the value is the string child. An example should give
+    the flavor of the change:
+
+    <foo><bar>baz</bar></foo>
+     =>
+    <foo bar="baz"><bar>baz</bar></foo>
+
+    You can then access fooTag['bar'] instead of fooTag.barTag.string.
+
+    This is, of course, useful for scraping structures that tend to
+    use subelements instead of attributes, such as SOAP messages. Note
+    that it modifies its input, so don't print the modified version
+    out.
+
+    I'm not sure how many people really want to use this class; let me
+    know if you do. Mainly I like the name."""
+
+    def popTag(self):
+        if len(self.tagStack) > 1:
+            tag = self.tagStack[-1]
+            parent = self.tagStack[-2]
+            parent._getAttrMap()
+            if (isinstance(tag, Tag) and len(tag.contents) == 1 and
+                isinstance(tag.contents[0], NavigableString) and
+                not parent.attrMap.has_key(tag.name)):
+                parent[tag.name] = tag.contents[0]
+        BeautifulStoneSoup.popTag(self)
+
+#Enterprise class names! It has come to our attention that some people
+#think the names of the Beautiful Soup parser classes are too silly
+#and "unprofessional" for use in enterprise screen-scraping. We feel
+#your pain! For such-minded folk, the Beautiful Soup Consortium And
+#All-Night Kosher Bakery recommends renaming this file to
+#"RobustParser.py" (or, in cases of extreme enterprisiness,
+#"RobustParserBeanInterface.class") and using the following
+#enterprise-friendly class aliases:
+class RobustXMLParser(BeautifulStoneSoup):
+    pass
+class RobustHTMLParser(BeautifulSoup):
+    pass
+class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):
+    pass
+class RobustInsanelyWackAssHTMLParser(MinimalSoup):
+    pass
+class SimplifyingSOAPParser(BeautifulSOAP):
+    pass
+
+######################################################
+#
+# Bonus library: Unicode, Dammit
+#
+# This class forces XML data into a standard format (usually to UTF-8
+# or Unicode).  It is heavily based on code from Mark Pilgrim's
+# Universal Feed Parser. It does not rewrite the XML or HTML to
+# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi
+# (XML) and BeautifulSoup.start_meta (HTML).
+
+# Autodetects character encodings.
+# Download from http://chardet.feedparser.org/
+try:
+    import chardet
+#    import chardet.constants
+#    chardet.constants._debug = 1
+except ImportError:
+    chardet = None
+
+# cjkcodecs and iconv_codec make Python know about more character encodings.
+# Both are available from http://cjkpython.i18n.org/
+# They're built in if you use Python 2.4.
+try:
+    import cjkcodecs.aliases
+except ImportError:
+    pass
+try:
+    import iconv_codec
+except ImportError:
+    pass
+
+class UnicodeDammit:
+    """A class for detecting the encoding of a *ML document and
+    converting it to a Unicode string. If the source encoding is
+    windows-1252, can replace MS smart quotes with their HTML or XML
+    equivalents."""
+
+    # This dictionary maps commonly seen values for "charset" in HTML
+    # meta tags to the corresponding Python codec names. It only covers
+    # values that aren't in Python's aliases and can't be determined
+    # by the heuristics in find_codec.
+    CHARSET_ALIASES = { "macintosh" : "mac-roman",
+                        "x-sjis" : "shift-jis" }
+
+    def __init__(self, markup, overrideEncodings=[],
+                 smartQuotesTo='xml', isHTML=False):
+        self.declaredHTMLEncoding = None
+        self.markup, documentEncoding, sniffedEncoding = \
+                     self._detectEncoding(markup, isHTML)
+        self.smartQuotesTo = smartQuotesTo
+        self.triedEncodings = []
+        if markup == '' or isinstance(markup, unicode):
+            self.originalEncoding = None
+            self.unicode = unicode(markup)
+            return
+
+        u = None
+        for proposedEncoding in overrideEncodings:
+            u = self._convertFrom(proposedEncoding)
+            if u: break
+        if not u:
+            for proposedEncoding in (documentEncoding, sniffedEncoding):
+                u = self._convertFrom(proposedEncoding)
+                if u: break
+
+        # If no luck and we have auto-detection library, try that:
+        if not u and chardet and not isinstance(self.markup, unicode):
+            u = self._convertFrom(chardet.detect(self.markup)['encoding'])
+
+        # As a last resort, try utf-8 and windows-1252:
+        if not u:
+            for proposed_encoding in ("utf-8", "windows-1252"):
+                u = self._convertFrom(proposed_encoding)
+                if u: break
+
+        self.unicode = u
+        if not u: self.originalEncoding = None
+
+    def _subMSChar(self, orig):
+        """Changes a MS smart quote character to an XML or HTML
+        entity."""
+        sub = self.MS_CHARS.get(orig)
+        if isinstance(sub, tuple):
+            if self.smartQuotesTo == 'xml':
+                sub = '&#x%s;' % sub[1]
+            else:
+                sub = '&%s;' % sub[0]
+        return sub
+
+    def _convertFrom(self, proposed):
+        proposed = self.find_codec(proposed)
+        if not proposed or proposed in self.triedEncodings:
+            return None
+        self.triedEncodings.append(proposed)
+        markup = self.markup
+
+        # Convert smart quotes to HTML if coming from an encoding
+        # that might have them.
+        if self.smartQuotesTo and proposed.lower() in("windows-1252",
+                                                      "iso-8859-1",
+                                                      "iso-8859-2"):
+            markup = re.compile("([\x80-\x9f])").sub \
+                     (lambda(x): self._subMSChar(x.group(1)),
+                      markup)
+
+        try:
+            # print "Trying to convert document to %s" % proposed
+            u = self._toUnicode(markup, proposed)
+            self.markup = u
+            self.originalEncoding = proposed
+        except Exception, e:
+            # print "That didn't work!"
+            # print e
+            return None
+        #print "Correct encoding: %s" % proposed
+        return self.markup
+
+    def _toUnicode(self, data, encoding):
+        '''Given a string and its encoding, decodes the string into Unicode.
+        %encoding is a string recognized by encodings.aliases'''
+
+        # strip Byte Order Mark (if present)
+        if (len(data) >= 4) and (data[:2] == '\xfe\xff') \
+               and (data[2:4] != '\x00\x00'):
+            encoding = 'utf-16be'
+            data = data[2:]
+        elif (len(data) >= 4) and (data[:2] == '\xff\xfe') \
+                 and (data[2:4] != '\x00\x00'):
+            encoding = 'utf-16le'
+            data = data[2:]
+        elif data[:3] == '\xef\xbb\xbf':
+            encoding = 'utf-8'
+            data = data[3:]
+        elif data[:4] == '\x00\x00\xfe\xff':
+            encoding = 'utf-32be'
+            data = data[4:]
+        elif data[:4] == '\xff\xfe\x00\x00':
+            encoding = 'utf-32le'
+            data = data[4:]
+        newdata = unicode(data, encoding)
+        return newdata
+
+    def _detectEncoding(self, xml_data, isHTML=False):
+        """Given a document, tries to detect its XML encoding."""
+        xml_encoding = sniffed_xml_encoding = None
+        try:
+            if xml_data[:4] == '\x4c\x6f\xa7\x94':
+                # EBCDIC
+                xml_data = self._ebcdic_to_ascii(xml_data)
+            elif xml_data[:4] == '\x00\x3c\x00\x3f':
+                # UTF-16BE
+                sniffed_xml_encoding = 'utf-16be'
+                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') \
+                     and (xml_data[2:4] != '\x00\x00'):
+                # UTF-16BE with BOM
+                sniffed_xml_encoding = 'utf-16be'
+                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
+            elif xml_data[:4] == '\x3c\x00\x3f\x00':
+                # UTF-16LE
+                sniffed_xml_encoding = 'utf-16le'
+                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and \
+                     (xml_data[2:4] != '\x00\x00'):
+                # UTF-16LE with BOM
+                sniffed_xml_encoding = 'utf-16le'
+                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
+            elif xml_data[:4] == '\x00\x00\x00\x3c':
+                # UTF-32BE
+                sniffed_xml_encoding = 'utf-32be'
+                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
+            elif xml_data[:4] == '\x3c\x00\x00\x00':
+                # UTF-32LE
+                sniffed_xml_encoding = 'utf-32le'
+                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
+            elif xml_data[:4] == '\x00\x00\xfe\xff':
+                # UTF-32BE with BOM
+                sniffed_xml_encoding = 'utf-32be'
+                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
+            elif xml_data[:4] == '\xff\xfe\x00\x00':
+                # UTF-32LE with BOM
+                sniffed_xml_encoding = 'utf-32le'
+                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
+            elif xml_data[:3] == '\xef\xbb\xbf':
+                # UTF-8 with BOM
+                sniffed_xml_encoding = 'utf-8'
+                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
+            else:
+                sniffed_xml_encoding = 'ascii'
+                pass
+        except:
+            xml_encoding_match = None
+        xml_encoding_match = re.compile(
+            '^<\?.*encoding=[\'"](.*?)[\'"].*\?>').match(xml_data)
+        if not xml_encoding_match and isHTML:
+            regexp = re.compile('<\s*meta[^>]+charset=([^>]*?)[;\'">]', re.I)
+            xml_encoding_match = regexp.search(xml_data)
+        if xml_encoding_match is not None:
+            xml_encoding = xml_encoding_match.groups()[0].lower()
+            if isHTML:
+                self.declaredHTMLEncoding = xml_encoding
+            if sniffed_xml_encoding and \
+               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',
+                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',
+                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',
+                                 'utf16', 'u16')):
+                xml_encoding = sniffed_xml_encoding
+        return xml_data, xml_encoding, sniffed_xml_encoding
+
+
+    def find_codec(self, charset):
+        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \
+               or (charset and self._codec(charset.replace("-", ""))) \
+               or (charset and self._codec(charset.replace("-", "_"))) \
+               or charset
+
+    def _codec(self, charset):
+        if not charset: return charset
+        codec = None
+        try:
+            codecs.lookup(charset)
+            codec = charset
+        except (LookupError, ValueError):
+            pass
+        return codec
+
+    EBCDIC_TO_ASCII_MAP = None
+    def _ebcdic_to_ascii(self, s):
+        c = self.__class__
+        if not c.EBCDIC_TO_ASCII_MAP:
+            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,
+                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,
+                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,
+                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,
+                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,
+                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,
+                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,
+                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,
+                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,
+                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,
+                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,
+                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,
+                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,
+                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,
+                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,
+                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,
+                    250,251,252,253,254,255)
+            import string
+            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \
+            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
+        return s.translate(c.EBCDIC_TO_ASCII_MAP)
+
+    MS_CHARS = { '\x80' : ('euro', '20AC'),
+                 '\x81' : ' ',
+                 '\x82' : ('sbquo', '201A'),
+                 '\x83' : ('fnof', '192'),
+                 '\x84' : ('bdquo', '201E'),
+                 '\x85' : ('hellip', '2026'),
+                 '\x86' : ('dagger', '2020'),
+                 '\x87' : ('Dagger', '2021'),
+                 '\x88' : ('circ', '2C6'),
+                 '\x89' : ('permil', '2030'),
+                 '\x8A' : ('Scaron', '160'),
+                 '\x8B' : ('lsaquo', '2039'),
+                 '\x8C' : ('OElig', '152'),
+                 '\x8D' : '?',
+                 '\x8E' : ('#x17D', '17D'),
+                 '\x8F' : '?',
+                 '\x90' : '?',
+                 '\x91' : ('lsquo', '2018'),
+                 '\x92' : ('rsquo', '2019'),
+                 '\x93' : ('ldquo', '201C'),
+                 '\x94' : ('rdquo', '201D'),
+                 '\x95' : ('bull', '2022'),
+                 '\x96' : ('ndash', '2013'),
+                 '\x97' : ('mdash', '2014'),
+                 '\x98' : ('tilde', '2DC'),
+                 '\x99' : ('trade', '2122'),
+                 '\x9a' : ('scaron', '161'),
+                 '\x9b' : ('rsaquo', '203A'),
+                 '\x9c' : ('oelig', '153'),
+                 '\x9d' : '?',
+                 '\x9e' : ('#x17E', '17E'),
+                 '\x9f' : ('Yuml', ''),}
+
+#######################################################################
+
+
+#By default, act as an HTML pretty-printer.
+if __name__ == '__main__':
+    import sys
+    soup = BeautifulSoup(sys.stdin)
+    print soup.prettify()
diff -rupN grabber-original/BeautifulSoup/BeautifulSoupTests.py grabber-new/BeautifulSoup/BeautifulSoupTests.py
--- grabber-original/BeautifulSoup/BeautifulSoupTests.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoup/BeautifulSoupTests.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,902 @@
+# -*- coding: utf-8 -*-
+"""Unit tests for Beautiful Soup.
+
+These tests make sure the Beautiful Soup works as it should. If you
+find a bug in Beautiful Soup, the best way to express it is as a test
+case like this that fails."""
+
+import unittest
+from BeautifulSoup import *
+
+class SoupTest(unittest.TestCase):
+
+    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):
+        """Parse the given text and make sure its string rep is the other
+        given text."""
+        if rep == None:
+            rep = toParse
+        self.assertEqual(str(c(toParse)), rep)
+
+
+class FollowThatTag(SoupTest):
+
+    "Tests the various ways of fetching tags from a soup."
+
+    def setUp(self):
+        ml = """
+        <a id="x">1</a>
+        <A id="a">2</a>
+        <b id="b">3</a>
+        <b href="foo" id="x">4</a>
+        <ac width=100>4</ac>"""
+        self.soup = BeautifulStoneSoup(ml)
+
+    def testFindAllByName(self):
+        matching = self.soup('a')
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+        self.assertEqual(matching, self.soup.findAll('a'))
+        self.assertEqual(matching, self.soup.findAll(SoupStrainer('a')))
+
+    def testFindAllByAttribute(self):
+        matching = self.soup.findAll(id='x')
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+        self.assertEqual(matching[1].name, 'b')
+
+        matching2 = self.soup.findAll(attrs={'id' : 'x'})
+        self.assertEqual(matching, matching2)
+
+        strainer = SoupStrainer(attrs={'id' : 'x'})
+        self.assertEqual(matching, self.soup.findAll(strainer))
+
+        self.assertEqual(len(self.soup.findAll(id=None)), 1)
+
+        self.assertEqual(len(self.soup.findAll(width=100)), 1)
+        self.assertEqual(len(self.soup.findAll(junk=None)), 5)
+        self.assertEqual(len(self.soup.findAll(junk=[1, None])), 5)
+
+        self.assertEqual(len(self.soup.findAll(junk=re.compile('.*'))), 0)
+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)
+
+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)
+        self.assertEqual(len(self.soup.findAll(href=True)), 1)
+
+    def testFindallByClass(self):
+        soup = BeautifulSoup('<b class="foo">Foo</b><a class="1 23 4">Bar</a>')
+        self.assertEqual(soup.find(attrs='foo').string, "Foo")
+        self.assertEqual(soup.find('a', '1').string, "Bar")
+        self.assertEqual(soup.find('a', '23').string, "Bar")
+        self.assertEqual(soup.find('a', '4').string, "Bar")
+
+        self.assertEqual(soup.find('a', '2'), None)
+
+    def testFindAllByList(self):
+        matching = self.soup(['a', 'ac'])
+        self.assertEqual(len(matching), 3)
+
+    def testFindAllByHash(self):
+        matching = self.soup({'a' : True, 'b' : True})
+        self.assertEqual(len(matching), 4)
+
+    def testFindAllText(self):
+        soup = BeautifulSoup("<html>\xbb</html>")
+        self.assertEqual(soup.findAll(text=re.compile('.*')),
+                         [u'\xbb'])
+
+    def testFindAllByRE(self):
+        import re
+        r = re.compile('a.*')
+        self.assertEqual(len(self.soup(r)), 3)
+
+    def testFindAllByMethod(self):
+        def matchTagWhereIDMatchesName(tag):
+            return tag.name == tag.get('id')
+
+        matching = self.soup.findAll(matchTagWhereIDMatchesName)
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+
+    def testFindByIndex(self):
+        """For when you have the tag and you want to know where it is."""
+        tag = self.soup.find('a', id="a")
+        self.assertEqual(self.soup.index(tag), 3)
+
+        # It works for NavigableStrings as well.
+        s = tag.string
+        self.assertEqual(tag.index(s), 0)
+
+        # If the tag isn't present, a ValueError is raised.
+        soup2 = BeautifulSoup("<b></b>")
+        tag2 = soup2.find('b')
+        self.assertRaises(ValueError, self.soup.index, tag2)
+
+    def testConflictingFindArguments(self):
+        """The 'text' argument takes precedence."""
+        soup = BeautifulSoup('Foo<b>Bar</b>Baz')
+        self.assertEqual(soup.find('b', text='Baz'), 'Baz')
+        self.assertEqual(soup.findAll('b', text='Baz'), ['Baz'])
+
+        self.assertEqual(soup.find(True, text='Baz'), 'Baz')
+        self.assertEqual(soup.findAll(True, text='Baz'), ['Baz'])
+
+    def testParents(self):
+        soup = BeautifulSoup('<ul id="foo"></ul><ul id="foo"><ul><ul id="foo" a="b"><b>Blah')
+        b = soup.b
+        self.assertEquals(len(b.findParents('ul', {'id' : 'foo'})), 2)
+        self.assertEquals(b.findParent('ul')['a'], 'b')
+
+    PROXIMITY_TEST = BeautifulSoup('<b id="1"><b id="2"><b id="3"><b id="4">')
+
+    def testNext(self):
+        soup = self.PROXIMITY_TEST
+        b = soup.find('b', {'id' : 2})
+        self.assertEquals(b.findNext('b')['id'], '3')
+        self.assertEquals(b.findNext('b')['id'], '3')
+        self.assertEquals(len(b.findAllNext('b')), 2)
+        self.assertEquals(len(b.findAllNext('b', {'id' : 4})), 1)
+
+    def testPrevious(self):
+        soup = self.PROXIMITY_TEST
+        b = soup.find('b', {'id' : 3})
+        self.assertEquals(b.findPrevious('b')['id'], '2')
+        self.assertEquals(b.findPrevious('b')['id'], '2')
+        self.assertEquals(len(b.findAllPrevious('b')), 2)
+        self.assertEquals(len(b.findAllPrevious('b', {'id' : 2})), 1)
+
+
+    SIBLING_TEST = BeautifulSoup('<blockquote id="1"><blockquote id="1.1"></blockquote></blockquote><blockquote id="2"><blockquote id="2.1"></blockquote></blockquote><blockquote id="3"><blockquote id="3.1"></blockquote></blockquote><blockquote id="4">')
+
+    def testNextSibling(self):
+        soup = self.SIBLING_TEST
+        tag = 'blockquote'
+        b = soup.find(tag, {'id' : 2})
+        self.assertEquals(b.findNext(tag)['id'], '2.1')
+        self.assertEquals(b.findNextSibling(tag)['id'], '3')
+        self.assertEquals(b.findNextSibling(tag)['id'], '3')
+        self.assertEquals(len(b.findNextSiblings(tag)), 2)
+        self.assertEquals(len(b.findNextSiblings(tag, {'id' : 4})), 1)
+
+    def testPreviousSibling(self):
+        soup = self.SIBLING_TEST
+        tag = 'blockquote'
+        b = soup.find(tag, {'id' : 3})
+        self.assertEquals(b.findPrevious(tag)['id'], '2.1')
+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')
+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')
+        self.assertEquals(len(b.findPreviousSiblings(tag)), 2)
+        self.assertEquals(len(b.findPreviousSiblings(tag, id=1)), 1)
+
+    def testTextNavigation(self):
+        soup = BeautifulSoup('Foo<b>Bar</b><i id="1"><b>Baz<br />Blee<hr id="1"/></b></i>Blargh')
+        baz = soup.find(text='Baz')
+        self.assertEquals(baz.findParent("i")['id'], '1')
+        self.assertEquals(baz.findNext(text='Blee'), 'Blee')
+        self.assertEquals(baz.findNextSibling(text='Blee'), 'Blee')
+        self.assertEquals(baz.findNextSibling(text='Blargh'), None)
+        self.assertEquals(baz.findNextSibling('hr')['id'], '1')
+
+class SiblingRivalry(SoupTest):
+    "Tests the nextSibling and previousSibling navigation."
+
+    def testSiblings(self):
+        soup = BeautifulSoup("<ul><li>1<p>A</p>B<li>2<li>3</ul>")
+        secondLI = soup.find('li').nextSibling
+        self.assert_(secondLI.name == 'li' and secondLI.string == '2')
+        self.assertEquals(soup.find(text='1').nextSibling.name, 'p')
+        self.assertEquals(soup.find('p').nextSibling, 'B')
+        self.assertEquals(soup.find('p').nextSibling.previousSibling.nextSibling, 'B')
+
+class TagsAreObjectsToo(SoupTest):
+    "Tests the various built-in functions of Tag objects."
+
+    def testLen(self):
+        soup = BeautifulSoup("<top>1<b>2</b>3</top>")
+        self.assertEquals(len(soup.top), 3)
+
+class StringEmUp(SoupTest):
+    "Tests the use of 'string' as an alias for a tag's only content."
+
+    def testString(self):
+        s = BeautifulSoup("<b>foo</b>")
+        self.assertEquals(s.b.string, 'foo')
+
+    def testLackOfString(self):
+        s = BeautifulSoup("<b>f<i>e</i>o</b>")
+        self.assert_(not s.b.string)
+
+    def testStringAssign(self):
+        s = BeautifulSoup("<b></b>")
+        b = s.b
+        b.string = "foo"
+        string = b.string
+        self.assertEquals(string, "foo")
+        self.assert_(isinstance(string, NavigableString))
+
+class AllText(SoupTest):
+    "Tests the use of 'text' to get all of string content from the tag."
+
+    def testText(self):
+        soup = BeautifulSoup("<ul><li>spam</li><li>eggs</li><li>cheese</li>")
+        self.assertEquals(soup.ul.text, "spameggscheese")
+        self.assertEquals(soup.ul.getText('/'), "spam/eggs/cheese")
+
+class ThatsMyLimit(SoupTest):
+    "Tests the limit argument."
+
+    def testBasicLimits(self):
+        s = BeautifulSoup('<br id="1" /><br id="1" /><br id="1" /><br id="1" />')
+        self.assertEquals(len(s.findAll('br')), 4)
+        self.assertEquals(len(s.findAll('br', limit=2)), 2)
+        self.assertEquals(len(s('br', limit=2)), 2)
+
+class OnlyTheLonely(SoupTest):
+    "Tests the parseOnly argument to the constructor."
+    def setUp(self):
+        x = []
+        for i in range(1,6):
+            x.append('<a id="%s">' % i)
+            for j in range(100,103):
+                x.append('<b id="%s.%s">Content %s.%s</b>' % (i,j, i,j))
+            x.append('</a>')
+        self.x = ''.join(x)
+
+    def testOnly(self):
+        strainer = SoupStrainer("b")
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 15)
+
+        strainer = SoupStrainer(id=re.compile("100.*"))
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 5)
+
+        strainer = SoupStrainer(text=re.compile("10[01].*"))
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 10)
+
+        strainer = SoupStrainer(text=lambda(x):x[8]=='3')
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 3)
+
+class PickleMeThis(SoupTest):
+    "Testing features like pickle and deepcopy."
+
+    def setUp(self):
+        self.page = """<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
+"http://www.w3.org/TR/REC-html40/transitional.dtd">
+<html>
+<head>
+<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
+<title>Beautiful Soup: We called him Tortoise because he taught us.</title>
+<link rev="made" href="mailto:leonardr@segfault.org">
+<meta name="Description" content="Beautiful Soup: an HTML parser optimized for screen-scraping.">
+<meta name="generator" content="Markov Approximation 1.4 (module: leonardr)">
+<meta name="author" content="Leonard Richardson">
+</head>
+<body>
+<a href="foo">foo</a>
+<a href="foo"><b>bar</b></a>
+</body>
+</html>"""
+
+        self.soup = BeautifulSoup(self.page)
+
+    def testPickle(self):
+        import pickle
+        dumped = pickle.dumps(self.soup, 2)
+        loaded = pickle.loads(dumped)
+        self.assertEqual(loaded.__class__, BeautifulSoup)
+        self.assertEqual(str(loaded), str(self.soup))
+
+    def testDeepcopy(self):
+        from copy import deepcopy
+        copied = deepcopy(self.soup)
+        self.assertEqual(str(copied), str(self.soup))
+
+    def testUnicodePickle(self):
+        import cPickle as pickle
+        html = "<b>" + chr(0xc3) + "</b>"
+        soup = BeautifulSoup(html)
+        dumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)
+        loaded = pickle.loads(dumped)
+        self.assertEqual(str(loaded), str(soup))
+
+
+class WriteOnlyCode(SoupTest):
+    "Testing the modification of the tree."
+
+    def testModifyAttributes(self):
+        soup = BeautifulSoup('<a id="1"></a>')
+        soup.a['id'] = 2
+        self.assertEqual(soup.renderContents(), '<a id="2"></a>')
+        del(soup.a['id'])
+        self.assertEqual(soup.renderContents(), '<a></a>')
+        soup.a['id2'] = 'foo'
+        self.assertEqual(soup.renderContents(), '<a id2="foo"></a>')
+
+    def testNewTagCreation(self):
+        "Makes sure tags don't step on each others' toes."
+        soup = BeautifulSoup()
+        a = Tag(soup, 'a')
+        ol = Tag(soup, 'ol')
+        a['href'] = 'http://foo.com/'
+        self.assertRaises(KeyError, lambda : ol['href'])
+
+    def testNewTagWithAttributes(self):
+        """Makes sure new tags can be created complete with attributes."""
+        soup = BeautifulSoup()
+        a = Tag(soup, 'a', [('href', 'foo')])
+        b = Tag(soup, 'b', {'class':'bar'})
+        soup.insert(0,a)
+        soup.insert(1,b)
+        self.assertEqual(soup.a['href'], 'foo')
+        self.assertEqual(soup.b['class'], 'bar')
+
+    def testTagReplacement(self):
+        # Make sure you can replace an element with itself.
+        text = "<a><b></b><c>Foo<d></d></c></a><a><e></e></a>"
+        soup = BeautifulSoup(text)
+        c = soup.c
+        soup.c.replaceWith(c)
+        self.assertEquals(str(soup), text)
+
+        # A very simple case
+        soup = BeautifulSoup("<b>Argh!</b>")
+        soup.find(text="Argh!").replaceWith("Hooray!")
+        newText = soup.find(text="Hooray!")
+        b = soup.b
+        self.assertEqual(newText.previous, b)
+        self.assertEqual(newText.parent, b)
+        self.assertEqual(newText.previous.next, newText)
+        self.assertEqual(newText.next, None)
+
+        # A more complex case
+        soup = BeautifulSoup("<a><b>Argh!</b><c></c><d></d></a>")
+        soup.b.insert(1, "Hooray!")
+        newText = soup.find(text="Hooray!")
+        self.assertEqual(newText.previous, "Argh!")
+        self.assertEqual(newText.previous.next, newText)
+
+        self.assertEqual(newText.previousSibling, "Argh!")
+        self.assertEqual(newText.previousSibling.nextSibling, newText)
+
+        self.assertEqual(newText.nextSibling, None)
+        self.assertEqual(newText.next, soup.c)
+
+        text = "<html>There's <b>no</b> business like <b>show</b> business</html>"
+        soup = BeautifulSoup(text)
+        no, show = soup.findAll('b')
+        show.replaceWith(no)
+        self.assertEquals(str(soup), "<html>There's  business like <b>no</b> business</html>")
+
+        # Even more complex
+        soup = BeautifulSoup("<a><b>Find</b><c>lady!</c><d></d></a>")
+        tag = Tag(soup, 'magictag')
+        tag.insert(0, "the")
+        soup.a.insert(1, tag)
+
+        b = soup.b
+        c = soup.c
+        theText = tag.find(text=True)
+        findText = b.find(text="Find")
+
+        self.assertEqual(findText.next, tag)
+        self.assertEqual(tag.previous, findText)
+        self.assertEqual(b.nextSibling, tag)
+        self.assertEqual(tag.previousSibling, b)
+        self.assertEqual(tag.nextSibling, c)
+        self.assertEqual(c.previousSibling, tag)
+
+        self.assertEqual(theText.next, c)
+        self.assertEqual(c.previous, theText)
+
+        # Aand... incredibly complex.
+        soup = BeautifulSoup("""<a>We<b>reserve<c>the</c><d>right</d></b></a><e>to<f>refuse</f><g>service</g></e>""")
+        f = soup.f
+        a = soup.a
+        c = soup.c
+        e = soup.e
+        weText = a.find(text="We")
+        soup.b.replaceWith(soup.f)
+        self.assertEqual(str(soup), "<a>We<f>refuse</f></a><e>to<g>service</g></e>")
+
+        self.assertEqual(f.previous, weText)
+        self.assertEqual(weText.next, f)
+        self.assertEqual(f.previousSibling, weText)
+        self.assertEqual(f.nextSibling, None)
+        self.assertEqual(weText.nextSibling, f)
+
+    def testReplaceWithChildren(self):
+        soup = BeautifulStoneSoup(
+            "<top><replace><child1/><child2/></replace></top>",
+            selfClosingTags=["child1", "child2"])
+        soup.replaceTag.replaceWithChildren()
+        self.assertEqual(soup.top.contents[0].name, "child1")
+        self.assertEqual(soup.top.contents[1].name, "child2")
+
+    def testAppend(self):
+       doc = "<p>Don't leave me <b>here</b>.</p> <p>Don't leave me.</p>"
+       soup = BeautifulSoup(doc)
+       second_para = soup('p')[1]
+       bold = soup.find('b')
+       soup('p')[1].append(soup.find('b'))
+       self.assertEqual(bold.parent, second_para)
+       self.assertEqual(str(soup),
+                        "<p>Don't leave me .</p> "
+                        "<p>Don't leave me.<b>here</b></p>")
+
+    def testTagExtraction(self):
+        # A very simple case
+        text = '<html><div id="nav">Nav crap</div>Real content here.</html>'
+        soup = BeautifulSoup(text)
+        extracted = soup.find("div", id="nav").extract()
+        self.assertEqual(str(soup), "<html>Real content here.</html>")
+        self.assertEqual(str(extracted), '<div id="nav">Nav crap</div>')
+
+        # A simple case, a more complex test.
+        text = "<doc><a>1<b>2</b></a><a>i<b>ii</b></a><a>A<b>B</b></a></doc>"
+        soup = BeautifulStoneSoup(text)
+        doc = soup.doc
+        numbers, roman, letters = soup("a")
+
+        self.assertEqual(roman.parent, doc)
+        oldPrevious = roman.previous
+        endOfThisTag = roman.nextSibling.previous
+        self.assertEqual(oldPrevious, "2")
+        self.assertEqual(roman.next, "i")
+        self.assertEqual(endOfThisTag, "ii")
+        self.assertEqual(roman.previousSibling, numbers)
+        self.assertEqual(roman.nextSibling, letters)
+
+        roman.extract()
+        self.assertEqual(roman.parent, None)
+        self.assertEqual(roman.previous, None)
+        self.assertEqual(roman.next, "i")
+        self.assertEqual(letters.previous, '2')
+        self.assertEqual(roman.previousSibling, None)
+        self.assertEqual(roman.nextSibling, None)
+        self.assertEqual(endOfThisTag.next, None)
+        self.assertEqual(roman.b.contents[0].next, None)
+        self.assertEqual(numbers.nextSibling, letters)
+        self.assertEqual(letters.previousSibling, numbers)
+        self.assertEqual(len(doc.contents), 2)
+        self.assertEqual(doc.contents[0], numbers)
+        self.assertEqual(doc.contents[1], letters)
+
+        # A more complex case.
+        text = "<a>1<b>2<c>Hollywood, baby!</c></b></a>3"
+        soup = BeautifulStoneSoup(text)
+        one = soup.find(text="1")
+        three = soup.find(text="3")
+        toExtract = soup.b
+        soup.b.extract()
+        self.assertEqual(one.next, three)
+        self.assertEqual(three.previous, one)
+        self.assertEqual(one.parent.nextSibling, three)
+        self.assertEqual(three.previousSibling, soup.a)
+        
+    def testClear(self):
+        soup = BeautifulSoup("<ul><li></li><li></li></ul>")
+        soup.ul.clear()
+        self.assertEqual(len(soup.ul.contents), 0)
+
+class TheManWithoutAttributes(SoupTest):
+    "Test attribute access"
+
+    def testHasKey(self):
+        text = "<foo attr='bar'>"
+        self.assertEquals(BeautifulSoup(text).foo.has_key('attr'), True)
+
+class QuoteMeOnThat(SoupTest):
+    "Test quoting"
+    def testQuotedAttributeValues(self):
+        self.assertSoupEquals("<foo attr='bar'></foo>",
+                              '<foo attr="bar"></foo>')
+
+        text = """<foo attr='bar "brawls" happen'>a</foo>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.renderContents(), text)
+
+        soup.foo['attr'] = 'Brawls happen at "Bob\'s Bar"'
+        newText = """<foo attr='Brawls happen at "Bob&squot;s Bar"'>a</foo>"""
+        self.assertSoupEquals(soup.renderContents(), newText)
+
+        self.assertSoupEquals('<this is="really messed up & stuff">',
+                              '<this is="really messed up &amp; stuff"></this>')
+
+        # This is not what the original author had in mind, but it's
+        # a legitimate interpretation of what they wrote.
+        self.assertSoupEquals("""<a href="foo</a>, </a><a href="bar">baz</a>""",
+        '<a href="foo&lt;/a&gt;, &lt;/a&gt;&lt;a href="></a>, <a href="bar">baz</a>')
+
+        # SGMLParser generates bogus parse events when attribute values
+        # contain embedded brackets, but at least Beautiful Soup fixes
+        # it up a little.
+        self.assertSoupEquals('<a b="<a>">', '<a b="&lt;a&gt;"></a><a>"&gt;</a>')
+        self.assertSoupEquals('<a href="http://foo.com/<a> and blah and blah',
+                              """<a href='"http://foo.com/'></a><a> and blah and blah</a>""")
+
+
+
+class YoureSoLiteral(SoupTest):
+    "Test literal mode."
+    def testLiteralMode(self):
+        text = "<script>if (i<imgs.length)</script><b>Foo</b>"
+        soup = BeautifulSoup(text)
+        self.assertEqual(soup.script.contents[0], "if (i<imgs.length)")
+        self.assertEqual(soup.b.contents[0], "Foo")
+
+    def testTextArea(self):
+        text = "<textarea><b>This is an example of an HTML tag</b><&<&</textarea>"
+        soup = BeautifulSoup(text)
+        self.assertEqual(soup.textarea.contents[0],
+                         "<b>This is an example of an HTML tag</b><&<&")
+
+class OperatorOverload(SoupTest):
+    "Our operators do it all! Call now!"
+
+    def testTagNameAsFind(self):
+        "Tests that referencing a tag name as a member delegates to find()."
+        soup = BeautifulSoup('<b id="1">foo<i>bar</i></b><b>Red herring</b>')
+        self.assertEqual(soup.b.i, soup.find('b').find('i'))
+        self.assertEqual(soup.b.i.string, 'bar')
+        self.assertEqual(soup.b['id'], '1')
+        self.assertEqual(soup.b.contents[0], 'foo')
+        self.assert_(not soup.a)
+
+        #Test the .fooTag variant of .foo.
+        self.assertEqual(soup.bTag.iTag.string, 'bar')
+        self.assertEqual(soup.b.iTag.string, 'bar')
+        self.assertEqual(soup.find('b').find('i'), soup.bTag.iTag)
+
+class NestableEgg(SoupTest):
+    """Here we test tag nesting. TEST THE NEST, DUDE! X-TREME!"""
+
+    def testParaInsideBlockquote(self):
+        soup = BeautifulSoup('<blockquote><p><b>Foo</blockquote><p>Bar')
+        self.assertEqual(soup.blockquote.p.b.string, 'Foo')
+        self.assertEqual(soup.blockquote.b.string, 'Foo')
+        self.assertEqual(soup.find('p', recursive=False).string, 'Bar')
+
+    def testNestedTables(self):
+        text = """<table id="1"><tr><td>Here's another table:
+        <table id="2"><tr><td>Juicy text</td></tr></table></td></tr></table>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.table.td.string, 'Juicy text')
+        self.assertEquals(len(soup.findAll('table')), 2)
+        self.assertEquals(len(soup.table.findAll('table')), 1)
+        self.assertEquals(soup.find('table', {'id' : 2}).parent.parent.parent.name,
+                          'table')
+
+        text = "<table><tr><td><div><table>Foo</table></div></td></tr></table>"
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.tr.td.div.table.contents[0], "Foo")
+
+        text = """<table><thead><tr>Foo</tr></thead><tbody><tr>Bar</tr></tbody>
+        <tfoot><tr>Baz</tr></tfoot></table>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.thead.tr.contents[0], "Foo")
+
+    def testBadNestedTables(self):
+        soup = BeautifulSoup("<table><tr><table><tr id='nested'>")
+        self.assertEquals(soup.table.tr.table.tr['id'], 'nested')
+
+class CleanupOnAisleFour(SoupTest):
+    """Here we test cleanup of text that breaks SGMLParser or is just
+    obnoxious."""
+
+    def testSelfClosingtag(self):
+        self.assertEqual(str(BeautifulSoup("Foo<br/>Bar").find('br')),
+                         '<br />')
+
+        self.assertSoupEquals('<p>test1<br/>test2</p>',
+                              '<p>test1<br />test2</p>')
+
+        text = '<p>test1<selfclosing>test2'
+        soup = BeautifulStoneSoup(text)
+        self.assertEqual(str(soup),
+                         '<p>test1<selfclosing>test2</selfclosing></p>')
+
+        soup = BeautifulStoneSoup(text, selfClosingTags='selfclosing')
+        self.assertEqual(str(soup),
+                         '<p>test1<selfclosing />test2</p>')
+
+    def testSelfClosingTagOrNot(self):
+        text = "<item><link>http://foo.com/</link></item>"
+        self.assertEqual(BeautifulStoneSoup(text).renderContents(), text)
+        self.assertEqual(BeautifulSoup(text).renderContents(),
+                         '<item><link />http://foo.com/</item>')
+
+    def testCData(self):
+        xml = "<root>foo<![CDATA[foobar]]>bar</root>"
+        self.assertSoupEquals(xml, xml)
+        r = re.compile("foo.*bar")
+        soup = BeautifulSoup(xml)
+        self.assertEquals(soup.find(text=r).string, "foobar")
+        self.assertEquals(soup.find(text=r).__class__, CData)
+
+    def testComments(self):
+        xml = "foo<!--foobar-->baz"
+        self.assertSoupEquals(xml)
+        r = re.compile("foo.*bar")
+        soup = BeautifulSoup(xml)
+        self.assertEquals(soup.find(text=r).string, "foobar")
+        self.assertEquals(soup.find(text="foobar").__class__, Comment)
+
+    def testDeclaration(self):
+        xml = "foo<!DOCTYPE foobar>baz"
+        self.assertSoupEquals(xml)
+        r = re.compile(".*foo.*bar")
+        soup = BeautifulSoup(xml)
+        text = "DOCTYPE foobar"
+        self.assertEquals(soup.find(text=r).string, text)
+        self.assertEquals(soup.find(text=text).__class__, Declaration)
+
+        namespaced_doctype = ('<!DOCTYPE xsl:stylesheet SYSTEM "htmlent.dtd">'
+                              '<html>foo</html>')
+        soup = BeautifulSoup(namespaced_doctype)
+        self.assertEquals(soup.contents[0],
+                          'DOCTYPE xsl:stylesheet SYSTEM "htmlent.dtd"')
+        self.assertEquals(soup.html.contents[0], 'foo')
+
+    def testEntityConversions(self):
+        text = "&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;"
+        soup = BeautifulStoneSoup(text)
+        self.assertSoupEquals(text)
+
+        xmlEnt = BeautifulStoneSoup.XML_ENTITIES
+        htmlEnt = BeautifulStoneSoup.HTML_ENTITIES
+        xhtmlEnt = BeautifulStoneSoup.XHTML_ENTITIES
+
+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)
+        self.assertEquals(str(soup), "&lt;&lt;sacr&eacute; bleu!&gt;&gt;")
+
+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;&lt;sacr\xe9 bleu!&gt;&gt;")
+
+        # Make sure the "XML", "HTML", and "XHTML" settings work.
+        text = "&lt;&trade;&apos;"
+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;&trade;'")
+
+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;\u2122&apos;")
+
+        soup = BeautifulStoneSoup(text, convertEntities=xhtmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;\u2122'")
+
+        invalidEntity = "foo&#bar;baz"
+        soup = BeautifulStoneSoup\
+               (invalidEntity,
+                convertEntities=htmlEnt)
+        self.assertEquals(str(soup), "foo&amp;#bar;baz")
+
+        nonexistentEntity = "foo&bar;baz"
+        soup = BeautifulStoneSoup\
+               (nonexistentEntity,
+                convertEntities="xml")
+        self.assertEquals(str(soup), nonexistentEntity)
+
+
+    def testNonBreakingSpaces(self):
+        soup = BeautifulSoup("<a>&nbsp;&nbsp;</a>",
+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup), u"<a>\xa0\xa0</a>")
+
+    def testWhitespaceInDeclaration(self):
+        self.assertSoupEquals('<! DOCTYPE>', '<!DOCTYPE>')
+
+    def testJunkInDeclaration(self):
+        self.assertSoupEquals('<! Foo = -8>a', '&lt;!Foo = -8&gt;a')
+
+    def testIncompleteDeclaration(self):
+        self.assertSoupEquals('a<!b <p>c', 'a&lt;!b &lt;p&gt;c')
+
+    def testEntityReplacement(self):
+        self.assertSoupEquals('<b>hello&nbsp;there</b>')
+
+    def testEntitiesInAttributeValues(self):
+        self.assertSoupEquals('<x t="x&#241;">', '<x t="x\xc3\xb1"></x>')
+        self.assertSoupEquals('<x t="x&#xf1;">', '<x t="x\xc3\xb1"></x>')
+
+        soup = BeautifulSoup('<x t="&gt;&trade;">',
+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup), u'<x t="&gt;\u2122"></x>')
+
+        uri = "http://crummy.com?sacr&eacute;&amp;bleu"
+        link = '<a href="%s"></a>' % uri
+        soup = BeautifulSoup(link)
+        self.assertEquals(unicode(soup), link)
+        #self.assertEquals(unicode(soup.a['href']), uri)
+
+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup),
+                          link.replace("&eacute;", u"\xe9"))
+
+        uri = "http://crummy.com?sacr&eacute;&bleu"
+        link = '<a href="%s"></a>' % uri
+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup.a['href']),
+                          uri.replace("&eacute;", u"\xe9"))
+
+    def testNakedAmpersands(self):
+        html = {'convertEntities':BeautifulStoneSoup.HTML_ENTITIES}
+        soup = BeautifulStoneSoup("AT&T ", **html)
+        self.assertEquals(str(soup), 'AT&amp;T ')
+
+        nakedAmpersandInASentence = "AT&T was Ma Bell"
+        soup = BeautifulStoneSoup(nakedAmpersandInASentence,**html)
+        self.assertEquals(str(soup), \
+               nakedAmpersandInASentence.replace('&','&amp;'))
+
+        invalidURL = '<a href="http://example.org?a=1&b=2;3">foo</a>'
+        validURL = invalidURL.replace('&','&amp;')
+        soup = BeautifulStoneSoup(invalidURL)
+        self.assertEquals(str(soup), validURL)
+
+        soup = BeautifulStoneSoup(validURL)
+        self.assertEquals(str(soup), validURL)
+
+
+class EncodeRed(SoupTest):
+    """Tests encoding conversion, Unicode conversion, and Microsoft
+    smart quote fixes."""
+
+    def testUnicodeDammitStandalone(self):
+        markup = "<foo>\x92</foo>"
+        dammit = UnicodeDammit(markup)
+        self.assertEquals(dammit.unicode, "<foo>&#x2019;</foo>")
+
+        hebrew = "\xed\xe5\xec\xf9"
+        dammit = UnicodeDammit(hebrew, ["iso-8859-8"])
+        self.assertEquals(dammit.unicode, u'\u05dd\u05d5\u05dc\u05e9')
+        self.assertEquals(dammit.originalEncoding, 'iso-8859-8')
+
+    def testGarbageInGarbageOut(self):
+        ascii = "<foo>a</foo>"
+        asciiSoup = BeautifulStoneSoup(ascii)
+        self.assertEquals(ascii, str(asciiSoup))
+
+        unicodeData = u"<foo>\u00FC</foo>"
+        utf8 = unicodeData.encode("utf-8")
+        self.assertEquals(utf8, '<foo>\xc3\xbc</foo>')
+
+        unicodeSoup = BeautifulStoneSoup(unicodeData)
+        self.assertEquals(unicodeData, unicode(unicodeSoup))
+        self.assertEquals(unicode(unicodeSoup.foo.string), u'\u00FC')
+
+        utf8Soup = BeautifulStoneSoup(utf8, fromEncoding='utf-8')
+        self.assertEquals(utf8, str(utf8Soup))
+        self.assertEquals(utf8Soup.originalEncoding, "utf-8")
+
+        utf8Soup = BeautifulStoneSoup(unicodeData)
+        self.assertEquals(utf8, str(utf8Soup))
+        self.assertEquals(utf8Soup.originalEncoding, None)
+
+
+    def testHandleInvalidCodec(self):
+        for bad_encoding in ['.utf8', '...', 'utF---16.!']:
+            soup = BeautifulSoup("Räksmörgås", fromEncoding=bad_encoding)
+            self.assertEquals(soup.originalEncoding, 'utf-8')
+
+    def testUnicodeSearch(self):
+        html = u'<html><body><h1>Räksmörgås</h1></body></html>'
+        soup = BeautifulSoup(html)
+        self.assertEqual(soup.find(text=u'Räksmörgås'),u'Räksmörgås')
+
+    def testRewrittenXMLHeader(self):
+        euc_jp = '<?xml version="1.0 encoding="euc-jp"?>\n<foo>\n\xa4\xb3\xa4\xec\xa4\xcfEUC-JP\xa4\xc7\xa5\xb3\xa1\xbc\xa5\xc7\xa5\xa3\xa5\xf3\xa5\xb0\xa4\xb5\xa4\xec\xa4\xbf\xc6\xfc\xcb\xdc\xb8\xec\xa4\xce\xa5\xd5\xa5\xa1\xa5\xa4\xa5\xeb\xa4\xc7\xa4\xb9\xa1\xa3\n</foo>\n'
+        utf8 = "<?xml version='1.0' encoding='utf-8'?>\n<foo>\n\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xafEUC-JP\xe3\x81\xa7\xe3\x82\xb3\xe3\x83\xbc\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x82\xb0\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e\xe3\x81\xae\xe3\x83\x95\xe3\x82\xa1\xe3\x82\xa4\xe3\x83\xab\xe3\x81\xa7\xe3\x81\x99\xe3\x80\x82\n</foo>\n"
+        soup = BeautifulStoneSoup(euc_jp)
+        if soup.originalEncoding != "euc-jp":
+            raise Exception("Test failed when parsing euc-jp document. "
+                            "If you're running Python >=2.4, or you have "
+                            "cjkcodecs installed, this is a real problem. "
+                            "Otherwise, ignore it.")
+
+        self.assertEquals(soup.originalEncoding, "euc-jp")
+        self.assertEquals(str(soup), utf8)
+
+        old_text = "<?xml encoding='windows-1252'><foo>\x92</foo>"
+        new_text = "<?xml version='1.0' encoding='utf-8'?><foo>&rsquo;</foo>"
+        self.assertSoupEquals(old_text, new_text)
+
+    def testRewrittenMetaTag(self):
+        no_shift_jis_html = '''<html><head>\n<meta http-equiv="Content-language" content="ja" /></head><body><pre>\n\x82\xb1\x82\xea\x82\xcdShift-JIS\x82\xc5\x83R\x81[\x83f\x83B\x83\x93\x83O\x82\xb3\x82\xea\x82\xbd\x93\xfa\x96{\x8c\xea\x82\xcc\x83t\x83@\x83C\x83\x8b\x82\xc5\x82\xb7\x81B\n</pre></body></html>'''
+        soup = BeautifulSoup(no_shift_jis_html)
+
+        # Beautiful Soup used to try to rewrite the meta tag even if the
+        # meta tag got filtered out by the strainer. This test makes
+        # sure that doesn't happen.
+        strainer = SoupStrainer('pre')
+        soup = BeautifulSoup(no_shift_jis_html, parseOnlyThese=strainer)
+        self.assertEquals(soup.contents[0].name, 'pre')
+
+        meta_tag = ('<meta content="text/html; charset=x-sjis" '
+                    'http-equiv="Content-type" />')
+        shift_jis_html = (
+            '<html><head>\n%s\n'
+            '<meta http-equiv="Content-language" content="ja" />'
+            '</head><body><pre>\n'
+            '\x82\xb1\x82\xea\x82\xcdShift-JIS\x82\xc5\x83R\x81[\x83f'
+            '\x83B\x83\x93\x83O\x82\xb3\x82\xea\x82\xbd\x93\xfa\x96{\x8c'
+            '\xea\x82\xcc\x83t\x83@\x83C\x83\x8b\x82\xc5\x82\xb7\x81B\n'
+            '</pre></body></html>') % meta_tag
+        soup = BeautifulSoup(shift_jis_html)
+        if soup.originalEncoding != "shift-jis":
+            raise Exception("Test failed when parsing shift-jis document "
+                            "with meta tag '%s'."
+                            "If you're running Python >=2.4, or you have "
+                            "cjkcodecs installed, this is a real problem. "
+                            "Otherwise, ignore it." % meta_tag)
+        self.assertEquals(soup.originalEncoding, "shift-jis")
+
+        content_type_tag = soup.meta['content']
+        self.assertEquals(content_type_tag[content_type_tag.find('charset='):],
+                          'charset=%SOUP-ENCODING%')
+        content_type = str(soup.meta)
+        index = content_type.find('charset=')
+        self.assertEqual(content_type[index:index+len('charset=utf8')+1],
+                         'charset=utf-8')
+        content_type = soup.meta.__str__('shift-jis')
+        index = content_type.find('charset=')
+        self.assertEqual(content_type[index:index+len('charset=shift-jis')],
+                         'charset=shift-jis')
+
+        self.assertEquals(str(soup), (
+                '<html><head>\n'
+                '<meta content="text/html; charset=utf-8" '
+                'http-equiv="Content-type" />\n'
+                '<meta http-equiv="Content-language" content="ja" />'
+                '</head><body><pre>\n'
+                '\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xafShift-JIS\xe3\x81\xa7\xe3'
+                '\x82\xb3\xe3\x83\xbc\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3'
+                '\x82\xb0\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe6\x97\xa5\xe6'
+                '\x9c\xac\xe8\xaa\x9e\xe3\x81\xae\xe3\x83\x95\xe3\x82\xa1\xe3'
+                '\x82\xa4\xe3\x83\xab\xe3\x81\xa7\xe3\x81\x99\xe3\x80\x82\n'
+                '</pre></body></html>'))
+        self.assertEquals(soup.renderContents("shift-jis"),
+                          shift_jis_html.replace('x-sjis', 'shift-jis'))
+
+        isolatin ="""<html><meta http-equiv="Content-type" content="text/html; charset=ISO-Latin-1" />Sacr\xe9 bleu!</html>"""
+        soup = BeautifulSoup(isolatin)
+        self.assertSoupEquals(soup.__str__("utf-8"),
+                              isolatin.replace("ISO-Latin-1", "utf-8").replace("\xe9", "\xc3\xa9"))
+
+    def testHebrew(self):
+        iso_8859_8= '<HEAD>\n<TITLE>Hebrew (ISO 8859-8) in Visual Directionality</TITLE>\n\n\n\n</HEAD>\n<BODY>\n<H1>Hebrew (ISO 8859-8) in Visual Directionality</H1>\n\xed\xe5\xec\xf9\n</BODY>\n'
+        utf8 = '<head>\n<title>Hebrew (ISO 8859-8) in Visual Directionality</title>\n</head>\n<body>\n<h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\n\xd7\x9d\xd7\x95\xd7\x9c\xd7\xa9\n</body>\n'
+        soup = BeautifulStoneSoup(iso_8859_8, fromEncoding="iso-8859-8")
+        self.assertEquals(str(soup), utf8)
+
+    def testSmartQuotesNotSoSmartAnymore(self):
+        self.assertSoupEquals("\x91Foo\x92 <!--blah-->",
+                              '&lsquo;Foo&rsquo; <!--blah-->')
+
+    def testDontConvertSmartQuotesWhenAlsoConvertingEntities(self):
+        smartQuotes = "Il a dit, \x8BSacr&eacute; bl&#101;u!\x9b"
+        soup = BeautifulSoup(smartQuotes)
+        self.assertEquals(str(soup),
+                          'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;')
+        soup = BeautifulSoup(smartQuotes, convertEntities="html")
+        self.assertEquals(str(soup),
+                          'Il a dit, \xe2\x80\xb9Sacr\xc3\xa9 bleu!\xe2\x80\xba')
+
+    def testDontSeeSmartQuotesWhereThereAreNone(self):
+        utf_8 = "\343\202\261\343\203\274\343\202\277\343\202\244 Watch"
+        self.assertSoupEquals(utf_8)
+
+
+class Whitewash(SoupTest):
+    """Test whitespace preservation."""
+
+    def testPreservedWhitespace(self):
+        self.assertSoupEquals("<pre>   </pre>")
+        self.assertSoupEquals("<pre> woo  </pre>")
+
+    def testCollapsedWhitespace(self):
+        self.assertSoupEquals("<p>   </p>", "<p> </p>")
+
+
+if __name__ == '__main__':
+    unittest.main()
diff -rupN grabber-original/BeautifulSoup/PKG-INFO grabber-new/BeautifulSoup/PKG-INFO
--- grabber-original/BeautifulSoup/PKG-INFO	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoup/PKG-INFO	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,19 @@
+Metadata-Version: 1.0
+Name: BeautifulSoup
+Version: 3.2.1
+Summary: HTML/XML parser for quick-turnaround applications like screen-scraping.
+Home-page: http://www.crummy.com/software/BeautifulSoup/
+Author: Leonard Richardson
+Author-email: leonardr@segfault.org
+License: BSD
+Download-URL: http://www.crummy.com/software/BeautifulSoup/download/
+Description: Beautiful Soup parses arbitrarily invalid SGML and provides a variety of methods and Pythonic idioms for iterating and searching the parse tree.
+Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: Python Software Foundation License
+Classifier: Programming Language :: Python
+Classifier: Topic :: Text Processing :: Markup :: HTML
+Classifier: Topic :: Text Processing :: Markup :: XML
+Classifier: Topic :: Text Processing :: Markup :: SGML
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
diff -rupN grabber-original/BeautifulSoup/setup.py grabber-new/BeautifulSoup/setup.py
--- grabber-original/BeautifulSoup/setup.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoup/setup.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,60 @@
+from distutils.core import setup
+import unittest
+import warnings
+warnings.filterwarnings("ignore", "Unknown distribution option")
+
+import sys
+# patch distutils if it can't cope with the "classifiers" keyword
+if sys.version < '2.2.3':
+    from distutils.dist import DistributionMetadata
+    DistributionMetadata.classifiers = None
+    DistributionMetadata.download_url = None
+
+from BeautifulSoup import __version__
+
+#Make sure all the tests complete.
+import BeautifulSoupTests
+loader = unittest.TestLoader()
+result = unittest.TestResult()
+suite = loader.loadTestsFromModule(BeautifulSoupTests)
+suite.run(result)
+if not result.wasSuccessful():
+    print "Unit tests have failed!"
+    for l in result.errors, result.failures:
+        for case, error in l:
+            print "-" * 80
+            desc = case.shortDescription()
+            if desc:
+                print desc
+            print error        
+    print '''If you see an error like: "'ascii' codec can't encode character...", see\nthe Beautiful Soup documentation:\n http://www.crummy.com/software/BeautifulSoup/documentation.html#Why%20can't%20Beautiful%20Soup%20print%20out%20the%20non-ASCII%20characters%20I%20gave%20it?'''
+    print "This might or might not be a problem depending on what you plan to do with\nBeautiful Soup."
+    if sys.argv[1] == 'sdist':
+        print
+        print "I'm not going to make a source distribution since the tests don't pass."
+        sys.exit(1)
+
+setup(name="BeautifulSoup",
+      version=__version__,
+      py_modules=['BeautifulSoup', 'BeautifulSoupTests'],
+      description="HTML/XML parser for quick-turnaround applications like screen-scraping.",
+      author="Leonard Richardson",
+      author_email = "leonardr@segfault.org",
+      long_description="""Beautiful Soup parses arbitrarily invalid SGML and provides a variety of methods and Pythonic idioms for iterating and searching the parse tree.""",
+      classifiers=["Development Status :: 5 - Production/Stable",
+                   "Intended Audience :: Developers",
+                   "License :: OSI Approved :: Python Software Foundation License",
+                   "Programming Language :: Python",
+                   "Topic :: Text Processing :: Markup :: HTML",
+                   "Topic :: Text Processing :: Markup :: XML",
+                   "Topic :: Text Processing :: Markup :: SGML",
+                   "Topic :: Software Development :: Libraries :: Python Modules",
+                   ],
+      url="http://www.crummy.com/software/BeautifulSoup/",
+      license="BSD",
+      download_url="http://www.crummy.com/software/BeautifulSoup/download/"
+      )
+    
+    # Send announce to:
+    #   python-announce@python.org
+    #   python-list@python.org
diff -rupN grabber-original/BeautifulSoup.py grabber-new/BeautifulSoup.py
--- grabber-original/BeautifulSoup.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoup.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,2017 @@
+"""Beautiful Soup
+Elixir and Tonic
+"The Screen-Scraper's Friend"
+http://www.crummy.com/software/BeautifulSoup/
+
+Beautiful Soup parses a (possibly invalid) XML or HTML document into a
+tree representation. It provides methods and Pythonic idioms that make
+it easy to navigate, search, and modify the tree.
+
+A well-formed XML/HTML document yields a well-formed data
+structure. An ill-formed XML/HTML document yields a correspondingly
+ill-formed data structure. If your document is only locally
+well-formed, you can use this library to find and process the
+well-formed part of it.
+
+Beautiful Soup works with Python 2.2 and up. It has no external
+dependencies, but you'll have more success at converting data to UTF-8
+if you also install these three packages:
+
+* chardet, for auto-detecting character encodings
+  http://chardet.feedparser.org/
+* cjkcodecs and iconv_codec, which add more encodings to the ones supported
+  by stock Python.
+  http://cjkpython.i18n.org/
+
+Beautiful Soup defines classes for two main parsing strategies:
+
+ * BeautifulStoneSoup, for parsing XML, SGML, or your domain-specific
+   language that kind of looks like XML.
+
+ * BeautifulSoup, for parsing run-of-the-mill HTML code, be it valid
+   or invalid. This class has web browser-like heuristics for
+   obtaining a sensible parse tree in the face of common HTML errors.
+
+Beautiful Soup also defines a class (UnicodeDammit) for autodetecting
+the encoding of an HTML or XML document, and converting it to
+Unicode. Much of this code is taken from Mark Pilgrim's Universal Feed Parser.
+
+For more than you ever wanted to know about Beautiful Soup, see the
+documentation:
+http://www.crummy.com/software/BeautifulSoup/documentation.html
+
+Here, have some legalese:
+
+Copyright (c) 2004-2010, Leonard Richardson
+
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are
+met:
+
+  * Redistributions of source code must retain the above copyright
+    notice, this list of conditions and the following disclaimer.
+
+  * Redistributions in binary form must reproduce the above
+    copyright notice, this list of conditions and the following
+    disclaimer in the documentation and/or other materials provided
+    with the distribution.
+
+  * Neither the name of the the Beautiful Soup Consortium and All
+    Night Kosher Bakery nor the names of its contributors may be
+    used to endorse or promote products derived from this software
+    without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, DAMMIT.
+
+"""
+from __future__ import generators
+
+__author__ = "Leonard Richardson (leonardr@segfault.org)"
+__version__ = "3.2.1"
+__copyright__ = "Copyright (c) 2004-2012 Leonard Richardson"
+__license__ = "New-style BSD"
+
+from sgmllib import SGMLParser, SGMLParseError
+import codecs
+import markupbase
+import types
+import re
+import sgmllib
+try:
+  from htmlentitydefs import name2codepoint
+except ImportError:
+  name2codepoint = {}
+try:
+    set
+except NameError:
+    from sets import Set as set
+
+#These hacks make Beautiful Soup able to parse XML with namespaces
+sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')
+markupbase._declname_match = re.compile(r'[a-zA-Z][-_.:a-zA-Z0-9]*\s*').match
+
+DEFAULT_OUTPUT_ENCODING = "utf-8"
+
+def _match_css_class(str):
+    """Build a RE to match the given CSS class."""
+    return re.compile(r"(^|.*\s)%s($|\s)" % str)
+
+# First, the classes that represent markup elements.
+
+class PageElement(object):
+    """Contains the navigational information for some part of the page
+    (either a tag or a piece of text)"""
+
+    def _invert(h):
+        "Cheap function to invert a hash."
+        i = {}
+        for k,v in h.items():
+            i[v] = k
+        return i
+
+    XML_ENTITIES_TO_SPECIAL_CHARS = { "apos" : "'",
+                                      "quot" : '"',
+                                      "amp" : "&",
+                                      "lt" : "<",
+                                      "gt" : ">" }
+
+    XML_SPECIAL_CHARS_TO_ENTITIES = _invert(XML_ENTITIES_TO_SPECIAL_CHARS)
+
+    def setup(self, parent=None, previous=None):
+        """Sets up the initial relations between this element and
+        other elements."""
+        self.parent = parent
+        self.previous = previous
+        self.next = None
+        self.previousSibling = None
+        self.nextSibling = None
+        if self.parent and self.parent.contents:
+            self.previousSibling = self.parent.contents[-1]
+            self.previousSibling.nextSibling = self
+
+    def replaceWith(self, replaceWith):
+        oldParent = self.parent
+        myIndex = self.parent.index(self)
+        if hasattr(replaceWith, "parent")\
+                  and replaceWith.parent is self.parent:
+            # We're replacing this element with one of its siblings.
+            index = replaceWith.parent.index(replaceWith)
+            if index and index < myIndex:
+                # Furthermore, it comes before this element. That
+                # means that when we extract it, the index of this
+                # element will change.
+                myIndex = myIndex - 1
+        self.extract()
+        oldParent.insert(myIndex, replaceWith)
+
+    def replaceWithChildren(self):
+        myParent = self.parent
+        myIndex = self.parent.index(self)
+        self.extract()
+        reversedChildren = list(self.contents)
+        reversedChildren.reverse()
+        for child in reversedChildren:
+            myParent.insert(myIndex, child)
+
+    def extract(self):
+        """Destructively rips this element out of the tree."""
+        if self.parent:
+            try:
+                del self.parent.contents[self.parent.index(self)]
+            except ValueError:
+                pass
+
+        #Find the two elements that would be next to each other if
+        #this element (and any children) hadn't been parsed. Connect
+        #the two.
+        lastChild = self._lastRecursiveChild()
+        nextElement = lastChild.next
+
+        if self.previous:
+            self.previous.next = nextElement
+        if nextElement:
+            nextElement.previous = self.previous
+        self.previous = None
+        lastChild.next = None
+
+        self.parent = None
+        if self.previousSibling:
+            self.previousSibling.nextSibling = self.nextSibling
+        if self.nextSibling:
+            self.nextSibling.previousSibling = self.previousSibling
+        self.previousSibling = self.nextSibling = None
+        return self
+
+    def _lastRecursiveChild(self):
+        "Finds the last element beneath this object to be parsed."
+        lastChild = self
+        while hasattr(lastChild, 'contents') and lastChild.contents:
+            lastChild = lastChild.contents[-1]
+        return lastChild
+
+    def insert(self, position, newChild):
+        if isinstance(newChild, basestring) \
+            and not isinstance(newChild, NavigableString):
+            newChild = NavigableString(newChild)
+
+        position =  min(position, len(self.contents))
+        if hasattr(newChild, 'parent') and newChild.parent is not None:
+            # We're 'inserting' an element that's already one
+            # of this object's children.
+            if newChild.parent is self:
+                index = self.index(newChild)
+                if index > position:
+                    # Furthermore we're moving it further down the
+                    # list of this object's children. That means that
+                    # when we extract this element, our target index
+                    # will jump down one.
+                    position = position - 1
+            newChild.extract()
+
+        newChild.parent = self
+        previousChild = None
+        if position == 0:
+            newChild.previousSibling = None
+            newChild.previous = self
+        else:
+            previousChild = self.contents[position-1]
+            newChild.previousSibling = previousChild
+            newChild.previousSibling.nextSibling = newChild
+            newChild.previous = previousChild._lastRecursiveChild()
+        if newChild.previous:
+            newChild.previous.next = newChild
+
+        newChildsLastElement = newChild._lastRecursiveChild()
+
+        if position >= len(self.contents):
+            newChild.nextSibling = None
+
+            parent = self
+            parentsNextSibling = None
+            while not parentsNextSibling:
+                parentsNextSibling = parent.nextSibling
+                parent = parent.parent
+                if not parent: # This is the last element in the document.
+                    break
+            if parentsNextSibling:
+                newChildsLastElement.next = parentsNextSibling
+            else:
+                newChildsLastElement.next = None
+        else:
+            nextChild = self.contents[position]
+            newChild.nextSibling = nextChild
+            if newChild.nextSibling:
+                newChild.nextSibling.previousSibling = newChild
+            newChildsLastElement.next = nextChild
+
+        if newChildsLastElement.next:
+            newChildsLastElement.next.previous = newChildsLastElement
+        self.contents.insert(position, newChild)
+
+    def append(self, tag):
+        """Appends the given tag to the contents of this tag."""
+        self.insert(len(self.contents), tag)
+
+    def findNext(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the first item that matches the given criteria and
+        appears after this Tag in the document."""
+        return self._findOne(self.findAllNext, name, attrs, text, **kwargs)
+
+    def findAllNext(self, name=None, attrs={}, text=None, limit=None,
+                    **kwargs):
+        """Returns all items that match the given criteria and appear
+        after this Tag in the document."""
+        return self._findAll(name, attrs, text, limit, self.nextGenerator,
+                             **kwargs)
+
+    def findNextSibling(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the closest sibling to this Tag that matches the
+        given criteria and appears after this Tag in the document."""
+        return self._findOne(self.findNextSiblings, name, attrs, text,
+                             **kwargs)
+
+    def findNextSiblings(self, name=None, attrs={}, text=None, limit=None,
+                         **kwargs):
+        """Returns the siblings of this Tag that match the given
+        criteria and appear after this Tag in the document."""
+        return self._findAll(name, attrs, text, limit,
+                             self.nextSiblingGenerator, **kwargs)
+    fetchNextSiblings = findNextSiblings # Compatibility with pre-3.x
+
+    def findPrevious(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the first item that matches the given criteria and
+        appears before this Tag in the document."""
+        return self._findOne(self.findAllPrevious, name, attrs, text, **kwargs)
+
+    def findAllPrevious(self, name=None, attrs={}, text=None, limit=None,
+                        **kwargs):
+        """Returns all items that match the given criteria and appear
+        before this Tag in the document."""
+        return self._findAll(name, attrs, text, limit, self.previousGenerator,
+                           **kwargs)
+    fetchPrevious = findAllPrevious # Compatibility with pre-3.x
+
+    def findPreviousSibling(self, name=None, attrs={}, text=None, **kwargs):
+        """Returns the closest sibling to this Tag that matches the
+        given criteria and appears before this Tag in the document."""
+        return self._findOne(self.findPreviousSiblings, name, attrs, text,
+                             **kwargs)
+
+    def findPreviousSiblings(self, name=None, attrs={}, text=None,
+                             limit=None, **kwargs):
+        """Returns the siblings of this Tag that match the given
+        criteria and appear before this Tag in the document."""
+        return self._findAll(name, attrs, text, limit,
+                             self.previousSiblingGenerator, **kwargs)
+    fetchPreviousSiblings = findPreviousSiblings # Compatibility with pre-3.x
+
+    def findParent(self, name=None, attrs={}, **kwargs):
+        """Returns the closest parent of this Tag that matches the given
+        criteria."""
+        # NOTE: We can't use _findOne because findParents takes a different
+        # set of arguments.
+        r = None
+        l = self.findParents(name, attrs, 1)
+        if l:
+            r = l[0]
+        return r
+
+    def findParents(self, name=None, attrs={}, limit=None, **kwargs):
+        """Returns the parents of this Tag that match the given
+        criteria."""
+
+        return self._findAll(name, attrs, None, limit, self.parentGenerator,
+                             **kwargs)
+    fetchParents = findParents # Compatibility with pre-3.x
+
+    #These methods do the real heavy lifting.
+
+    def _findOne(self, method, name, attrs, text, **kwargs):
+        r = None
+        l = method(name, attrs, text, 1, **kwargs)
+        if l:
+            r = l[0]
+        return r
+
+    def _findAll(self, name, attrs, text, limit, generator, **kwargs):
+        "Iterates over a generator looking for things that match."
+
+        if isinstance(name, SoupStrainer):
+            strainer = name
+        # (Possibly) special case some findAll*(...) searches
+        elif text is None and not limit and not attrs and not kwargs:
+            # findAll*(True)
+            if name is True:
+                return [element for element in generator()
+                        if isinstance(element, Tag)]
+            # findAll*('tag-name')
+            elif isinstance(name, basestring):
+                return [element for element in generator()
+                        if isinstance(element, Tag) and
+                        element.name == name]
+            else:
+                strainer = SoupStrainer(name, attrs, text, **kwargs)
+        # Build a SoupStrainer
+        else:
+            strainer = SoupStrainer(name, attrs, text, **kwargs)
+        results = ResultSet(strainer)
+        g = generator()
+        while True:
+            try:
+                i = g.next()
+            except StopIteration:
+                break
+            if i:
+                found = strainer.search(i)
+                if found:
+                    results.append(found)
+                    if limit and len(results) >= limit:
+                        break
+        return results
+
+    #These Generators can be used to navigate starting from both
+    #NavigableStrings and Tags.
+    def nextGenerator(self):
+        i = self
+        while i is not None:
+            i = i.next
+            yield i
+
+    def nextSiblingGenerator(self):
+        i = self
+        while i is not None:
+            i = i.nextSibling
+            yield i
+
+    def previousGenerator(self):
+        i = self
+        while i is not None:
+            i = i.previous
+            yield i
+
+    def previousSiblingGenerator(self):
+        i = self
+        while i is not None:
+            i = i.previousSibling
+            yield i
+
+    def parentGenerator(self):
+        i = self
+        while i is not None:
+            i = i.parent
+            yield i
+
+    # Utility methods
+    def substituteEncoding(self, str, encoding=None):
+        encoding = encoding or "utf-8"
+        return str.replace("%SOUP-ENCODING%", encoding)
+
+    def toEncoding(self, s, encoding=None):
+        """Encodes an object to a string in some encoding, or to Unicode.
+        ."""
+        if isinstance(s, unicode):
+            if encoding:
+                s = s.encode(encoding)
+        elif isinstance(s, str):
+            if encoding:
+                s = s.encode(encoding)
+            else:
+                s = unicode(s)
+        else:
+            if encoding:
+                s  = self.toEncoding(str(s), encoding)
+            else:
+                s = unicode(s)
+        return s
+
+    BARE_AMPERSAND_OR_BRACKET = re.compile("([<>]|"
+                                           + "&(?!#\d+;|#x[0-9a-fA-F]+;|\w+;)"
+                                           + ")")
+
+    def _sub_entity(self, x):
+        """Used with a regular expression to substitute the
+        appropriate XML entity for an XML special character."""
+        return "&" + self.XML_SPECIAL_CHARS_TO_ENTITIES[x.group(0)[0]] + ";"
+
+
+class NavigableString(unicode, PageElement):
+
+    def __new__(cls, value):
+        """Create a new NavigableString.
+
+        When unpickling a NavigableString, this method is called with
+        the string in DEFAULT_OUTPUT_ENCODING. That encoding needs to be
+        passed in to the superclass's __new__ or the superclass won't know
+        how to handle non-ASCII characters.
+        """
+        if isinstance(value, unicode):
+            return unicode.__new__(cls, value)
+        return unicode.__new__(cls, value, DEFAULT_OUTPUT_ENCODING)
+
+    def __getnewargs__(self):
+        return (NavigableString.__str__(self),)
+
+    def __getattr__(self, attr):
+        """text.string gives you text. This is for backwards
+        compatibility for Navigable*String, but for CData* it lets you
+        get the string without the CData wrapper."""
+        if attr == 'string':
+            return self
+        else:
+            raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__.__name__, attr)
+
+    def __unicode__(self):
+        return str(self).decode(DEFAULT_OUTPUT_ENCODING)
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        # Substitute outgoing XML entities.
+        data = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, self)
+        if encoding:
+            return data.encode(encoding)
+        else:
+            return data
+
+class CData(NavigableString):
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<![CDATA[%s]]>" % NavigableString.__str__(self, encoding)
+
+class ProcessingInstruction(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        output = self
+        if "%SOUP-ENCODING%" in output:
+            output = self.substituteEncoding(output, encoding)
+        return "<?%s?>" % self.toEncoding(output, encoding)
+
+class Comment(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<!--%s-->" % NavigableString.__str__(self, encoding)
+
+class Declaration(NavigableString):
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return "<!%s>" % NavigableString.__str__(self, encoding)
+
+class Tag(PageElement):
+
+    """Represents a found HTML tag with its attributes and contents."""
+
+    def _convertEntities(self, match):
+        """Used in a call to re.sub to replace HTML, XML, and numeric
+        entities with the appropriate Unicode characters. If HTML
+        entities are being converted, any unrecognized entities are
+        escaped."""
+        x = match.group(1)
+        if self.convertHTMLEntities and x in name2codepoint:
+            return unichr(name2codepoint[x])
+        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:
+            if self.convertXMLEntities:
+                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]
+            else:
+                return u'&%s;' % x
+        elif len(x) > 0 and x[0] == '#':
+            # Handle numeric entities
+            if len(x) > 1 and x[1] == 'x':
+                return unichr(int(x[2:], 16))
+            else:
+                return unichr(int(x[1:]))
+
+        elif self.escapeUnrecognizedEntities:
+            return u'&amp;%s;' % x
+        else:
+            return u'&%s;' % x
+
+    def __init__(self, parser, name, attrs=None, parent=None,
+                 previous=None):
+        "Basic constructor."
+
+        # We don't actually store the parser object: that lets extracted
+        # chunks be garbage-collected
+        self.parserClass = parser.__class__
+        self.isSelfClosing = parser.isSelfClosingTag(name)
+        self.name = name
+        if attrs is None:
+            attrs = []
+        elif isinstance(attrs, dict):
+            attrs = attrs.items()
+        self.attrs = attrs
+        self.contents = []
+        self.setup(parent, previous)
+        self.hidden = False
+        self.containsSubstitutions = False
+        self.convertHTMLEntities = parser.convertHTMLEntities
+        self.convertXMLEntities = parser.convertXMLEntities
+        self.escapeUnrecognizedEntities = parser.escapeUnrecognizedEntities
+
+        # Convert any HTML, XML, or numeric entities in the attribute values.
+        convert = lambda(k, val): (k,
+                                   re.sub("&(#\d+|#x[0-9a-fA-F]+|\w+);",
+                                          self._convertEntities,
+                                          val))
+        self.attrs = map(convert, self.attrs)
+
+    def getString(self):
+        if (len(self.contents) == 1
+            and isinstance(self.contents[0], NavigableString)):
+            return self.contents[0]
+
+    def setString(self, string):
+        """Replace the contents of the tag with a string"""
+        self.clear()
+        self.append(string)
+
+    string = property(getString, setString)
+
+    def getText(self, separator=u""):
+        if not len(self.contents):
+            return u""
+        stopNode = self._lastRecursiveChild().next
+        strings = []
+        current = self.contents[0]
+        while current is not stopNode:
+            if isinstance(current, NavigableString):
+                strings.append(current.strip())
+            current = current.next
+        return separator.join(strings)
+
+    text = property(getText)
+
+    def get(self, key, default=None):
+        """Returns the value of the 'key' attribute for the tag, or
+        the value given for 'default' if it doesn't have that
+        attribute."""
+        return self._getAttrMap().get(key, default)
+
+    def clear(self):
+        """Extract all children."""
+        for child in self.contents[:]:
+            child.extract()
+
+    def index(self, element):
+        for i, child in enumerate(self.contents):
+            if child is element:
+                return i
+        raise ValueError("Tag.index: element not in tag")
+
+    def has_key(self, key):
+        return self._getAttrMap().has_key(key)
+
+    def __getitem__(self, key):
+        """tag[key] returns the value of the 'key' attribute for the tag,
+        and throws an exception if it's not there."""
+        return self._getAttrMap()[key]
+
+    def __iter__(self):
+        "Iterating over a tag iterates over its contents."
+        return iter(self.contents)
+
+    def __len__(self):
+        "The length of a tag is the length of its list of contents."
+        return len(self.contents)
+
+    def __contains__(self, x):
+        return x in self.contents
+
+    def __nonzero__(self):
+        "A tag is non-None even if it has no contents."
+        return True
+
+    def __setitem__(self, key, value):
+        """Setting tag[key] sets the value of the 'key' attribute for the
+        tag."""
+        self._getAttrMap()
+        self.attrMap[key] = value
+        found = False
+        for i in range(0, len(self.attrs)):
+            if self.attrs[i][0] == key:
+                self.attrs[i] = (key, value)
+                found = True
+        if not found:
+            self.attrs.append((key, value))
+        self._getAttrMap()[key] = value
+
+    def __delitem__(self, key):
+        "Deleting tag[key] deletes all 'key' attributes for the tag."
+        for item in self.attrs:
+            if item[0] == key:
+                self.attrs.remove(item)
+                #We don't break because bad HTML can define the same
+                #attribute multiple times.
+            self._getAttrMap()
+            if self.attrMap.has_key(key):
+                del self.attrMap[key]
+
+    def __call__(self, *args, **kwargs):
+        """Calling a tag like a function is the same as calling its
+        findAll() method. Eg. tag('a') returns a list of all the A tags
+        found within this tag."""
+        return apply(self.findAll, args, kwargs)
+
+    def __getattr__(self, tag):
+        #print "Getattr %s.%s" % (self.__class__, tag)
+        if len(tag) > 3 and tag.rfind('Tag') == len(tag)-3:
+            return self.find(tag[:-3])
+        elif tag.find('__') != 0:
+            return self.find(tag)
+        raise AttributeError, "'%s' object has no attribute '%s'" % (self.__class__, tag)
+
+    def __eq__(self, other):
+        """Returns true iff this tag has the same name, the same attributes,
+        and the same contents (recursively) as the given tag.
+
+        NOTE: right now this will return false if two tags have the
+        same attributes in a different order. Should this be fixed?"""
+        if other is self:
+            return True
+        if not hasattr(other, 'name') or not hasattr(other, 'attrs') or not hasattr(other, 'contents') or self.name != other.name or self.attrs != other.attrs or len(self) != len(other):
+            return False
+        for i in range(0, len(self.contents)):
+            if self.contents[i] != other.contents[i]:
+                return False
+        return True
+
+    def __ne__(self, other):
+        """Returns true iff this tag is not identical to the other tag,
+        as defined in __eq__."""
+        return not self == other
+
+    def __repr__(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        """Renders this tag as a string."""
+        return self.__str__(encoding)
+
+    def __unicode__(self):
+        return self.__str__(None)
+
+    def __str__(self, encoding=DEFAULT_OUTPUT_ENCODING,
+                prettyPrint=False, indentLevel=0):
+        """Returns a string or Unicode representation of this tag and
+        its contents. To get Unicode, pass None for encoding.
+
+        NOTE: since Python's HTML parser consumes whitespace, this
+        method is not certain to reproduce the whitespace present in
+        the original string."""
+
+        encodedName = self.toEncoding(self.name, encoding)
+
+        attrs = []
+        if self.attrs:
+            for key, val in self.attrs:
+                fmt = '%s="%s"'
+                if isinstance(val, basestring):
+                    if self.containsSubstitutions and '%SOUP-ENCODING%' in val:
+                        val = self.substituteEncoding(val, encoding)
+
+                    # The attribute value either:
+                    #
+                    # * Contains no embedded double quotes or single quotes.
+                    #   No problem: we enclose it in double quotes.
+                    # * Contains embedded single quotes. No problem:
+                    #   double quotes work here too.
+                    # * Contains embedded double quotes. No problem:
+                    #   we enclose it in single quotes.
+                    # * Embeds both single _and_ double quotes. This
+                    #   can't happen naturally, but it can happen if
+                    #   you modify an attribute value after parsing
+                    #   the document. Now we have a bit of a
+                    #   problem. We solve it by enclosing the
+                    #   attribute in single quotes, and escaping any
+                    #   embedded single quotes to XML entities.
+                    if '"' in val:
+                        fmt = "%s='%s'"
+                        if "'" in val:
+                            # TODO: replace with apos when
+                            # appropriate.
+                            val = val.replace("'", "&squot;")
+
+                    # Now we're okay w/r/t quotes. But the attribute
+                    # value might also contain angle brackets, or
+                    # ampersands that aren't part of entities. We need
+                    # to escape those to XML entities too.
+                    val = self.BARE_AMPERSAND_OR_BRACKET.sub(self._sub_entity, val)
+
+                attrs.append(fmt % (self.toEncoding(key, encoding),
+                                    self.toEncoding(val, encoding)))
+        close = ''
+        closeTag = ''
+        if self.isSelfClosing:
+            close = ' /'
+        else:
+            closeTag = '</%s>' % encodedName
+
+        indentTag, indentContents = 0, 0
+        if prettyPrint:
+            indentTag = indentLevel
+            space = (' ' * (indentTag-1))
+            indentContents = indentTag + 1
+        contents = self.renderContents(encoding, prettyPrint, indentContents)
+        if self.hidden:
+            s = contents
+        else:
+            s = []
+            attributeString = ''
+            if attrs:
+                attributeString = ' ' + ' '.join(attrs)
+            if prettyPrint:
+                s.append(space)
+            s.append('<%s%s%s>' % (encodedName, attributeString, close))
+            if prettyPrint:
+                s.append("\n")
+            s.append(contents)
+            if prettyPrint and contents and contents[-1] != "\n":
+                s.append("\n")
+            if prettyPrint and closeTag:
+                s.append(space)
+            s.append(closeTag)
+            if prettyPrint and closeTag and self.nextSibling:
+                s.append("\n")
+            s = ''.join(s)
+        return s
+
+    def decompose(self):
+        """Recursively destroys the contents of this tree."""
+        self.extract()
+        if len(self.contents) == 0:
+            return
+        current = self.contents[0]
+        while current is not None:
+            next = current.next
+            if isinstance(current, Tag):
+                del current.contents[:]
+            current.parent = None
+            current.previous = None
+            current.previousSibling = None
+            current.next = None
+            current.nextSibling = None
+            current = next
+
+    def prettify(self, encoding=DEFAULT_OUTPUT_ENCODING):
+        return self.__str__(encoding, True)
+
+    def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,
+                       prettyPrint=False, indentLevel=0):
+        """Renders the contents of this tag as a string in the given
+        encoding. If encoding is None, returns a Unicode string.."""
+        s=[]
+        for c in self:
+            text = None
+            if isinstance(c, NavigableString):
+                text = c.__str__(encoding)
+            elif isinstance(c, Tag):
+                s.append(c.__str__(encoding, prettyPrint, indentLevel))
+            if text and prettyPrint:
+                text = text.strip()
+            if text:
+                if prettyPrint:
+                    s.append(" " * (indentLevel-1))
+                s.append(text)
+                if prettyPrint:
+                    s.append("\n")
+        return ''.join(s)
+
+    #Soup methods
+
+    def find(self, name=None, attrs={}, recursive=True, text=None,
+             **kwargs):
+        """Return only the first child of this Tag matching the given
+        criteria."""
+        r = None
+        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)
+        if l:
+            r = l[0]
+        return r
+    findChild = find
+
+    def findAll(self, name=None, attrs={}, recursive=True, text=None,
+                limit=None, **kwargs):
+        """Extracts a list of Tag objects that match the given
+        criteria.  You can specify the name of the Tag and any
+        attributes you want the Tag to have.
+
+        The value of a key-value pair in the 'attrs' map can be a
+        string, a list of strings, a regular expression object, or a
+        callable that takes a string and returns whether or not the
+        string matches for some custom definition of 'matches'. The
+        same is true of the tag name."""
+        generator = self.recursiveChildGenerator
+        if not recursive:
+            generator = self.childGenerator
+        return self._findAll(name, attrs, text, limit, generator, **kwargs)
+    findChildren = findAll
+
+    # Pre-3.x compatibility methods
+    first = find
+    fetch = findAll
+
+    def fetchText(self, text=None, recursive=True, limit=None):
+        return self.findAll(text=text, recursive=recursive, limit=limit)
+
+    def firstText(self, text=None, recursive=True):
+        return self.find(text=text, recursive=recursive)
+
+    #Private methods
+
+    def _getAttrMap(self):
+        """Initializes a map representation of this tag's attributes,
+        if not already initialized."""
+        if not getattr(self, 'attrMap'):
+            self.attrMap = {}
+            for (key, value) in self.attrs:
+                self.attrMap[key] = value
+        return self.attrMap
+
+    #Generator methods
+    def childGenerator(self):
+        # Just use the iterator from the contents
+        return iter(self.contents)
+
+    def recursiveChildGenerator(self):
+        if not len(self.contents):
+            raise StopIteration
+        stopNode = self._lastRecursiveChild().next
+        current = self.contents[0]
+        while current is not stopNode:
+            yield current
+            current = current.next
+
+
+# Next, a couple classes to represent queries and their results.
+class SoupStrainer:
+    """Encapsulates a number of ways of matching a markup element (tag or
+    text)."""
+
+    def __init__(self, name=None, attrs={}, text=None, **kwargs):
+        self.name = name
+        if isinstance(attrs, basestring):
+            kwargs['class'] = _match_css_class(attrs)
+            attrs = None
+        if kwargs:
+            if attrs:
+                attrs = attrs.copy()
+                attrs.update(kwargs)
+            else:
+                attrs = kwargs
+        self.attrs = attrs
+        self.text = text
+
+    def __str__(self):
+        if self.text:
+            return self.text
+        else:
+            return "%s|%s" % (self.name, self.attrs)
+
+    def searchTag(self, markupName=None, markupAttrs={}):
+        found = None
+        markup = None
+        if isinstance(markupName, Tag):
+            markup = markupName
+            markupAttrs = markup
+        callFunctionWithTagData = callable(self.name) \
+                                and not isinstance(markupName, Tag)
+
+        if (not self.name) \
+               or callFunctionWithTagData \
+               or (markup and self._matches(markup, self.name)) \
+               or (not markup and self._matches(markupName, self.name)):
+            if callFunctionWithTagData:
+                match = self.name(markupName, markupAttrs)
+            else:
+                match = True
+                markupAttrMap = None
+                for attr, matchAgainst in self.attrs.items():
+                    if not markupAttrMap:
+                         if hasattr(markupAttrs, 'get'):
+                            markupAttrMap = markupAttrs
+                         else:
+                            markupAttrMap = {}
+                            for k,v in markupAttrs:
+                                markupAttrMap[k] = v
+                    attrValue = markupAttrMap.get(attr)
+                    if not self._matches(attrValue, matchAgainst):
+                        match = False
+                        break
+            if match:
+                if markup:
+                    found = markup
+                else:
+                    found = markupName
+        return found
+
+    def search(self, markup):
+        #print 'looking for %s in %s' % (self, markup)
+        found = None
+        # If given a list of items, scan it for a text element that
+        # matches.
+        if hasattr(markup, "__iter__") \
+                and not isinstance(markup, Tag):
+            for element in markup:
+                if isinstance(element, NavigableString) \
+                       and self.search(element):
+                    found = element
+                    break
+        # If it's a Tag, make sure its name or attributes match.
+        # Don't bother with Tags if we're searching for text.
+        elif isinstance(markup, Tag):
+            if not self.text:
+                found = self.searchTag(markup)
+        # If it's text, make sure the text matches.
+        elif isinstance(markup, NavigableString) or \
+                 isinstance(markup, basestring):
+            if self._matches(markup, self.text):
+                found = markup
+        else:
+            raise Exception, "I don't know how to match against a %s" \
+                  % markup.__class__
+        return found
+
+    def _matches(self, markup, matchAgainst):
+        #print "Matching %s against %s" % (markup, matchAgainst)
+        result = False
+        if matchAgainst is True:
+            result = markup is not None
+        elif callable(matchAgainst):
+            result = matchAgainst(markup)
+        else:
+            #Custom match methods take the tag as an argument, but all
+            #other ways of matching match the tag name as a string.
+            if isinstance(markup, Tag):
+                markup = markup.name
+            if markup and not isinstance(markup, basestring):
+                markup = unicode(markup)
+            #Now we know that chunk is either a string, or None.
+            if hasattr(matchAgainst, 'match'):
+                # It's a regexp object.
+                result = markup and matchAgainst.search(markup)
+            elif hasattr(matchAgainst, '__iter__'): # list-like
+                result = markup in matchAgainst
+            elif hasattr(matchAgainst, 'items'):
+                result = markup.has_key(matchAgainst)
+            elif matchAgainst and isinstance(markup, basestring):
+                if isinstance(markup, unicode):
+                    matchAgainst = unicode(matchAgainst)
+                else:
+                    matchAgainst = str(matchAgainst)
+
+            if not result:
+                result = matchAgainst == markup
+        return result
+
+class ResultSet(list):
+    """A ResultSet is just a list that keeps track of the SoupStrainer
+    that created it."""
+    def __init__(self, source):
+        list.__init__([])
+        self.source = source
+
+# Now, some helper functions.
+
+def buildTagMap(default, *args):
+    """Turns a list of maps, lists, or scalars into a single map.
+    Used to build the SELF_CLOSING_TAGS, NESTABLE_TAGS, and
+    NESTING_RESET_TAGS maps out of lists and partial maps."""
+    built = {}
+    for portion in args:
+        if hasattr(portion, 'items'):
+            #It's a map. Merge it.
+            for k,v in portion.items():
+                built[k] = v
+        elif hasattr(portion, '__iter__'): # is a list
+            #It's a list. Map each item to the default.
+            for k in portion:
+                built[k] = default
+        else:
+            #It's a scalar. Map it to the default.
+            built[portion] = default
+    return built
+
+# Now, the parser classes.
+
+class BeautifulStoneSoup(Tag, SGMLParser):
+
+    """This class contains the basic parser and search code. It defines
+    a parser that knows nothing about tag behavior except for the
+    following:
+
+      You can't close a tag without closing all the tags it encloses.
+      That is, "<foo><bar></foo>" actually means
+      "<foo><bar></bar></foo>".
+
+    [Another possible explanation is "<foo><bar /></foo>", but since
+    this class defines no SELF_CLOSING_TAGS, it will never use that
+    explanation.]
+
+    This class is useful for parsing XML or made-up markup languages,
+    or when BeautifulSoup makes an assumption counter to what you were
+    expecting."""
+
+    SELF_CLOSING_TAGS = {}
+    NESTABLE_TAGS = {}
+    RESET_NESTING_TAGS = {}
+    QUOTE_TAGS = {}
+    PRESERVE_WHITESPACE_TAGS = []
+
+    MARKUP_MASSAGE = [(re.compile('(<[^<>]*)/>'),
+                       lambda x: x.group(1) + ' />'),
+                      (re.compile('<!\s+([^<>]*)>'),
+                       lambda x: '<!' + x.group(1) + '>')
+                      ]
+
+    ROOT_TAG_NAME = u'[document]'
+
+    HTML_ENTITIES = "html"
+    XML_ENTITIES = "xml"
+    XHTML_ENTITIES = "xhtml"
+    # TODO: This only exists for backwards-compatibility
+    ALL_ENTITIES = XHTML_ENTITIES
+
+    # Used when determining whether a text node is all whitespace and
+    # can be replaced with a single space. A text node that contains
+    # fancy Unicode spaces (usually non-breaking) should be left
+    # alone.
+    STRIP_ASCII_SPACES = { 9: None, 10: None, 12: None, 13: None, 32: None, }
+
+    def __init__(self, markup="", parseOnlyThese=None, fromEncoding=None,
+                 markupMassage=True, smartQuotesTo=XML_ENTITIES,
+                 convertEntities=None, selfClosingTags=None, isHTML=False):
+        """The Soup object is initialized as the 'root tag', and the
+        provided markup (which can be a string or a file-like object)
+        is fed into the underlying parser.
+
+        sgmllib will process most bad HTML, and the BeautifulSoup
+        class has some tricks for dealing with some HTML that kills
+        sgmllib, but Beautiful Soup can nonetheless choke or lose data
+        if your data uses self-closing tags or declarations
+        incorrectly.
+
+        By default, Beautiful Soup uses regexes to sanitize input,
+        avoiding the vast majority of these problems. If the problems
+        don't apply to you, pass in False for markupMassage, and
+        you'll get better performance.
+
+        The default parser massage techniques fix the two most common
+        instances of invalid HTML that choke sgmllib:
+
+         <br/> (No space between name of closing tag and tag close)
+         <! --Comment--> (Extraneous whitespace in declaration)
+
+        You can pass in a custom list of (RE object, replace method)
+        tuples to get Beautiful Soup to scrub your input the way you
+        want."""
+
+        self.parseOnlyThese = parseOnlyThese
+        self.fromEncoding = fromEncoding
+        self.smartQuotesTo = smartQuotesTo
+        self.convertEntities = convertEntities
+        # Set the rules for how we'll deal with the entities we
+        # encounter
+        if self.convertEntities:
+            # It doesn't make sense to convert encoded characters to
+            # entities even while you're converting entities to Unicode.
+            # Just convert it all to Unicode.
+            self.smartQuotesTo = None
+            if convertEntities == self.HTML_ENTITIES:
+                self.convertXMLEntities = False
+                self.convertHTMLEntities = True
+                self.escapeUnrecognizedEntities = True
+            elif convertEntities == self.XHTML_ENTITIES:
+                self.convertXMLEntities = True
+                self.convertHTMLEntities = True
+                self.escapeUnrecognizedEntities = False
+            elif convertEntities == self.XML_ENTITIES:
+                self.convertXMLEntities = True
+                self.convertHTMLEntities = False
+                self.escapeUnrecognizedEntities = False
+        else:
+            self.convertXMLEntities = False
+            self.convertHTMLEntities = False
+            self.escapeUnrecognizedEntities = False
+
+        self.instanceSelfClosingTags = buildTagMap(None, selfClosingTags)
+        SGMLParser.__init__(self)
+
+        if hasattr(markup, 'read'):        # It's a file-type object.
+            markup = markup.read()
+        self.markup = markup
+        self.markupMassage = markupMassage
+        try:
+            self._feed(isHTML=isHTML)
+        except StopParsing:
+            pass
+        self.markup = None                 # The markup can now be GCed
+
+    def convert_charref(self, name):
+        """This method fixes a bug in Python's SGMLParser."""
+        try:
+            n = int(name)
+        except ValueError:
+            return
+        if not 0 <= n <= 127 : # ASCII ends at 127, not 255
+            return
+        return self.convert_codepoint(n)
+
+    def _feed(self, inDocumentEncoding=None, isHTML=False):
+        # Convert the document to Unicode.
+        markup = self.markup
+        if isinstance(markup, unicode):
+            if not hasattr(self, 'originalEncoding'):
+                self.originalEncoding = None
+        else:
+            dammit = UnicodeDammit\
+                     (markup, [self.fromEncoding, inDocumentEncoding],
+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)
+            markup = dammit.unicode
+            self.originalEncoding = dammit.originalEncoding
+            self.declaredHTMLEncoding = dammit.declaredHTMLEncoding
+        if markup:
+            if self.markupMassage:
+                if not hasattr(self.markupMassage, "__iter__"):
+                    self.markupMassage = self.MARKUP_MASSAGE
+                for fix, m in self.markupMassage:
+                    markup = fix.sub(m, markup)
+                # TODO: We get rid of markupMassage so that the
+                # soup object can be deepcopied later on. Some
+                # Python installations can't copy regexes. If anyone
+                # was relying on the existence of markupMassage, this
+                # might cause problems.
+                del(self.markupMassage)
+        self.reset()
+
+        SGMLParser.feed(self, markup)
+        # Close out any unfinished strings and close all the open tags.
+        self.endData()
+        while self.currentTag.name != self.ROOT_TAG_NAME:
+            self.popTag()
+
+    def __getattr__(self, methodName):
+        """This method routes method call requests to either the SGMLParser
+        superclass or the Tag superclass, depending on the method name."""
+        #print "__getattr__ called on %s.%s" % (self.__class__, methodName)
+
+        if methodName.startswith('start_') or methodName.startswith('end_') \
+               or methodName.startswith('do_'):
+            return SGMLParser.__getattr__(self, methodName)
+        elif not methodName.startswith('__'):
+            return Tag.__getattr__(self, methodName)
+        else:
+            raise AttributeError
+
+    def isSelfClosingTag(self, name):
+        """Returns true iff the given string is the name of a
+        self-closing tag according to this parser."""
+        return self.SELF_CLOSING_TAGS.has_key(name) \
+               or self.instanceSelfClosingTags.has_key(name)
+
+    def reset(self):
+        Tag.__init__(self, self, self.ROOT_TAG_NAME)
+        self.hidden = 1
+        SGMLParser.reset(self)
+        self.currentData = []
+        self.currentTag = None
+        self.tagStack = []
+        self.quoteStack = []
+        self.pushTag(self)
+
+    def popTag(self):
+        tag = self.tagStack.pop()
+
+        #print "Pop", tag.name
+        if self.tagStack:
+            self.currentTag = self.tagStack[-1]
+        return self.currentTag
+
+    def pushTag(self, tag):
+        #print "Push", tag.name
+        if self.currentTag:
+            self.currentTag.contents.append(tag)
+        self.tagStack.append(tag)
+        self.currentTag = self.tagStack[-1]
+
+    def endData(self, containerClass=NavigableString):
+        if self.currentData:
+            currentData = u''.join(self.currentData)
+            if (currentData.translate(self.STRIP_ASCII_SPACES) == '' and
+                not set([tag.name for tag in self.tagStack]).intersection(
+                    self.PRESERVE_WHITESPACE_TAGS)):
+                if '\n' in currentData:
+                    currentData = '\n'
+                else:
+                    currentData = ' '
+            self.currentData = []
+            if self.parseOnlyThese and len(self.tagStack) <= 1 and \
+                   (not self.parseOnlyThese.text or \
+                    not self.parseOnlyThese.search(currentData)):
+                return
+            o = containerClass(currentData)
+            o.setup(self.currentTag, self.previous)
+            if self.previous:
+                self.previous.next = o
+            self.previous = o
+            self.currentTag.contents.append(o)
+
+
+    def _popToTag(self, name, inclusivePop=True):
+        """Pops the tag stack up to and including the most recent
+        instance of the given tag. If inclusivePop is false, pops the tag
+        stack up to but *not* including the most recent instqance of
+        the given tag."""
+        #print "Popping to %s" % name
+        if name == self.ROOT_TAG_NAME:
+            return
+
+        numPops = 0
+        mostRecentTag = None
+        for i in range(len(self.tagStack)-1, 0, -1):
+            if name == self.tagStack[i].name:
+                numPops = len(self.tagStack)-i
+                break
+        if not inclusivePop:
+            numPops = numPops - 1
+
+        for i in range(0, numPops):
+            mostRecentTag = self.popTag()
+        return mostRecentTag
+
+    def _smartPop(self, name):
+
+        """We need to pop up to the previous tag of this type, unless
+        one of this tag's nesting reset triggers comes between this
+        tag and the previous tag of this type, OR unless this tag is a
+        generic nesting trigger and another generic nesting trigger
+        comes between this tag and the previous tag of this type.
+
+        Examples:
+         <p>Foo<b>Bar *<p>* should pop to 'p', not 'b'.
+         <p>Foo<table>Bar *<p>* should pop to 'table', not 'p'.
+         <p>Foo<table><tr>Bar *<p>* should pop to 'tr', not 'p'.
+
+         <li><ul><li> *<li>* should pop to 'ul', not the first 'li'.
+         <tr><table><tr> *<tr>* should pop to 'table', not the first 'tr'
+         <td><tr><td> *<td>* should pop to 'tr', not the first 'td'
+        """
+
+        nestingResetTriggers = self.NESTABLE_TAGS.get(name)
+        isNestable = nestingResetTriggers != None
+        isResetNesting = self.RESET_NESTING_TAGS.has_key(name)
+        popTo = None
+        inclusive = True
+        for i in range(len(self.tagStack)-1, 0, -1):
+            p = self.tagStack[i]
+            if (not p or p.name == name) and not isNestable:
+                #Non-nestable tags get popped to the top or to their
+                #last occurance.
+                popTo = name
+                break
+            if (nestingResetTriggers is not None
+                and p.name in nestingResetTriggers) \
+                or (nestingResetTriggers is None and isResetNesting
+                    and self.RESET_NESTING_TAGS.has_key(p.name)):
+
+                #If we encounter one of the nesting reset triggers
+                #peculiar to this tag, or we encounter another tag
+                #that causes nesting to reset, pop up to but not
+                #including that tag.
+                popTo = p.name
+                inclusive = False
+                break
+            p = p.parent
+        if popTo:
+            self._popToTag(popTo, inclusive)
+
+    def unknown_starttag(self, name, attrs, selfClosing=0):
+        #print "Start tag %s: %s" % (name, attrs)
+        if self.quoteStack:
+            #This is not a real tag.
+            #print "<%s> is not real!" % name
+            attrs = ''.join([' %s="%s"' % (x, y) for x, y in attrs])
+            self.handle_data('<%s%s>' % (name, attrs))
+            return
+        self.endData()
+
+        if not self.isSelfClosingTag(name) and not selfClosing:
+            self._smartPop(name)
+
+        if self.parseOnlyThese and len(self.tagStack) <= 1 \
+               and (self.parseOnlyThese.text or not self.parseOnlyThese.searchTag(name, attrs)):
+            return
+
+        tag = Tag(self, name, attrs, self.currentTag, self.previous)
+        if self.previous:
+            self.previous.next = tag
+        self.previous = tag
+        self.pushTag(tag)
+        if selfClosing or self.isSelfClosingTag(name):
+            self.popTag()
+        if name in self.QUOTE_TAGS:
+            #print "Beginning quote (%s)" % name
+            self.quoteStack.append(name)
+            self.literal = 1
+        return tag
+
+    def unknown_endtag(self, name):
+        #print "End tag %s" % name
+        if self.quoteStack and self.quoteStack[-1] != name:
+            #This is not a real end tag.
+            #print "</%s> is not real!" % name
+            self.handle_data('</%s>' % name)
+            return
+        self.endData()
+        self._popToTag(name)
+        if self.quoteStack and self.quoteStack[-1] == name:
+            self.quoteStack.pop()
+            self.literal = (len(self.quoteStack) > 0)
+
+    def handle_data(self, data):
+        self.currentData.append(data)
+
+    def _toStringSubclass(self, text, subclass):
+        """Adds a certain piece of text to the tree as a NavigableString
+        subclass."""
+        self.endData()
+        self.handle_data(text)
+        self.endData(subclass)
+
+    def handle_pi(self, text):
+        """Handle a processing instruction as a ProcessingInstruction
+        object, possibly one with a %SOUP-ENCODING% slot into which an
+        encoding will be plugged later."""
+        if text[:3] == "xml":
+            text = u"xml version='1.0' encoding='%SOUP-ENCODING%'"
+        self._toStringSubclass(text, ProcessingInstruction)
+
+    def handle_comment(self, text):
+        "Handle comments as Comment objects."
+        self._toStringSubclass(text, Comment)
+
+    def handle_charref(self, ref):
+        "Handle character references as data."
+        if self.convertEntities:
+            data = unichr(int(ref))
+        else:
+            data = '&#%s;' % ref
+        self.handle_data(data)
+
+    def handle_entityref(self, ref):
+        """Handle entity references as data, possibly converting known
+        HTML and/or XML entity references to the corresponding Unicode
+        characters."""
+        data = None
+        if self.convertHTMLEntities:
+            try:
+                data = unichr(name2codepoint[ref])
+            except KeyError:
+                pass
+
+        if not data and self.convertXMLEntities:
+                data = self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref)
+
+        if not data and self.convertHTMLEntities and \
+            not self.XML_ENTITIES_TO_SPECIAL_CHARS.get(ref):
+                # TODO: We've got a problem here. We're told this is
+                # an entity reference, but it's not an XML entity
+                # reference or an HTML entity reference. Nonetheless,
+                # the logical thing to do is to pass it through as an
+                # unrecognized entity reference.
+                #
+                # Except: when the input is "&carol;" this function
+                # will be called with input "carol". When the input is
+                # "AT&T", this function will be called with input
+                # "T". We have no way of knowing whether a semicolon
+                # was present originally, so we don't know whether
+                # this is an unknown entity or just a misplaced
+                # ampersand.
+                #
+                # The more common case is a misplaced ampersand, so I
+                # escape the ampersand and omit the trailing semicolon.
+                data = "&amp;%s" % ref
+        if not data:
+            # This case is different from the one above, because we
+            # haven't already gone through a supposedly comprehensive
+            # mapping of entities to Unicode characters. We might not
+            # have gone through any mapping at all. So the chances are
+            # very high that this is a real entity, and not a
+            # misplaced ampersand.
+            data = "&%s;" % ref
+        self.handle_data(data)
+
+    def handle_decl(self, data):
+        "Handle DOCTYPEs and the like as Declaration objects."
+        self._toStringSubclass(data, Declaration)
+
+    def parse_declaration(self, i):
+        """Treat a bogus SGML declaration as raw data. Treat a CDATA
+        declaration as a CData object."""
+        j = None
+        if self.rawdata[i:i+9] == '<![CDATA[':
+             k = self.rawdata.find(']]>', i)
+             if k == -1:
+                 k = len(self.rawdata)
+             data = self.rawdata[i+9:k]
+             j = k+3
+             self._toStringSubclass(data, CData)
+        else:
+            try:
+                j = SGMLParser.parse_declaration(self, i)
+            except SGMLParseError:
+                toHandle = self.rawdata[i:]
+                self.handle_data(toHandle)
+                j = i + len(toHandle)
+        return j
+
+class BeautifulSoup(BeautifulStoneSoup):
+
+    """This parser knows the following facts about HTML:
+
+    * Some tags have no closing tag and should be interpreted as being
+      closed as soon as they are encountered.
+
+    * The text inside some tags (ie. 'script') may contain tags which
+      are not really part of the document and which should be parsed
+      as text, not tags. If you want to parse the text as tags, you can
+      always fetch it and parse it explicitly.
+
+    * Tag nesting rules:
+
+      Most tags can't be nested at all. For instance, the occurance of
+      a <p> tag should implicitly close the previous <p> tag.
+
+       <p>Para1<p>Para2
+        should be transformed into:
+       <p>Para1</p><p>Para2
+
+      Some tags can be nested arbitrarily. For instance, the occurance
+      of a <blockquote> tag should _not_ implicitly close the previous
+      <blockquote> tag.
+
+       Alice said: <blockquote>Bob said: <blockquote>Blah
+        should NOT be transformed into:
+       Alice said: <blockquote>Bob said: </blockquote><blockquote>Blah
+
+      Some tags can be nested, but the nesting is reset by the
+      interposition of other tags. For instance, a <tr> tag should
+      implicitly close the previous <tr> tag within the same <table>,
+      but not close a <tr> tag in another table.
+
+       <table><tr>Blah<tr>Blah
+        should be transformed into:
+       <table><tr>Blah</tr><tr>Blah
+        but,
+       <tr>Blah<table><tr>Blah
+        should NOT be transformed into
+       <tr>Blah<table></tr><tr>Blah
+
+    Differing assumptions about tag nesting rules are a major source
+    of problems with the BeautifulSoup class. If BeautifulSoup is not
+    treating as nestable a tag your page author treats as nestable,
+    try ICantBelieveItsBeautifulSoup, MinimalSoup, or
+    BeautifulStoneSoup before writing your own subclass."""
+
+    def __init__(self, *args, **kwargs):
+        if not kwargs.has_key('smartQuotesTo'):
+            kwargs['smartQuotesTo'] = self.HTML_ENTITIES
+        kwargs['isHTML'] = True
+        BeautifulStoneSoup.__init__(self, *args, **kwargs)
+
+    SELF_CLOSING_TAGS = buildTagMap(None,
+                                    ('br' , 'hr', 'input', 'img', 'meta',
+                                    'spacer', 'link', 'frame', 'base', 'col'))
+
+    PRESERVE_WHITESPACE_TAGS = set(['pre', 'textarea'])
+
+    QUOTE_TAGS = {'script' : None, 'textarea' : None}
+
+    #According to the HTML standard, each of these inline tags can
+    #contain another tag of the same type. Furthermore, it's common
+    #to actually use these tags this way.
+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',
+                            'center')
+
+    #According to the HTML standard, these block tags can contain
+    #another tag of the same type. Furthermore, it's common
+    #to actually use these tags this way.
+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')
+
+    #Lists can contain other lists, but there are restrictions.
+    NESTABLE_LIST_TAGS = { 'ol' : [],
+                           'ul' : [],
+                           'li' : ['ul', 'ol'],
+                           'dl' : [],
+                           'dd' : ['dl'],
+                           'dt' : ['dl'] }
+
+    #Tables can contain other tables, but there are restrictions.
+    NESTABLE_TABLE_TAGS = {'table' : [],
+                           'tr' : ['table', 'tbody', 'tfoot', 'thead'],
+                           'td' : ['tr'],
+                           'th' : ['tr'],
+                           'thead' : ['table'],
+                           'tbody' : ['table'],
+                           'tfoot' : ['table'],
+                           }
+
+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')
+
+    #If one of these tags is encountered, all tags up to the next tag of
+    #this type are popped.
+    RESET_NESTING_TAGS = buildTagMap(None, NESTABLE_BLOCK_TAGS, 'noscript',
+                                     NON_NESTABLE_BLOCK_TAGS,
+                                     NESTABLE_LIST_TAGS,
+                                     NESTABLE_TABLE_TAGS)
+
+    NESTABLE_TAGS = buildTagMap([], NESTABLE_INLINE_TAGS, NESTABLE_BLOCK_TAGS,
+                                NESTABLE_LIST_TAGS, NESTABLE_TABLE_TAGS)
+
+    # Used to detect the charset in a META tag; see start_meta
+    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)", re.M)
+
+    def start_meta(self, attrs):
+        """Beautiful Soup can detect a charset included in a META tag,
+        try to convert the document to that charset, and re-parse the
+        document from the beginning."""
+        httpEquiv = None
+        contentType = None
+        contentTypeIndex = None
+        tagNeedsEncodingSubstitution = False
+
+        for i in range(0, len(attrs)):
+            key, value = attrs[i]
+            key = key.lower()
+            if key == 'http-equiv':
+                httpEquiv = value
+            elif key == 'content':
+                contentType = value
+                contentTypeIndex = i
+
+        if httpEquiv and contentType: # It's an interesting meta tag.
+            match = self.CHARSET_RE.search(contentType)
+            if match:
+                if (self.declaredHTMLEncoding is not None or
+                    self.originalEncoding == self.fromEncoding):
+                    # An HTML encoding was sniffed while converting
+                    # the document to Unicode, or an HTML encoding was
+                    # sniffed during a previous pass through the
+                    # document, or an encoding was specified
+                    # explicitly and it worked. Rewrite the meta tag.
+                    def rewrite(match):
+                        return match.group(1) + "%SOUP-ENCODING%"
+                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)
+                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],
+                                               newAttr)
+                    tagNeedsEncodingSubstitution = True
+                else:
+                    # This is our first pass through the document.
+                    # Go through it again with the encoding information.
+                    newCharset = match.group(3)
+                    if newCharset and newCharset != self.originalEncoding:
+                        self.declaredHTMLEncoding = newCharset
+                        self._feed(self.declaredHTMLEncoding)
+                        raise StopParsing
+                    pass
+        tag = self.unknown_starttag("meta", attrs)
+        if tag and tagNeedsEncodingSubstitution:
+            tag.containsSubstitutions = True
+
+class StopParsing(Exception):
+    pass
+
+class ICantBelieveItsBeautifulSoup(BeautifulSoup):
+
+    """The BeautifulSoup class is oriented towards skipping over
+    common HTML errors like unclosed tags. However, sometimes it makes
+    errors of its own. For instance, consider this fragment:
+
+     <b>Foo<b>Bar</b></b>
+
+    This is perfectly valid (if bizarre) HTML. However, the
+    BeautifulSoup class will implicitly close the first b tag when it
+    encounters the second 'b'. It will think the author wrote
+    "<b>Foo<b>Bar", and didn't close the first 'b' tag, because
+    there's no real-world reason to bold something that's already
+    bold. When it encounters '</b></b>' it will close two more 'b'
+    tags, for a grand total of three tags closed instead of two. This
+    can throw off the rest of your document structure. The same is
+    true of a number of other tags, listed below.
+
+    It's much more common for someone to forget to close a 'b' tag
+    than to actually use nested 'b' tags, and the BeautifulSoup class
+    handles the common case. This class handles the not-co-common
+    case: where you can't believe someone wrote what they did, but
+    it's valid HTML and BeautifulSoup screwed up by assuming it
+    wouldn't be."""
+
+    I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS = \
+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
+      'cite', 'code', 'dfn', 'kbd', 'samp', 'strong', 'var', 'b',
+      'big')
+
+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)
+
+    NESTABLE_TAGS = buildTagMap([], BeautifulSoup.NESTABLE_TAGS,
+                                I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS,
+                                I_CANT_BELIEVE_THEYRE_NESTABLE_INLINE_TAGS)
+
+class MinimalSoup(BeautifulSoup):
+    """The MinimalSoup class is for parsing HTML that contains
+    pathologically bad markup. It makes no assumptions about tag
+    nesting, but it does know which tags are self-closing, that
+    <script> tags contain Javascript and should not be parsed, that
+    META tags may contain encoding information, and so on.
+
+    This also makes it better for subclassing than BeautifulStoneSoup
+    or BeautifulSoup."""
+
+    RESET_NESTING_TAGS = buildTagMap('noscript')
+    NESTABLE_TAGS = {}
+
+class BeautifulSOAP(BeautifulStoneSoup):
+    """This class will push a tag with only a single string child into
+    the tag's parent as an attribute. The attribute's name is the tag
+    name, and the value is the string child. An example should give
+    the flavor of the change:
+
+    <foo><bar>baz</bar></foo>
+     =>
+    <foo bar="baz"><bar>baz</bar></foo>
+
+    You can then access fooTag['bar'] instead of fooTag.barTag.string.
+
+    This is, of course, useful for scraping structures that tend to
+    use subelements instead of attributes, such as SOAP messages. Note
+    that it modifies its input, so don't print the modified version
+    out.
+
+    I'm not sure how many people really want to use this class; let me
+    know if you do. Mainly I like the name."""
+
+    def popTag(self):
+        if len(self.tagStack) > 1:
+            tag = self.tagStack[-1]
+            parent = self.tagStack[-2]
+            parent._getAttrMap()
+            if (isinstance(tag, Tag) and len(tag.contents) == 1 and
+                isinstance(tag.contents[0], NavigableString) and
+                not parent.attrMap.has_key(tag.name)):
+                parent[tag.name] = tag.contents[0]
+        BeautifulStoneSoup.popTag(self)
+
+#Enterprise class names! It has come to our attention that some people
+#think the names of the Beautiful Soup parser classes are too silly
+#and "unprofessional" for use in enterprise screen-scraping. We feel
+#your pain! For such-minded folk, the Beautiful Soup Consortium And
+#All-Night Kosher Bakery recommends renaming this file to
+#"RobustParser.py" (or, in cases of extreme enterprisiness,
+#"RobustParserBeanInterface.class") and using the following
+#enterprise-friendly class aliases:
+class RobustXMLParser(BeautifulStoneSoup):
+    pass
+class RobustHTMLParser(BeautifulSoup):
+    pass
+class RobustWackAssHTMLParser(ICantBelieveItsBeautifulSoup):
+    pass
+class RobustInsanelyWackAssHTMLParser(MinimalSoup):
+    pass
+class SimplifyingSOAPParser(BeautifulSOAP):
+    pass
+
+######################################################
+#
+# Bonus library: Unicode, Dammit
+#
+# This class forces XML data into a standard format (usually to UTF-8
+# or Unicode).  It is heavily based on code from Mark Pilgrim's
+# Universal Feed Parser. It does not rewrite the XML or HTML to
+# reflect a new encoding: that happens in BeautifulStoneSoup.handle_pi
+# (XML) and BeautifulSoup.start_meta (HTML).
+
+# Autodetects character encodings.
+# Download from http://chardet.feedparser.org/
+try:
+    import chardet
+#    import chardet.constants
+#    chardet.constants._debug = 1
+except ImportError:
+    chardet = None
+
+# cjkcodecs and iconv_codec make Python know about more character encodings.
+# Both are available from http://cjkpython.i18n.org/
+# They're built in if you use Python 2.4.
+try:
+    import cjkcodecs.aliases
+except ImportError:
+    pass
+try:
+    import iconv_codec
+except ImportError:
+    pass
+
+class UnicodeDammit:
+    """A class for detecting the encoding of a *ML document and
+    converting it to a Unicode string. If the source encoding is
+    windows-1252, can replace MS smart quotes with their HTML or XML
+    equivalents."""
+
+    # This dictionary maps commonly seen values for "charset" in HTML
+    # meta tags to the corresponding Python codec names. It only covers
+    # values that aren't in Python's aliases and can't be determined
+    # by the heuristics in find_codec.
+    CHARSET_ALIASES = { "macintosh" : "mac-roman",
+                        "x-sjis" : "shift-jis" }
+
+    def __init__(self, markup, overrideEncodings=[],
+                 smartQuotesTo='xml', isHTML=False):
+        self.declaredHTMLEncoding = None
+        self.markup, documentEncoding, sniffedEncoding = \
+                     self._detectEncoding(markup, isHTML)
+        self.smartQuotesTo = smartQuotesTo
+        self.triedEncodings = []
+        if markup == '' or isinstance(markup, unicode):
+            self.originalEncoding = None
+            self.unicode = unicode(markup)
+            return
+
+        u = None
+        for proposedEncoding in overrideEncodings:
+            u = self._convertFrom(proposedEncoding)
+            if u: break
+        if not u:
+            for proposedEncoding in (documentEncoding, sniffedEncoding):
+                u = self._convertFrom(proposedEncoding)
+                if u: break
+
+        # If no luck and we have auto-detection library, try that:
+        if not u and chardet and not isinstance(self.markup, unicode):
+            u = self._convertFrom(chardet.detect(self.markup)['encoding'])
+
+        # As a last resort, try utf-8 and windows-1252:
+        if not u:
+            for proposed_encoding in ("utf-8", "windows-1252"):
+                u = self._convertFrom(proposed_encoding)
+                if u: break
+
+        self.unicode = u
+        if not u: self.originalEncoding = None
+
+    def _subMSChar(self, orig):
+        """Changes a MS smart quote character to an XML or HTML
+        entity."""
+        sub = self.MS_CHARS.get(orig)
+        if isinstance(sub, tuple):
+            if self.smartQuotesTo == 'xml':
+                sub = '&#x%s;' % sub[1]
+            else:
+                sub = '&%s;' % sub[0]
+        return sub
+
+    def _convertFrom(self, proposed):
+        proposed = self.find_codec(proposed)
+        if not proposed or proposed in self.triedEncodings:
+            return None
+        self.triedEncodings.append(proposed)
+        markup = self.markup
+
+        # Convert smart quotes to HTML if coming from an encoding
+        # that might have them.
+        if self.smartQuotesTo and proposed.lower() in("windows-1252",
+                                                      "iso-8859-1",
+                                                      "iso-8859-2"):
+            markup = re.compile("([\x80-\x9f])").sub \
+                     (lambda(x): self._subMSChar(x.group(1)),
+                      markup)
+
+        try:
+            # print "Trying to convert document to %s" % proposed
+            u = self._toUnicode(markup, proposed)
+            self.markup = u
+            self.originalEncoding = proposed
+        except Exception, e:
+            # print "That didn't work!"
+            # print e
+            return None
+        #print "Correct encoding: %s" % proposed
+        return self.markup
+
+    def _toUnicode(self, data, encoding):
+        '''Given a string and its encoding, decodes the string into Unicode.
+        %encoding is a string recognized by encodings.aliases'''
+
+        # strip Byte Order Mark (if present)
+        if (len(data) >= 4) and (data[:2] == '\xfe\xff') \
+               and (data[2:4] != '\x00\x00'):
+            encoding = 'utf-16be'
+            data = data[2:]
+        elif (len(data) >= 4) and (data[:2] == '\xff\xfe') \
+                 and (data[2:4] != '\x00\x00'):
+            encoding = 'utf-16le'
+            data = data[2:]
+        elif data[:3] == '\xef\xbb\xbf':
+            encoding = 'utf-8'
+            data = data[3:]
+        elif data[:4] == '\x00\x00\xfe\xff':
+            encoding = 'utf-32be'
+            data = data[4:]
+        elif data[:4] == '\xff\xfe\x00\x00':
+            encoding = 'utf-32le'
+            data = data[4:]
+        newdata = unicode(data, encoding)
+        return newdata
+
+    def _detectEncoding(self, xml_data, isHTML=False):
+        """Given a document, tries to detect its XML encoding."""
+        xml_encoding = sniffed_xml_encoding = None
+        try:
+            if xml_data[:4] == '\x4c\x6f\xa7\x94':
+                # EBCDIC
+                xml_data = self._ebcdic_to_ascii(xml_data)
+            elif xml_data[:4] == '\x00\x3c\x00\x3f':
+                # UTF-16BE
+                sniffed_xml_encoding = 'utf-16be'
+                xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') \
+                     and (xml_data[2:4] != '\x00\x00'):
+                # UTF-16BE with BOM
+                sniffed_xml_encoding = 'utf-16be'
+                xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
+            elif xml_data[:4] == '\x3c\x00\x3f\x00':
+                # UTF-16LE
+                sniffed_xml_encoding = 'utf-16le'
+                xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
+            elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and \
+                     (xml_data[2:4] != '\x00\x00'):
+                # UTF-16LE with BOM
+                sniffed_xml_encoding = 'utf-16le'
+                xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
+            elif xml_data[:4] == '\x00\x00\x00\x3c':
+                # UTF-32BE
+                sniffed_xml_encoding = 'utf-32be'
+                xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
+            elif xml_data[:4] == '\x3c\x00\x00\x00':
+                # UTF-32LE
+                sniffed_xml_encoding = 'utf-32le'
+                xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
+            elif xml_data[:4] == '\x00\x00\xfe\xff':
+                # UTF-32BE with BOM
+                sniffed_xml_encoding = 'utf-32be'
+                xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
+            elif xml_data[:4] == '\xff\xfe\x00\x00':
+                # UTF-32LE with BOM
+                sniffed_xml_encoding = 'utf-32le'
+                xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
+            elif xml_data[:3] == '\xef\xbb\xbf':
+                # UTF-8 with BOM
+                sniffed_xml_encoding = 'utf-8'
+                xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
+            else:
+                sniffed_xml_encoding = 'ascii'
+                pass
+        except:
+            xml_encoding_match = None
+        xml_encoding_match = re.compile(
+            '^<\?.*encoding=[\'"](.*?)[\'"].*\?>').match(xml_data)
+        if not xml_encoding_match and isHTML:
+            regexp = re.compile('<\s*meta[^>]+charset=([^>]*?)[;\'">]', re.I)
+            xml_encoding_match = regexp.search(xml_data)
+        if xml_encoding_match is not None:
+            xml_encoding = xml_encoding_match.groups()[0].lower()
+            if isHTML:
+                self.declaredHTMLEncoding = xml_encoding
+            if sniffed_xml_encoding and \
+               (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode',
+                                 'iso-10646-ucs-4', 'ucs-4', 'csucs4',
+                                 'utf-16', 'utf-32', 'utf_16', 'utf_32',
+                                 'utf16', 'u16')):
+                xml_encoding = sniffed_xml_encoding
+        return xml_data, xml_encoding, sniffed_xml_encoding
+
+
+    def find_codec(self, charset):
+        return self._codec(self.CHARSET_ALIASES.get(charset, charset)) \
+               or (charset and self._codec(charset.replace("-", ""))) \
+               or (charset and self._codec(charset.replace("-", "_"))) \
+               or charset
+
+    def _codec(self, charset):
+        if not charset: return charset
+        codec = None
+        try:
+            codecs.lookup(charset)
+            codec = charset
+        except (LookupError, ValueError):
+            pass
+        return codec
+
+    EBCDIC_TO_ASCII_MAP = None
+    def _ebcdic_to_ascii(self, s):
+        c = self.__class__
+        if not c.EBCDIC_TO_ASCII_MAP:
+            emap = (0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,
+                    16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,
+                    128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,
+                    144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,
+                    32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,
+                    38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,
+                    45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,
+                    186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,
+                    195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,
+                    201,202,106,107,108,109,110,111,112,113,114,203,204,205,
+                    206,207,208,209,126,115,116,117,118,119,120,121,122,210,
+                    211,212,213,214,215,216,217,218,219,220,221,222,223,224,
+                    225,226,227,228,229,230,231,123,65,66,67,68,69,70,71,72,
+                    73,232,233,234,235,236,237,125,74,75,76,77,78,79,80,81,
+                    82,238,239,240,241,242,243,92,159,83,84,85,86,87,88,89,
+                    90,244,245,246,247,248,249,48,49,50,51,52,53,54,55,56,57,
+                    250,251,252,253,254,255)
+            import string
+            c.EBCDIC_TO_ASCII_MAP = string.maketrans( \
+            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
+        return s.translate(c.EBCDIC_TO_ASCII_MAP)
+
+    MS_CHARS = { '\x80' : ('euro', '20AC'),
+                 '\x81' : ' ',
+                 '\x82' : ('sbquo', '201A'),
+                 '\x83' : ('fnof', '192'),
+                 '\x84' : ('bdquo', '201E'),
+                 '\x85' : ('hellip', '2026'),
+                 '\x86' : ('dagger', '2020'),
+                 '\x87' : ('Dagger', '2021'),
+                 '\x88' : ('circ', '2C6'),
+                 '\x89' : ('permil', '2030'),
+                 '\x8A' : ('Scaron', '160'),
+                 '\x8B' : ('lsaquo', '2039'),
+                 '\x8C' : ('OElig', '152'),
+                 '\x8D' : '?',
+                 '\x8E' : ('#x17D', '17D'),
+                 '\x8F' : '?',
+                 '\x90' : '?',
+                 '\x91' : ('lsquo', '2018'),
+                 '\x92' : ('rsquo', '2019'),
+                 '\x93' : ('ldquo', '201C'),
+                 '\x94' : ('rdquo', '201D'),
+                 '\x95' : ('bull', '2022'),
+                 '\x96' : ('ndash', '2013'),
+                 '\x97' : ('mdash', '2014'),
+                 '\x98' : ('tilde', '2DC'),
+                 '\x99' : ('trade', '2122'),
+                 '\x9a' : ('scaron', '161'),
+                 '\x9b' : ('rsaquo', '203A'),
+                 '\x9c' : ('oelig', '153'),
+                 '\x9d' : '?',
+                 '\x9e' : ('#x17E', '17E'),
+                 '\x9f' : ('Yuml', ''),}
+
+#######################################################################
+
+
+#By default, act as an HTML pretty-printer.
+if __name__ == '__main__':
+    import sys
+    soup = BeautifulSoup(sys.stdin)
+    print soup.prettify()
diff -rupN grabber-original/BeautifulSoupTests.py grabber-new/BeautifulSoupTests.py
--- grabber-original/BeautifulSoupTests.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/BeautifulSoupTests.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,902 @@
+# -*- coding: utf-8 -*-
+"""Unit tests for Beautiful Soup.
+
+These tests make sure the Beautiful Soup works as it should. If you
+find a bug in Beautiful Soup, the best way to express it is as a test
+case like this that fails."""
+
+import unittest
+from BeautifulSoup import *
+
+class SoupTest(unittest.TestCase):
+
+    def assertSoupEquals(self, toParse, rep=None, c=BeautifulSoup):
+        """Parse the given text and make sure its string rep is the other
+        given text."""
+        if rep == None:
+            rep = toParse
+        self.assertEqual(str(c(toParse)), rep)
+
+
+class FollowThatTag(SoupTest):
+
+    "Tests the various ways of fetching tags from a soup."
+
+    def setUp(self):
+        ml = """
+        <a id="x">1</a>
+        <A id="a">2</a>
+        <b id="b">3</a>
+        <b href="foo" id="x">4</a>
+        <ac width=100>4</ac>"""
+        self.soup = BeautifulStoneSoup(ml)
+
+    def testFindAllByName(self):
+        matching = self.soup('a')
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+        self.assertEqual(matching, self.soup.findAll('a'))
+        self.assertEqual(matching, self.soup.findAll(SoupStrainer('a')))
+
+    def testFindAllByAttribute(self):
+        matching = self.soup.findAll(id='x')
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+        self.assertEqual(matching[1].name, 'b')
+
+        matching2 = self.soup.findAll(attrs={'id' : 'x'})
+        self.assertEqual(matching, matching2)
+
+        strainer = SoupStrainer(attrs={'id' : 'x'})
+        self.assertEqual(matching, self.soup.findAll(strainer))
+
+        self.assertEqual(len(self.soup.findAll(id=None)), 1)
+
+        self.assertEqual(len(self.soup.findAll(width=100)), 1)
+        self.assertEqual(len(self.soup.findAll(junk=None)), 5)
+        self.assertEqual(len(self.soup.findAll(junk=[1, None])), 5)
+
+        self.assertEqual(len(self.soup.findAll(junk=re.compile('.*'))), 0)
+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)
+
+        self.assertEqual(len(self.soup.findAll(junk=True)), 0)
+        self.assertEqual(len(self.soup.findAll(href=True)), 1)
+
+    def testFindallByClass(self):
+        soup = BeautifulSoup('<b class="foo">Foo</b><a class="1 23 4">Bar</a>')
+        self.assertEqual(soup.find(attrs='foo').string, "Foo")
+        self.assertEqual(soup.find('a', '1').string, "Bar")
+        self.assertEqual(soup.find('a', '23').string, "Bar")
+        self.assertEqual(soup.find('a', '4').string, "Bar")
+
+        self.assertEqual(soup.find('a', '2'), None)
+
+    def testFindAllByList(self):
+        matching = self.soup(['a', 'ac'])
+        self.assertEqual(len(matching), 3)
+
+    def testFindAllByHash(self):
+        matching = self.soup({'a' : True, 'b' : True})
+        self.assertEqual(len(matching), 4)
+
+    def testFindAllText(self):
+        soup = BeautifulSoup("<html>\xbb</html>")
+        self.assertEqual(soup.findAll(text=re.compile('.*')),
+                         [u'\xbb'])
+
+    def testFindAllByRE(self):
+        import re
+        r = re.compile('a.*')
+        self.assertEqual(len(self.soup(r)), 3)
+
+    def testFindAllByMethod(self):
+        def matchTagWhereIDMatchesName(tag):
+            return tag.name == tag.get('id')
+
+        matching = self.soup.findAll(matchTagWhereIDMatchesName)
+        self.assertEqual(len(matching), 2)
+        self.assertEqual(matching[0].name, 'a')
+
+    def testFindByIndex(self):
+        """For when you have the tag and you want to know where it is."""
+        tag = self.soup.find('a', id="a")
+        self.assertEqual(self.soup.index(tag), 3)
+
+        # It works for NavigableStrings as well.
+        s = tag.string
+        self.assertEqual(tag.index(s), 0)
+
+        # If the tag isn't present, a ValueError is raised.
+        soup2 = BeautifulSoup("<b></b>")
+        tag2 = soup2.find('b')
+        self.assertRaises(ValueError, self.soup.index, tag2)
+
+    def testConflictingFindArguments(self):
+        """The 'text' argument takes precedence."""
+        soup = BeautifulSoup('Foo<b>Bar</b>Baz')
+        self.assertEqual(soup.find('b', text='Baz'), 'Baz')
+        self.assertEqual(soup.findAll('b', text='Baz'), ['Baz'])
+
+        self.assertEqual(soup.find(True, text='Baz'), 'Baz')
+        self.assertEqual(soup.findAll(True, text='Baz'), ['Baz'])
+
+    def testParents(self):
+        soup = BeautifulSoup('<ul id="foo"></ul><ul id="foo"><ul><ul id="foo" a="b"><b>Blah')
+        b = soup.b
+        self.assertEquals(len(b.findParents('ul', {'id' : 'foo'})), 2)
+        self.assertEquals(b.findParent('ul')['a'], 'b')
+
+    PROXIMITY_TEST = BeautifulSoup('<b id="1"><b id="2"><b id="3"><b id="4">')
+
+    def testNext(self):
+        soup = self.PROXIMITY_TEST
+        b = soup.find('b', {'id' : 2})
+        self.assertEquals(b.findNext('b')['id'], '3')
+        self.assertEquals(b.findNext('b')['id'], '3')
+        self.assertEquals(len(b.findAllNext('b')), 2)
+        self.assertEquals(len(b.findAllNext('b', {'id' : 4})), 1)
+
+    def testPrevious(self):
+        soup = self.PROXIMITY_TEST
+        b = soup.find('b', {'id' : 3})
+        self.assertEquals(b.findPrevious('b')['id'], '2')
+        self.assertEquals(b.findPrevious('b')['id'], '2')
+        self.assertEquals(len(b.findAllPrevious('b')), 2)
+        self.assertEquals(len(b.findAllPrevious('b', {'id' : 2})), 1)
+
+
+    SIBLING_TEST = BeautifulSoup('<blockquote id="1"><blockquote id="1.1"></blockquote></blockquote><blockquote id="2"><blockquote id="2.1"></blockquote></blockquote><blockquote id="3"><blockquote id="3.1"></blockquote></blockquote><blockquote id="4">')
+
+    def testNextSibling(self):
+        soup = self.SIBLING_TEST
+        tag = 'blockquote'
+        b = soup.find(tag, {'id' : 2})
+        self.assertEquals(b.findNext(tag)['id'], '2.1')
+        self.assertEquals(b.findNextSibling(tag)['id'], '3')
+        self.assertEquals(b.findNextSibling(tag)['id'], '3')
+        self.assertEquals(len(b.findNextSiblings(tag)), 2)
+        self.assertEquals(len(b.findNextSiblings(tag, {'id' : 4})), 1)
+
+    def testPreviousSibling(self):
+        soup = self.SIBLING_TEST
+        tag = 'blockquote'
+        b = soup.find(tag, {'id' : 3})
+        self.assertEquals(b.findPrevious(tag)['id'], '2.1')
+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')
+        self.assertEquals(b.findPreviousSibling(tag)['id'], '2')
+        self.assertEquals(len(b.findPreviousSiblings(tag)), 2)
+        self.assertEquals(len(b.findPreviousSiblings(tag, id=1)), 1)
+
+    def testTextNavigation(self):
+        soup = BeautifulSoup('Foo<b>Bar</b><i id="1"><b>Baz<br />Blee<hr id="1"/></b></i>Blargh')
+        baz = soup.find(text='Baz')
+        self.assertEquals(baz.findParent("i")['id'], '1')
+        self.assertEquals(baz.findNext(text='Blee'), 'Blee')
+        self.assertEquals(baz.findNextSibling(text='Blee'), 'Blee')
+        self.assertEquals(baz.findNextSibling(text='Blargh'), None)
+        self.assertEquals(baz.findNextSibling('hr')['id'], '1')
+
+class SiblingRivalry(SoupTest):
+    "Tests the nextSibling and previousSibling navigation."
+
+    def testSiblings(self):
+        soup = BeautifulSoup("<ul><li>1<p>A</p>B<li>2<li>3</ul>")
+        secondLI = soup.find('li').nextSibling
+        self.assert_(secondLI.name == 'li' and secondLI.string == '2')
+        self.assertEquals(soup.find(text='1').nextSibling.name, 'p')
+        self.assertEquals(soup.find('p').nextSibling, 'B')
+        self.assertEquals(soup.find('p').nextSibling.previousSibling.nextSibling, 'B')
+
+class TagsAreObjectsToo(SoupTest):
+    "Tests the various built-in functions of Tag objects."
+
+    def testLen(self):
+        soup = BeautifulSoup("<top>1<b>2</b>3</top>")
+        self.assertEquals(len(soup.top), 3)
+
+class StringEmUp(SoupTest):
+    "Tests the use of 'string' as an alias for a tag's only content."
+
+    def testString(self):
+        s = BeautifulSoup("<b>foo</b>")
+        self.assertEquals(s.b.string, 'foo')
+
+    def testLackOfString(self):
+        s = BeautifulSoup("<b>f<i>e</i>o</b>")
+        self.assert_(not s.b.string)
+
+    def testStringAssign(self):
+        s = BeautifulSoup("<b></b>")
+        b = s.b
+        b.string = "foo"
+        string = b.string
+        self.assertEquals(string, "foo")
+        self.assert_(isinstance(string, NavigableString))
+
+class AllText(SoupTest):
+    "Tests the use of 'text' to get all of string content from the tag."
+
+    def testText(self):
+        soup = BeautifulSoup("<ul><li>spam</li><li>eggs</li><li>cheese</li>")
+        self.assertEquals(soup.ul.text, "spameggscheese")
+        self.assertEquals(soup.ul.getText('/'), "spam/eggs/cheese")
+
+class ThatsMyLimit(SoupTest):
+    "Tests the limit argument."
+
+    def testBasicLimits(self):
+        s = BeautifulSoup('<br id="1" /><br id="1" /><br id="1" /><br id="1" />')
+        self.assertEquals(len(s.findAll('br')), 4)
+        self.assertEquals(len(s.findAll('br', limit=2)), 2)
+        self.assertEquals(len(s('br', limit=2)), 2)
+
+class OnlyTheLonely(SoupTest):
+    "Tests the parseOnly argument to the constructor."
+    def setUp(self):
+        x = []
+        for i in range(1,6):
+            x.append('<a id="%s">' % i)
+            for j in range(100,103):
+                x.append('<b id="%s.%s">Content %s.%s</b>' % (i,j, i,j))
+            x.append('</a>')
+        self.x = ''.join(x)
+
+    def testOnly(self):
+        strainer = SoupStrainer("b")
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 15)
+
+        strainer = SoupStrainer(id=re.compile("100.*"))
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 5)
+
+        strainer = SoupStrainer(text=re.compile("10[01].*"))
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 10)
+
+        strainer = SoupStrainer(text=lambda(x):x[8]=='3')
+        soup = BeautifulSoup(self.x, parseOnlyThese=strainer)
+        self.assertEquals(len(soup), 3)
+
+class PickleMeThis(SoupTest):
+    "Testing features like pickle and deepcopy."
+
+    def setUp(self):
+        self.page = """<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
+"http://www.w3.org/TR/REC-html40/transitional.dtd">
+<html>
+<head>
+<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
+<title>Beautiful Soup: We called him Tortoise because he taught us.</title>
+<link rev="made" href="mailto:leonardr@segfault.org">
+<meta name="Description" content="Beautiful Soup: an HTML parser optimized for screen-scraping.">
+<meta name="generator" content="Markov Approximation 1.4 (module: leonardr)">
+<meta name="author" content="Leonard Richardson">
+</head>
+<body>
+<a href="foo">foo</a>
+<a href="foo"><b>bar</b></a>
+</body>
+</html>"""
+
+        self.soup = BeautifulSoup(self.page)
+
+    def testPickle(self):
+        import pickle
+        dumped = pickle.dumps(self.soup, 2)
+        loaded = pickle.loads(dumped)
+        self.assertEqual(loaded.__class__, BeautifulSoup)
+        self.assertEqual(str(loaded), str(self.soup))
+
+    def testDeepcopy(self):
+        from copy import deepcopy
+        copied = deepcopy(self.soup)
+        self.assertEqual(str(copied), str(self.soup))
+
+    def testUnicodePickle(self):
+        import cPickle as pickle
+        html = "<b>" + chr(0xc3) + "</b>"
+        soup = BeautifulSoup(html)
+        dumped = pickle.dumps(soup, pickle.HIGHEST_PROTOCOL)
+        loaded = pickle.loads(dumped)
+        self.assertEqual(str(loaded), str(soup))
+
+
+class WriteOnlyCode(SoupTest):
+    "Testing the modification of the tree."
+
+    def testModifyAttributes(self):
+        soup = BeautifulSoup('<a id="1"></a>')
+        soup.a['id'] = 2
+        self.assertEqual(soup.renderContents(), '<a id="2"></a>')
+        del(soup.a['id'])
+        self.assertEqual(soup.renderContents(), '<a></a>')
+        soup.a['id2'] = 'foo'
+        self.assertEqual(soup.renderContents(), '<a id2="foo"></a>')
+
+    def testNewTagCreation(self):
+        "Makes sure tags don't step on each others' toes."
+        soup = BeautifulSoup()
+        a = Tag(soup, 'a')
+        ol = Tag(soup, 'ol')
+        a['href'] = 'http://foo.com/'
+        self.assertRaises(KeyError, lambda : ol['href'])
+
+    def testNewTagWithAttributes(self):
+        """Makes sure new tags can be created complete with attributes."""
+        soup = BeautifulSoup()
+        a = Tag(soup, 'a', [('href', 'foo')])
+        b = Tag(soup, 'b', {'class':'bar'})
+        soup.insert(0,a)
+        soup.insert(1,b)
+        self.assertEqual(soup.a['href'], 'foo')
+        self.assertEqual(soup.b['class'], 'bar')
+
+    def testTagReplacement(self):
+        # Make sure you can replace an element with itself.
+        text = "<a><b></b><c>Foo<d></d></c></a><a><e></e></a>"
+        soup = BeautifulSoup(text)
+        c = soup.c
+        soup.c.replaceWith(c)
+        self.assertEquals(str(soup), text)
+
+        # A very simple case
+        soup = BeautifulSoup("<b>Argh!</b>")
+        soup.find(text="Argh!").replaceWith("Hooray!")
+        newText = soup.find(text="Hooray!")
+        b = soup.b
+        self.assertEqual(newText.previous, b)
+        self.assertEqual(newText.parent, b)
+        self.assertEqual(newText.previous.next, newText)
+        self.assertEqual(newText.next, None)
+
+        # A more complex case
+        soup = BeautifulSoup("<a><b>Argh!</b><c></c><d></d></a>")
+        soup.b.insert(1, "Hooray!")
+        newText = soup.find(text="Hooray!")
+        self.assertEqual(newText.previous, "Argh!")
+        self.assertEqual(newText.previous.next, newText)
+
+        self.assertEqual(newText.previousSibling, "Argh!")
+        self.assertEqual(newText.previousSibling.nextSibling, newText)
+
+        self.assertEqual(newText.nextSibling, None)
+        self.assertEqual(newText.next, soup.c)
+
+        text = "<html>There's <b>no</b> business like <b>show</b> business</html>"
+        soup = BeautifulSoup(text)
+        no, show = soup.findAll('b')
+        show.replaceWith(no)
+        self.assertEquals(str(soup), "<html>There's  business like <b>no</b> business</html>")
+
+        # Even more complex
+        soup = BeautifulSoup("<a><b>Find</b><c>lady!</c><d></d></a>")
+        tag = Tag(soup, 'magictag')
+        tag.insert(0, "the")
+        soup.a.insert(1, tag)
+
+        b = soup.b
+        c = soup.c
+        theText = tag.find(text=True)
+        findText = b.find(text="Find")
+
+        self.assertEqual(findText.next, tag)
+        self.assertEqual(tag.previous, findText)
+        self.assertEqual(b.nextSibling, tag)
+        self.assertEqual(tag.previousSibling, b)
+        self.assertEqual(tag.nextSibling, c)
+        self.assertEqual(c.previousSibling, tag)
+
+        self.assertEqual(theText.next, c)
+        self.assertEqual(c.previous, theText)
+
+        # Aand... incredibly complex.
+        soup = BeautifulSoup("""<a>We<b>reserve<c>the</c><d>right</d></b></a><e>to<f>refuse</f><g>service</g></e>""")
+        f = soup.f
+        a = soup.a
+        c = soup.c
+        e = soup.e
+        weText = a.find(text="We")
+        soup.b.replaceWith(soup.f)
+        self.assertEqual(str(soup), "<a>We<f>refuse</f></a><e>to<g>service</g></e>")
+
+        self.assertEqual(f.previous, weText)
+        self.assertEqual(weText.next, f)
+        self.assertEqual(f.previousSibling, weText)
+        self.assertEqual(f.nextSibling, None)
+        self.assertEqual(weText.nextSibling, f)
+
+    def testReplaceWithChildren(self):
+        soup = BeautifulStoneSoup(
+            "<top><replace><child1/><child2/></replace></top>",
+            selfClosingTags=["child1", "child2"])
+        soup.replaceTag.replaceWithChildren()
+        self.assertEqual(soup.top.contents[0].name, "child1")
+        self.assertEqual(soup.top.contents[1].name, "child2")
+
+    def testAppend(self):
+       doc = "<p>Don't leave me <b>here</b>.</p> <p>Don't leave me.</p>"
+       soup = BeautifulSoup(doc)
+       second_para = soup('p')[1]
+       bold = soup.find('b')
+       soup('p')[1].append(soup.find('b'))
+       self.assertEqual(bold.parent, second_para)
+       self.assertEqual(str(soup),
+                        "<p>Don't leave me .</p> "
+                        "<p>Don't leave me.<b>here</b></p>")
+
+    def testTagExtraction(self):
+        # A very simple case
+        text = '<html><div id="nav">Nav crap</div>Real content here.</html>'
+        soup = BeautifulSoup(text)
+        extracted = soup.find("div", id="nav").extract()
+        self.assertEqual(str(soup), "<html>Real content here.</html>")
+        self.assertEqual(str(extracted), '<div id="nav">Nav crap</div>')
+
+        # A simple case, a more complex test.
+        text = "<doc><a>1<b>2</b></a><a>i<b>ii</b></a><a>A<b>B</b></a></doc>"
+        soup = BeautifulStoneSoup(text)
+        doc = soup.doc
+        numbers, roman, letters = soup("a")
+
+        self.assertEqual(roman.parent, doc)
+        oldPrevious = roman.previous
+        endOfThisTag = roman.nextSibling.previous
+        self.assertEqual(oldPrevious, "2")
+        self.assertEqual(roman.next, "i")
+        self.assertEqual(endOfThisTag, "ii")
+        self.assertEqual(roman.previousSibling, numbers)
+        self.assertEqual(roman.nextSibling, letters)
+
+        roman.extract()
+        self.assertEqual(roman.parent, None)
+        self.assertEqual(roman.previous, None)
+        self.assertEqual(roman.next, "i")
+        self.assertEqual(letters.previous, '2')
+        self.assertEqual(roman.previousSibling, None)
+        self.assertEqual(roman.nextSibling, None)
+        self.assertEqual(endOfThisTag.next, None)
+        self.assertEqual(roman.b.contents[0].next, None)
+        self.assertEqual(numbers.nextSibling, letters)
+        self.assertEqual(letters.previousSibling, numbers)
+        self.assertEqual(len(doc.contents), 2)
+        self.assertEqual(doc.contents[0], numbers)
+        self.assertEqual(doc.contents[1], letters)
+
+        # A more complex case.
+        text = "<a>1<b>2<c>Hollywood, baby!</c></b></a>3"
+        soup = BeautifulStoneSoup(text)
+        one = soup.find(text="1")
+        three = soup.find(text="3")
+        toExtract = soup.b
+        soup.b.extract()
+        self.assertEqual(one.next, three)
+        self.assertEqual(three.previous, one)
+        self.assertEqual(one.parent.nextSibling, three)
+        self.assertEqual(three.previousSibling, soup.a)
+        
+    def testClear(self):
+        soup = BeautifulSoup("<ul><li></li><li></li></ul>")
+        soup.ul.clear()
+        self.assertEqual(len(soup.ul.contents), 0)
+
+class TheManWithoutAttributes(SoupTest):
+    "Test attribute access"
+
+    def testHasKey(self):
+        text = "<foo attr='bar'>"
+        self.assertEquals(BeautifulSoup(text).foo.has_key('attr'), True)
+
+class QuoteMeOnThat(SoupTest):
+    "Test quoting"
+    def testQuotedAttributeValues(self):
+        self.assertSoupEquals("<foo attr='bar'></foo>",
+                              '<foo attr="bar"></foo>')
+
+        text = """<foo attr='bar "brawls" happen'>a</foo>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.renderContents(), text)
+
+        soup.foo['attr'] = 'Brawls happen at "Bob\'s Bar"'
+        newText = """<foo attr='Brawls happen at "Bob&squot;s Bar"'>a</foo>"""
+        self.assertSoupEquals(soup.renderContents(), newText)
+
+        self.assertSoupEquals('<this is="really messed up & stuff">',
+                              '<this is="really messed up &amp; stuff"></this>')
+
+        # This is not what the original author had in mind, but it's
+        # a legitimate interpretation of what they wrote.
+        self.assertSoupEquals("""<a href="foo</a>, </a><a href="bar">baz</a>""",
+        '<a href="foo&lt;/a&gt;, &lt;/a&gt;&lt;a href="></a>, <a href="bar">baz</a>')
+
+        # SGMLParser generates bogus parse events when attribute values
+        # contain embedded brackets, but at least Beautiful Soup fixes
+        # it up a little.
+        self.assertSoupEquals('<a b="<a>">', '<a b="&lt;a&gt;"></a><a>"&gt;</a>')
+        self.assertSoupEquals('<a href="http://foo.com/<a> and blah and blah',
+                              """<a href='"http://foo.com/'></a><a> and blah and blah</a>""")
+
+
+
+class YoureSoLiteral(SoupTest):
+    "Test literal mode."
+    def testLiteralMode(self):
+        text = "<script>if (i<imgs.length)</script><b>Foo</b>"
+        soup = BeautifulSoup(text)
+        self.assertEqual(soup.script.contents[0], "if (i<imgs.length)")
+        self.assertEqual(soup.b.contents[0], "Foo")
+
+    def testTextArea(self):
+        text = "<textarea><b>This is an example of an HTML tag</b><&<&</textarea>"
+        soup = BeautifulSoup(text)
+        self.assertEqual(soup.textarea.contents[0],
+                         "<b>This is an example of an HTML tag</b><&<&")
+
+class OperatorOverload(SoupTest):
+    "Our operators do it all! Call now!"
+
+    def testTagNameAsFind(self):
+        "Tests that referencing a tag name as a member delegates to find()."
+        soup = BeautifulSoup('<b id="1">foo<i>bar</i></b><b>Red herring</b>')
+        self.assertEqual(soup.b.i, soup.find('b').find('i'))
+        self.assertEqual(soup.b.i.string, 'bar')
+        self.assertEqual(soup.b['id'], '1')
+        self.assertEqual(soup.b.contents[0], 'foo')
+        self.assert_(not soup.a)
+
+        #Test the .fooTag variant of .foo.
+        self.assertEqual(soup.bTag.iTag.string, 'bar')
+        self.assertEqual(soup.b.iTag.string, 'bar')
+        self.assertEqual(soup.find('b').find('i'), soup.bTag.iTag)
+
+class NestableEgg(SoupTest):
+    """Here we test tag nesting. TEST THE NEST, DUDE! X-TREME!"""
+
+    def testParaInsideBlockquote(self):
+        soup = BeautifulSoup('<blockquote><p><b>Foo</blockquote><p>Bar')
+        self.assertEqual(soup.blockquote.p.b.string, 'Foo')
+        self.assertEqual(soup.blockquote.b.string, 'Foo')
+        self.assertEqual(soup.find('p', recursive=False).string, 'Bar')
+
+    def testNestedTables(self):
+        text = """<table id="1"><tr><td>Here's another table:
+        <table id="2"><tr><td>Juicy text</td></tr></table></td></tr></table>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.table.td.string, 'Juicy text')
+        self.assertEquals(len(soup.findAll('table')), 2)
+        self.assertEquals(len(soup.table.findAll('table')), 1)
+        self.assertEquals(soup.find('table', {'id' : 2}).parent.parent.parent.name,
+                          'table')
+
+        text = "<table><tr><td><div><table>Foo</table></div></td></tr></table>"
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.tr.td.div.table.contents[0], "Foo")
+
+        text = """<table><thead><tr>Foo</tr></thead><tbody><tr>Bar</tr></tbody>
+        <tfoot><tr>Baz</tr></tfoot></table>"""
+        soup = BeautifulSoup(text)
+        self.assertEquals(soup.table.thead.tr.contents[0], "Foo")
+
+    def testBadNestedTables(self):
+        soup = BeautifulSoup("<table><tr><table><tr id='nested'>")
+        self.assertEquals(soup.table.tr.table.tr['id'], 'nested')
+
+class CleanupOnAisleFour(SoupTest):
+    """Here we test cleanup of text that breaks SGMLParser or is just
+    obnoxious."""
+
+    def testSelfClosingtag(self):
+        self.assertEqual(str(BeautifulSoup("Foo<br/>Bar").find('br')),
+                         '<br />')
+
+        self.assertSoupEquals('<p>test1<br/>test2</p>',
+                              '<p>test1<br />test2</p>')
+
+        text = '<p>test1<selfclosing>test2'
+        soup = BeautifulStoneSoup(text)
+        self.assertEqual(str(soup),
+                         '<p>test1<selfclosing>test2</selfclosing></p>')
+
+        soup = BeautifulStoneSoup(text, selfClosingTags='selfclosing')
+        self.assertEqual(str(soup),
+                         '<p>test1<selfclosing />test2</p>')
+
+    def testSelfClosingTagOrNot(self):
+        text = "<item><link>http://foo.com/</link></item>"
+        self.assertEqual(BeautifulStoneSoup(text).renderContents(), text)
+        self.assertEqual(BeautifulSoup(text).renderContents(),
+                         '<item><link />http://foo.com/</item>')
+
+    def testCData(self):
+        xml = "<root>foo<![CDATA[foobar]]>bar</root>"
+        self.assertSoupEquals(xml, xml)
+        r = re.compile("foo.*bar")
+        soup = BeautifulSoup(xml)
+        self.assertEquals(soup.find(text=r).string, "foobar")
+        self.assertEquals(soup.find(text=r).__class__, CData)
+
+    def testComments(self):
+        xml = "foo<!--foobar-->baz"
+        self.assertSoupEquals(xml)
+        r = re.compile("foo.*bar")
+        soup = BeautifulSoup(xml)
+        self.assertEquals(soup.find(text=r).string, "foobar")
+        self.assertEquals(soup.find(text="foobar").__class__, Comment)
+
+    def testDeclaration(self):
+        xml = "foo<!DOCTYPE foobar>baz"
+        self.assertSoupEquals(xml)
+        r = re.compile(".*foo.*bar")
+        soup = BeautifulSoup(xml)
+        text = "DOCTYPE foobar"
+        self.assertEquals(soup.find(text=r).string, text)
+        self.assertEquals(soup.find(text=text).__class__, Declaration)
+
+        namespaced_doctype = ('<!DOCTYPE xsl:stylesheet SYSTEM "htmlent.dtd">'
+                              '<html>foo</html>')
+        soup = BeautifulSoup(namespaced_doctype)
+        self.assertEquals(soup.contents[0],
+                          'DOCTYPE xsl:stylesheet SYSTEM "htmlent.dtd"')
+        self.assertEquals(soup.html.contents[0], 'foo')
+
+    def testEntityConversions(self):
+        text = "&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;"
+        soup = BeautifulStoneSoup(text)
+        self.assertSoupEquals(text)
+
+        xmlEnt = BeautifulStoneSoup.XML_ENTITIES
+        htmlEnt = BeautifulStoneSoup.HTML_ENTITIES
+        xhtmlEnt = BeautifulStoneSoup.XHTML_ENTITIES
+
+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)
+        self.assertEquals(str(soup), "&lt;&lt;sacr&eacute; bleu!&gt;&gt;")
+
+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;&lt;sacr\xe9 bleu!&gt;&gt;")
+
+        # Make sure the "XML", "HTML", and "XHTML" settings work.
+        text = "&lt;&trade;&apos;"
+        soup = BeautifulStoneSoup(text, convertEntities=xmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;&trade;'")
+
+        soup = BeautifulStoneSoup(text, convertEntities=htmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;\u2122&apos;")
+
+        soup = BeautifulStoneSoup(text, convertEntities=xhtmlEnt)
+        self.assertEquals(unicode(soup), u"&lt;\u2122'")
+
+        invalidEntity = "foo&#bar;baz"
+        soup = BeautifulStoneSoup\
+               (invalidEntity,
+                convertEntities=htmlEnt)
+        self.assertEquals(str(soup), "foo&amp;#bar;baz")
+
+        nonexistentEntity = "foo&bar;baz"
+        soup = BeautifulStoneSoup\
+               (nonexistentEntity,
+                convertEntities="xml")
+        self.assertEquals(str(soup), nonexistentEntity)
+
+
+    def testNonBreakingSpaces(self):
+        soup = BeautifulSoup("<a>&nbsp;&nbsp;</a>",
+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup), u"<a>\xa0\xa0</a>")
+
+    def testWhitespaceInDeclaration(self):
+        self.assertSoupEquals('<! DOCTYPE>', '<!DOCTYPE>')
+
+    def testJunkInDeclaration(self):
+        self.assertSoupEquals('<! Foo = -8>a', '&lt;!Foo = -8&gt;a')
+
+    def testIncompleteDeclaration(self):
+        self.assertSoupEquals('a<!b <p>c', 'a&lt;!b &lt;p&gt;c')
+
+    def testEntityReplacement(self):
+        self.assertSoupEquals('<b>hello&nbsp;there</b>')
+
+    def testEntitiesInAttributeValues(self):
+        self.assertSoupEquals('<x t="x&#241;">', '<x t="x\xc3\xb1"></x>')
+        self.assertSoupEquals('<x t="x&#xf1;">', '<x t="x\xc3\xb1"></x>')
+
+        soup = BeautifulSoup('<x t="&gt;&trade;">',
+                             convertEntities=BeautifulStoneSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup), u'<x t="&gt;\u2122"></x>')
+
+        uri = "http://crummy.com?sacr&eacute;&amp;bleu"
+        link = '<a href="%s"></a>' % uri
+        soup = BeautifulSoup(link)
+        self.assertEquals(unicode(soup), link)
+        #self.assertEquals(unicode(soup.a['href']), uri)
+
+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup),
+                          link.replace("&eacute;", u"\xe9"))
+
+        uri = "http://crummy.com?sacr&eacute;&bleu"
+        link = '<a href="%s"></a>' % uri
+        soup = BeautifulSoup(link, convertEntities=BeautifulSoup.HTML_ENTITIES)
+        self.assertEquals(unicode(soup.a['href']),
+                          uri.replace("&eacute;", u"\xe9"))
+
+    def testNakedAmpersands(self):
+        html = {'convertEntities':BeautifulStoneSoup.HTML_ENTITIES}
+        soup = BeautifulStoneSoup("AT&T ", **html)
+        self.assertEquals(str(soup), 'AT&amp;T ')
+
+        nakedAmpersandInASentence = "AT&T was Ma Bell"
+        soup = BeautifulStoneSoup(nakedAmpersandInASentence,**html)
+        self.assertEquals(str(soup), \
+               nakedAmpersandInASentence.replace('&','&amp;'))
+
+        invalidURL = '<a href="http://example.org?a=1&b=2;3">foo</a>'
+        validURL = invalidURL.replace('&','&amp;')
+        soup = BeautifulStoneSoup(invalidURL)
+        self.assertEquals(str(soup), validURL)
+
+        soup = BeautifulStoneSoup(validURL)
+        self.assertEquals(str(soup), validURL)
+
+
+class EncodeRed(SoupTest):
+    """Tests encoding conversion, Unicode conversion, and Microsoft
+    smart quote fixes."""
+
+    def testUnicodeDammitStandalone(self):
+        markup = "<foo>\x92</foo>"
+        dammit = UnicodeDammit(markup)
+        self.assertEquals(dammit.unicode, "<foo>&#x2019;</foo>")
+
+        hebrew = "\xed\xe5\xec\xf9"
+        dammit = UnicodeDammit(hebrew, ["iso-8859-8"])
+        self.assertEquals(dammit.unicode, u'\u05dd\u05d5\u05dc\u05e9')
+        self.assertEquals(dammit.originalEncoding, 'iso-8859-8')
+
+    def testGarbageInGarbageOut(self):
+        ascii = "<foo>a</foo>"
+        asciiSoup = BeautifulStoneSoup(ascii)
+        self.assertEquals(ascii, str(asciiSoup))
+
+        unicodeData = u"<foo>\u00FC</foo>"
+        utf8 = unicodeData.encode("utf-8")
+        self.assertEquals(utf8, '<foo>\xc3\xbc</foo>')
+
+        unicodeSoup = BeautifulStoneSoup(unicodeData)
+        self.assertEquals(unicodeData, unicode(unicodeSoup))
+        self.assertEquals(unicode(unicodeSoup.foo.string), u'\u00FC')
+
+        utf8Soup = BeautifulStoneSoup(utf8, fromEncoding='utf-8')
+        self.assertEquals(utf8, str(utf8Soup))
+        self.assertEquals(utf8Soup.originalEncoding, "utf-8")
+
+        utf8Soup = BeautifulStoneSoup(unicodeData)
+        self.assertEquals(utf8, str(utf8Soup))
+        self.assertEquals(utf8Soup.originalEncoding, None)
+
+
+    def testHandleInvalidCodec(self):
+        for bad_encoding in ['.utf8', '...', 'utF---16.!']:
+            soup = BeautifulSoup("Räksmörgås", fromEncoding=bad_encoding)
+            self.assertEquals(soup.originalEncoding, 'utf-8')
+
+    def testUnicodeSearch(self):
+        html = u'<html><body><h1>Räksmörgås</h1></body></html>'
+        soup = BeautifulSoup(html)
+        self.assertEqual(soup.find(text=u'Räksmörgås'),u'Räksmörgås')
+
+    def testRewrittenXMLHeader(self):
+        euc_jp = '<?xml version="1.0 encoding="euc-jp"?>\n<foo>\n\xa4\xb3\xa4\xec\xa4\xcfEUC-JP\xa4\xc7\xa5\xb3\xa1\xbc\xa5\xc7\xa5\xa3\xa5\xf3\xa5\xb0\xa4\xb5\xa4\xec\xa4\xbf\xc6\xfc\xcb\xdc\xb8\xec\xa4\xce\xa5\xd5\xa5\xa1\xa5\xa4\xa5\xeb\xa4\xc7\xa4\xb9\xa1\xa3\n</foo>\n'
+        utf8 = "<?xml version='1.0' encoding='utf-8'?>\n<foo>\n\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xafEUC-JP\xe3\x81\xa7\xe3\x82\xb3\xe3\x83\xbc\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x82\xb0\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e\xe3\x81\xae\xe3\x83\x95\xe3\x82\xa1\xe3\x82\xa4\xe3\x83\xab\xe3\x81\xa7\xe3\x81\x99\xe3\x80\x82\n</foo>\n"
+        soup = BeautifulStoneSoup(euc_jp)
+        if soup.originalEncoding != "euc-jp":
+            raise Exception("Test failed when parsing euc-jp document. "
+                            "If you're running Python >=2.4, or you have "
+                            "cjkcodecs installed, this is a real problem. "
+                            "Otherwise, ignore it.")
+
+        self.assertEquals(soup.originalEncoding, "euc-jp")
+        self.assertEquals(str(soup), utf8)
+
+        old_text = "<?xml encoding='windows-1252'><foo>\x92</foo>"
+        new_text = "<?xml version='1.0' encoding='utf-8'?><foo>&rsquo;</foo>"
+        self.assertSoupEquals(old_text, new_text)
+
+    def testRewrittenMetaTag(self):
+        no_shift_jis_html = '''<html><head>\n<meta http-equiv="Content-language" content="ja" /></head><body><pre>\n\x82\xb1\x82\xea\x82\xcdShift-JIS\x82\xc5\x83R\x81[\x83f\x83B\x83\x93\x83O\x82\xb3\x82\xea\x82\xbd\x93\xfa\x96{\x8c\xea\x82\xcc\x83t\x83@\x83C\x83\x8b\x82\xc5\x82\xb7\x81B\n</pre></body></html>'''
+        soup = BeautifulSoup(no_shift_jis_html)
+
+        # Beautiful Soup used to try to rewrite the meta tag even if the
+        # meta tag got filtered out by the strainer. This test makes
+        # sure that doesn't happen.
+        strainer = SoupStrainer('pre')
+        soup = BeautifulSoup(no_shift_jis_html, parseOnlyThese=strainer)
+        self.assertEquals(soup.contents[0].name, 'pre')
+
+        meta_tag = ('<meta content="text/html; charset=x-sjis" '
+                    'http-equiv="Content-type" />')
+        shift_jis_html = (
+            '<html><head>\n%s\n'
+            '<meta http-equiv="Content-language" content="ja" />'
+            '</head><body><pre>\n'
+            '\x82\xb1\x82\xea\x82\xcdShift-JIS\x82\xc5\x83R\x81[\x83f'
+            '\x83B\x83\x93\x83O\x82\xb3\x82\xea\x82\xbd\x93\xfa\x96{\x8c'
+            '\xea\x82\xcc\x83t\x83@\x83C\x83\x8b\x82\xc5\x82\xb7\x81B\n'
+            '</pre></body></html>') % meta_tag
+        soup = BeautifulSoup(shift_jis_html)
+        if soup.originalEncoding != "shift-jis":
+            raise Exception("Test failed when parsing shift-jis document "
+                            "with meta tag '%s'."
+                            "If you're running Python >=2.4, or you have "
+                            "cjkcodecs installed, this is a real problem. "
+                            "Otherwise, ignore it." % meta_tag)
+        self.assertEquals(soup.originalEncoding, "shift-jis")
+
+        content_type_tag = soup.meta['content']
+        self.assertEquals(content_type_tag[content_type_tag.find('charset='):],
+                          'charset=%SOUP-ENCODING%')
+        content_type = str(soup.meta)
+        index = content_type.find('charset=')
+        self.assertEqual(content_type[index:index+len('charset=utf8')+1],
+                         'charset=utf-8')
+        content_type = soup.meta.__str__('shift-jis')
+        index = content_type.find('charset=')
+        self.assertEqual(content_type[index:index+len('charset=shift-jis')],
+                         'charset=shift-jis')
+
+        self.assertEquals(str(soup), (
+                '<html><head>\n'
+                '<meta content="text/html; charset=utf-8" '
+                'http-equiv="Content-type" />\n'
+                '<meta http-equiv="Content-language" content="ja" />'
+                '</head><body><pre>\n'
+                '\xe3\x81\x93\xe3\x82\x8c\xe3\x81\xafShift-JIS\xe3\x81\xa7\xe3'
+                '\x82\xb3\xe3\x83\xbc\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3'
+                '\x82\xb0\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe6\x97\xa5\xe6'
+                '\x9c\xac\xe8\xaa\x9e\xe3\x81\xae\xe3\x83\x95\xe3\x82\xa1\xe3'
+                '\x82\xa4\xe3\x83\xab\xe3\x81\xa7\xe3\x81\x99\xe3\x80\x82\n'
+                '</pre></body></html>'))
+        self.assertEquals(soup.renderContents("shift-jis"),
+                          shift_jis_html.replace('x-sjis', 'shift-jis'))
+
+        isolatin ="""<html><meta http-equiv="Content-type" content="text/html; charset=ISO-Latin-1" />Sacr\xe9 bleu!</html>"""
+        soup = BeautifulSoup(isolatin)
+        self.assertSoupEquals(soup.__str__("utf-8"),
+                              isolatin.replace("ISO-Latin-1", "utf-8").replace("\xe9", "\xc3\xa9"))
+
+    def testHebrew(self):
+        iso_8859_8= '<HEAD>\n<TITLE>Hebrew (ISO 8859-8) in Visual Directionality</TITLE>\n\n\n\n</HEAD>\n<BODY>\n<H1>Hebrew (ISO 8859-8) in Visual Directionality</H1>\n\xed\xe5\xec\xf9\n</BODY>\n'
+        utf8 = '<head>\n<title>Hebrew (ISO 8859-8) in Visual Directionality</title>\n</head>\n<body>\n<h1>Hebrew (ISO 8859-8) in Visual Directionality</h1>\n\xd7\x9d\xd7\x95\xd7\x9c\xd7\xa9\n</body>\n'
+        soup = BeautifulStoneSoup(iso_8859_8, fromEncoding="iso-8859-8")
+        self.assertEquals(str(soup), utf8)
+
+    def testSmartQuotesNotSoSmartAnymore(self):
+        self.assertSoupEquals("\x91Foo\x92 <!--blah-->",
+                              '&lsquo;Foo&rsquo; <!--blah-->')
+
+    def testDontConvertSmartQuotesWhenAlsoConvertingEntities(self):
+        smartQuotes = "Il a dit, \x8BSacr&eacute; bl&#101;u!\x9b"
+        soup = BeautifulSoup(smartQuotes)
+        self.assertEquals(str(soup),
+                          'Il a dit, &lsaquo;Sacr&eacute; bl&#101;u!&rsaquo;')
+        soup = BeautifulSoup(smartQuotes, convertEntities="html")
+        self.assertEquals(str(soup),
+                          'Il a dit, \xe2\x80\xb9Sacr\xc3\xa9 bleu!\xe2\x80\xba')
+
+    def testDontSeeSmartQuotesWhereThereAreNone(self):
+        utf_8 = "\343\202\261\343\203\274\343\202\277\343\202\244 Watch"
+        self.assertSoupEquals(utf_8)
+
+
+class Whitewash(SoupTest):
+    """Test whitespace preservation."""
+
+    def testPreservedWhitespace(self):
+        self.assertSoupEquals("<pre>   </pre>")
+        self.assertSoupEquals("<pre> woo  </pre>")
+
+    def testCollapsedWhitespace(self):
+        self.assertSoupEquals("<p>   </p>", "<p> </p>")
+
+
+if __name__ == '__main__':
+    unittest.main()
diff -rupN grabber-original/backup.py grabber-new/backup.py
--- grabber-original/backup.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/backup.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,41 +1,41 @@
-#!/usr/bin/env python
-"""
-	Backup Files Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys
-from grabber import getContentDirectURL_GET
-from spider  import allowed
-
-# list of possible add-in extension
-ext = [".bak",".old",".",".txt",".inc",".zip",".tar"]
-
-def generateOutput(url):
-	astr = "\n\t<backup>%s"  % (url)
-	astr += "</backup>"
-	return astr
-
-def allowed_inUrl(u):
-	for a in allowed:
-		if u.count('.'+a) > 0:
-			return True
-	return False
-
-def process(url, database, attack_list):
-	plop = open('results/backup_GrabberAttacks.xml','w')
-	plop.write("<backupFiles>\n")
-	for u in database.keys():
-		if allowed_inUrl(u):
-			for e in ext:
-				url1 = u + e
-				url2 = u + e.upper()
-				try:
-					if len(getContentDirectURL_GET(url1,'').read()) > 0:
-						plop.write(generateOutput(url1))
-					if len(getContentDirectURL_GET(url2,'').read()) > 0:
-						plop.write(generateOutput(url2))
-				except AttributeError:
-					continue
-	plop.write("\n</backupFiles>")
-	plop.close()
-	return ""
+#!/usr/bin/env python
+"""
+	Backup Files Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys
+from grabber import getContentDirectURL_GET
+from spider  import allowed
+
+# list of possible add-in extension
+ext = [".bak",".old",".",".txt",".inc",".zip",".tar"]
+
+def generateOutput(url):
+	astr = "\n\t<backup>%s"  % (url)
+	astr += "</backup>"
+	return astr
+
+def allowed_inUrl(u):
+	for a in allowed:
+		if u.count('.'+a) > 0:
+			return True
+	return False
+
+def process(url, database, attack_list):
+	plop = open('results/backup_GrabberAttacks.xml','w')
+	plop.write("<backupFiles>\n")
+	for u in database.keys():
+		if allowed_inUrl(u):
+			for e in ext:
+				url1 = u + e
+				url2 = u + e.upper()
+				try:
+					if len(getContentDirectURL_GET(url1,'').read()) > 0:
+						plop.write(generateOutput(url1))
+					if len(getContentDirectURL_GET(url2,'').read()) > 0:
+						plop.write(generateOutput(url2))
+				except AttributeError:
+					continue
+	plop.write("\n</backupFiles>")
+	plop.close()
+	return ""
diff -rupN grabber-original/bsql.py grabber-new/bsql.py
--- grabber-original/bsql.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/bsql.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,150 +1,154 @@
-#!/usr/bin/env python
-"""
-	Blind SQL Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys
-from grabber import getContent_POST, getContent_GET
-from grabber import getContentDirectURL_GET, getContentDirectURL_POST
-from grabber import single_urlencode
-
-# order of the Blind SQL operations
-orderBSQL = {'AND' : 'TEST', 'TEST' : ['OR','COMMENT','ESCAPE','EVASION']}
-
-
-overflowStr = "" 
-for k in range(0,512):
-	overflowStr += '9'
-
-def detect_sql(output, ):
-	listWords = ["SQL","MySQL","sql","mysql"]
-	for wrd in listWords:
-		if output.count(wrd) > 0:
-			return True
-	return False
-
-def equal(h1,h2):
-	if h1 == h2:
-		return True
-	return False
-
-def generateOutput(url, gParam, instance,method,type):
-	astr = "<bsql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='Blind SQL Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
-		astr += "\n\t<result>%s</result>" % p
-	astr += "\n</bsql>\n"
-	return astr
-
-def generateOutputLong(url, urlString ,method,type, allParams = {}):
-	astr = "<bsql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='Blind SQL Injection Type'>%s</type>"  % (method,url,type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+urlString)
-		astr += "\n\t<result>%s</result>" % (p)
-	else:
-		astr += "\n\t<parameters>"
-		for k in allParams:
-			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
-		astr += "\n\t</parameters>"
-	astr += "\n</bsql>\n"
-	return astr
-
-def permutations(L):
-	if len(L) == 1:
-		yield [L[0]]
-	elif len(L) >= 2:
-		(a, b) = (L[0:1], L[1:])
-		for p in permutations(b):
-			for i in range(len(p)+1):
-				yield b[:i] + a + b[i:]
-
-
-def process(url, database, attack_list):
-	plop = open('results/bsql_GrabberAttacks.xml','w')
-	plop.write("<bsqlAttacks>\n")
-
-	for u in database.keys():
-		if len(database[u]['GET']):
-			print "Method = GET ", u
-			# single parameter testing
-			for gParam in database[u]['GET']:
-				defaultValue = database[u]['GET'][gParam]
-				defaultReturn = getContent_GET(u,gParam,defaultValue)
-				if defaultReturn == None:
-					continue
-				# get the AND statments
-				for andSQL in attack_list['AND']:
-					tmpError = getContent_GET(u,gParam,andSQL)
-					if tmpError == None:
-						continue
-					if equal(defaultReturn.read(), tmpError.read()):
-						# dive here :)
-						basicError  = getContent_GET(u,gParam,'')
-						overflowErS = getContent_GET(u,gParam,overflowStr)
-						if basicError == None or overflowErS == None:
-							continue
-						if equal(basicError.read(), overflowErS.read()):
-							for key in orderBSQL[orderBSQL['AND']]:
-								for instance in attack_list[key]:
-									tmpError  = getContent_GET(u,gParam,instance)
-									if tmpError == None:
-										continue
-									if equal(basicError.read(), tmpError.read()):
-										# should be an error
-										# print u,gParam,instance
-										plop.write(generateOutput(u,gParam,instance,"GET",key))
-						else:
-							# report a overflow possible error
-							#print u,gParam, "overflow"
-							plop.write(generateOutput(u,gParam,"99999...99999","GET","Overflow"))
-			"""
-			# see the permutations
-			if len(database[u]['GET'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						url = ""
-						for gParam in database[u]['GET']:
-							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
-						handle = getContentDirectURL_GET(u,url)
-						if handle != None:
-							output = handle.read()
-							if detect_sql(output):
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
-			"""
-		if len(database[u]['POST']):
-			print "Method = POST ", u
-			# single parameter testing
-			for gParam in database[u]['POST']:
-				defaultValue = database[u]['POST'][gParam]
-				defaultReturn = getContent_POST(u,gParam,defaultValue)
-				if defaultReturn == None:
-					continue
-				# get the AND statments
-				for andSQL in attack_list['AND']:
-					tmpError = getContent_POST(u,gParam,andSQL)
-					if tmpError == None:
-						continue
-					if equal(defaultReturn.read(), tmpError.read()):
-						# dive here :)
-						basicError  = getContent_POST(u,gParam,'')
-						overflowErS = getContent_POST(u,gParam,overflowStr)
-						if basicError == None or overflowErS == None:
-							continue
-						if equal(basicError.read(), overflowErS.read()):
-							for key in orderBSQL[orderBSQL['AND']]:
-								for instance in attack_list[key]:
-									tmpError  = getContent_POST(u,gParam,instance)
-									if tmpError == None:
-										continue
-									if equal(basicError.read(), tmpError.read()):
-										# should be an error
-										plop.write(generateOutput(u,gParam,instance,"POST",key))
-						else:
-							# report a overflow possible error
-							plop.write(generateOutput(u,gParam,"99999...99999","POST","Overflow"))
-	plop.write("\n</bsqlAttacks>\n")
-	plop.close()
-	return ""
+#!/usr/bin/env python
+"""
+	Blind SQL Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys
+from grabber import getContent_POST, getContent_GET
+from grabber import getContentDirectURL_GET, getContentDirectURL_POST
+from grabber import single_urlencode
+from report import appendToReport
+
+# order of the Blind SQL operations
+orderBSQL = {'AND' : 'TEST', 'TEST' : ['OR','COMMENT','ESCAPE','EVASION']}
+
+
+overflowStr = "" 
+for k in range(0,512):
+	overflowStr += '9'
+
+def detect_sql(output, ):
+	listWords = ["SQL","MySQL","sql","mysql"]
+	for wrd in listWords:
+		if output.count(wrd) > 0:
+			return True
+	return False
+
+def equal(h1,h2):
+	if h1 == h2:
+		return True
+	return False
+
+def generateOutput(url, gParam, instance,method,type):
+	astr = "<bsql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='Blind SQL Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
+		astr += "\n\t<result>%s</result>" % p
+	astr += "\n</bsql>\n"
+	return astr
+
+def generateOutputLong(url, urlString ,method,type, allParams = {}):
+	astr = "<bsql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='Blind SQL Injection Type'>%s</type>"  % (method,url,type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+urlString)
+		astr += "\n\t<result>%s</result>" % (p)
+	else:
+		astr += "\n\t<parameters>"
+		for k in allParams:
+			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
+		astr += "\n\t</parameters>"
+	astr += "\n</bsql>\n"
+	return astr
+
+def permutations(L):
+	if len(L) == 1:
+		yield [L[0]]
+	elif len(L) >= 2:
+		(a, b) = (L[0:1], L[1:])
+		for p in permutations(b):
+			for i in range(len(p)+1):
+				yield b[:i] + a + b[i:]
+
+
+def process(url, database, attack_list, txheaders):
+	appendToReport(url, "<div class='panel panel-info'><div class='panel-heading'><h3 class='panel-title'> <a data-toggle='collapse' data-target='#collapseBSql' href='#collapseBSql'>Blind SQL Injection Attacks </a></h3></div>")
+	plop = open('results/bsql_GrabberAttacks.xml','w')
+	plop.write("<bsqlAttacks>\n")
+	appendToReport(url, '<div id="collapseBSql" class="panel-collapse collapse in"><div class="panel-body">');
+	for u in database.keys():
+		appendToReport(u, "<h4><div class='label label-default'><a target='_balnk' href='"+ u +"'>"+ u +"</a></div></h4>")
+		if len(database[u]['GET']):
+			print "Method = GET ", u
+			# single parameter testing
+			for gParam in database[u]['GET']:
+				defaultValue = database[u]['GET'][gParam]
+				defaultReturn = getContent_GET(u,gParam,defaultValue, txheaders)
+				if defaultReturn == None:
+					continue
+				# get the AND statments
+				for andSQL in attack_list['AND']:
+					tmpError = getContent_GET(u,gParam,andSQL, txheaders)
+					if tmpError == None:
+						continue
+					if equal(defaultReturn.read(), tmpError.read()):
+						# dive here :)
+						basicError  = getContent_GET(u,gParam,'', txheaders)
+						overflowErS = getContent_GET(u,gParam,overflowStr, txheaders)
+						if basicError == None or overflowErS == None:
+							continue
+						if equal(basicError.read(), overflowErS.read()):
+							for key in orderBSQL[orderBSQL['AND']]:
+								for instance in attack_list[key]:
+									tmpError  = getContent_GET(u,gParam,instance, txheaders)
+									if tmpError == None:
+										continue
+									if equal(basicError.read(), tmpError.read()):
+										# should be an error
+										# print u,gParam,instance
+										plop.write(generateOutput(u,gParam,instance,"GET",key))
+						else:
+							# report a overflow possible error
+							#print u,gParam, "overflow"
+							plop.write(generateOutput(u,gParam,"99999...99999","GET","Overflow"))
+			"""
+			# see the permutations
+			if len(database[u]['GET'].keys()) > 1:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						url = ""
+						for gParam in database[u]['GET']:
+							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
+						handle = getContentDirectURL_GET(u,url)
+						if handle != None:
+							output = handle.read()
+							if detect_sql(output):
+								# generate the info...
+								plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
+			"""
+		if len(database[u]['POST']):
+			print "Method = POST ", u
+			# single parameter testing
+			for gParam in database[u]['POST']:
+				defaultValue = database[u]['POST'][gParam]
+				defaultReturn = getContent_POST(u,gParam,defaultValue, txheaders)
+				if defaultReturn == None:
+					continue
+				# get the AND statments
+				for andSQL in attack_list['AND']:
+					tmpError = getContent_POST(u,gParam,andSQL, txheaders)
+					if tmpError == None:
+						continue
+					if equal(defaultReturn.read(), tmpError.read()):
+						# dive here :)
+						basicError  = getContent_POST(u,gParam,'', txheaders)
+						overflowErS = getContent_POST(u,gParam,overflowStr, txheaders)
+						if basicError == None or overflowErS == None:
+							continue
+						if equal(basicError.read(), overflowErS.read()):
+							for key in orderBSQL[orderBSQL['AND']]:
+								for instance in attack_list[key]:
+									tmpError  = getContent_POST(u,gParam,instance, txheaders)
+									if tmpError == None:
+										continue
+									if equal(basicError.read(), tmpError.read()):
+										# should be an error
+										plop.write(generateOutput(u,gParam,instance,"POST",key))
+						else:
+							# report a overflow possible error
+							plop.write(generateOutput(u,gParam,"99999...99999","POST","Overflow"))
+	plop.write("\n</bsqlAttacks>\n")
+	appendToReport(url, "</div></div>");
+	plop.close()
+	return ""
diff -rupN grabber-original/bsqlAttacks.xml grabber-new/bsqlAttacks.xml
--- grabber-original/bsqlAttacks.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/bsqlAttacks.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,59 +1,59 @@
-<?xml version="1.0"?>
-<!-- Some of theses SQL Injection are from ha.ckers.org web security lab -->
-<sql>
-	<attack>
-		<name>TEST</name>
-		<code>1&apos;GRABBER_SQL_INJECTION</code>
-		<label>Basic SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>OR</name>
-		<code>1&apos; OR 1 OR 1=&apos;</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>OR</name>
-		<code>OR 1=1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>OR</name>
-		<code>"OR &quot;1&quot;=&quot;1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>OR</name>
-		<code>1&apos; OR 1 OR 1=&apos;</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>AND</name>
-		<code>1&apos; AND 1=1 OR 1=&apos;</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>AND</name>
-		<code>1 AND 1=1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>AND</name>
-		<code>" AND &quot;1&quot;=&quot;1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>COMMENT</name>
-		<code>&apos;;--</code>
-		<label>Command SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>ESCAPE</name>
-		<code>\&apos;; GRABBER_SQL_STATEMENT; --</code>
-		<label>Escape SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>EVASION</name>
-		<code>1 UNI/**/ON SELECT ALL FROM WHERE</code>
-		<label>Evasion SQL Injection Attacks</label>
-	</attack>
+<?xml version="1.0"?>
+<!-- Some of theses SQL Injection are from ha.ckers.org web security lab -->
+<sql>
+	<attack>
+		<name>TEST</name>
+		<code>1&apos;GRABBER_SQL_INJECTION</code>
+		<label>Basic SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>OR</name>
+		<code>1&apos; OR 1 OR 1=&apos;</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>OR</name>
+		<code>OR 1=1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>OR</name>
+		<code>"OR &quot;1&quot;=&quot;1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>OR</name>
+		<code>1&apos; OR 1 OR 1=&apos;</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>AND</name>
+		<code>1&apos; AND 1=1 OR 1=&apos;</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>AND</name>
+		<code>1 AND 1=1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>AND</name>
+		<code>" AND &quot;1&quot;=&quot;1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>COMMENT</name>
+		<code>&apos;;--</code>
+		<label>Command SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>ESCAPE</name>
+		<code>\&apos;; GRABBER_SQL_STATEMENT; --</code>
+		<label>Escape SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>EVASION</name>
+		<code>1 UNI/**/ON SELECT ALL FROM WHERE</code>
+		<label>Evasion SQL Injection Attacks</label>
+	</attack>
 </sql>
\ No newline at end of file
diff -rupN grabber-original/cookies.lwp grabber-new/cookies.lwp
--- grabber-original/cookies.lwp	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/cookies.lwp	2015-05-06 16:35:54.000000000 -0700
@@ -1 +1 @@
-#LWP-Cookies-2.0
+#LWP-Cookies-2.0
diff -rupN grabber-original/count.py grabber-new/count.py
--- grabber-original/count.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/count.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,21 +1,21 @@
-# return the loc of the files
-import os, sys, re
-
-
-count = 0
-python = re.compile(r'(.*).py$')
-all = [f for f in os.listdir('./') if os.path.isfile(os.path.join('./', f)) and python.match(f)]
-for a in all:
-	print a
-	try:
-		f = open(a, 'r')
-		for l in f.readlines():
-			if len(l) > 0:
-				count += 1
-		f.close()
-	except IOError:
-		print "Prout!"
-
-print "Lines of codes ", count
-
-
+# return the loc of the files
+import os, sys, re
+
+
+count = 0
+python = re.compile(r'(.*).py$')
+all = [f for f in os.listdir('./') if os.path.isfile(os.path.join('./', f)) and python.match(f)]
+for a in all:
+	print a
+	try:
+		f = open(a, 'r')
+		for l in f.readlines():
+			if len(l) > 0:
+				count += 1
+		f.close()
+	except IOError:
+		print "Prout!"
+
+print "Lines of codes ", count
+
+
diff -rupN grabber-original/crystal.conf.xml grabber-new/crystal.conf.xml
--- grabber-original/crystal.conf.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/crystal.conf.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,30 +1,30 @@
-<?xml version="1.0"?>
-<!-- Crystal Module for Grabber configuration file -->
-<crystal version="0.1">
-	<!-- Give some information, distant/local files -->
-	<url >http://192.168.1.2/bank</url>
-	<files>\\192.168.1.2\htdocs\bank</files>
-	<!-- Analyzer information, here PHP-Sat -->
-	<analyzer>
-		<path input="--config-file C:\Samate\WebAppScanner\Grabber\php-sat.grabber.ini --preserve-positions --complex-inclusion -i" output="-o">C:\msys\1.0\bin\php-sat.exe</path>
-		<extension>php</extension>
-		<!--
-			Typical pattern block
-
-		    11: not flagged php line
-			12: /**
-			13: 	pattern content...
-			14: */
-			15: php line with the flaw +- FLAGGED!
-		-->
-		<patterns name="php-sat" start="/**" end="*/">
-			<!-- Analyze with the pattern -->
-			<pattern module="xss,include">
-			   Pattern ID: MCV000
-			</pattern>
-			<pattern module="sql,bsql">
-			   Pattern ID: MCV001
-			</pattern>
-		</patterns>
-	</analyzer>
-</crystal>
+<?xml version="1.0"?>
+<!-- Crystal Module for Grabber configuration file -->
+<crystal version="0.1">
+	<!-- Give some information, distant/local files -->
+	<url >http://192.168.1.2/bank</url>
+	<files>\\192.168.1.2\htdocs\bank</files>
+	<!-- Analyzer information, here PHP-Sat -->
+	<analyzer>
+		<path input="--config-file C:\Samate\WebAppScanner\Grabber\php-sat.grabber.ini --preserve-positions --complex-inclusion -i" output="-o">C:\msys\1.0\bin\php-sat.exe</path>
+		<extension>php</extension>
+		<!--
+			Typical pattern block
+
+		    11: not flagged php line
+			12: /**
+			13: 	pattern content...
+			14: */
+			15: php line with the flaw +- FLAGGED!
+		-->
+		<patterns name="php-sat" start="/**" end="*/">
+			<!-- Analyze with the pattern -->
+			<pattern module="xss,include">
+			   Pattern ID: MCV000
+			</pattern>
+			<pattern module="sql,bsql">
+			   Pattern ID: MCV001
+			</pattern>
+		</patterns>
+	</analyzer>
+</crystal>
diff -rupN grabber-original/crystal.py grabber-new/crystal.py
--- grabber-original/crystal.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/crystal.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,532 +1,532 @@
-#!/usr/bin/env python
-"""
-	Crystal Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys,os,re, string, shutil
-from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
-from grabber import getContent_POST, getContentDirectURL_POST
-from grabber import getContent_GET , getContentDirectURL_GET
-from grabber import single_urlencode, partially_in, unescape
-from grabber import investigate, setDatabase
-from spider  import flatten, htmlencode, dict_add
-from spider  import database
-
-
-vulnToDescritiveNames = {
-	'xss' : 'Cross-Site Scripting',
-	'sql' : 'SQL Injection',
-	'bsql': 'Specific Blind Injection...',
-	'include' : 'PHP Include Vulnerability'
-}
-
-
-"""
-	Crystal Module Cooking Book
-	---------------------------
-
-	Make-ahead Tip: Prepare lots of coffee before starting...
-	Preparation: 24 hours
-	Ingredients:
-		A: PHP-Sat
-		B: Grabber Modules lambda
-	Tools:
-		A: Context editor
-		B: Python 2.4
-		C: Nice music (Opera is not needed but you should listen this)
-
-	Directions:
-		0) Read the configuration file (with boolean operator in patterns)
-		1) Scan the PHP sources with PHP-Sat handler (which copy everything in the
-		'/local/crystal/' directory).
-		2) Make a kind of diff then:
-			If the diff results, check for the patterns (given in the configuration file)
-				Parse the PHP line under the end of the pattern
-				Try to get a variable value
-				<after>
-					If no direct variable... backtrack sequentially or in the AST
-				</after>
-		3) Generate the XML report of "the crystal-static-analysis" module
-		4) Build a database of:
-			>>> transformed_into_URL(hypothetical flawed files) : {list of "flawed" params}
-		5) Run the classical tools
-"""
-
-# Crystal Configuration variables
-crystalFiles      = None
-crystalUrl        = None
-crystalExtension  = None
-
-crystalAnalyzerBin= None
-crystalAnalyzerInputParam = None
-crystalAnalyzerOutputParam = None
-
-crystalCheckStart = None
-crystalCheckEnd   = None
-# example: {'xss' : ['pattern_1 __AND__ pattern_3','pattern_2'], 'sql' : ['pattern_3'], 'bsql' : ['pattern_3']}
-crystalPatterns   = {}
-# example: {'xss' : [{'var-position' : reg1}], 'sql' : [{'var-position' : reg2}]}
-crystalRegExpPatterns = {}
-crystalStorage    = []
-crystalDatabase   = {}
-crystalFinalStorage = {}
-
-
-def normalize_whitespace(text):
-	return ' '.join(text.split())
-
-def clear_whitespace(text):
-	return text.replace(' ','')
-
-# Handle the XML file with a SAX Parser
-class CrystalConfHandler(ContentHandler):
-	def __init__(self):
-		self.inAnalyzer  = False
-		self.inPatterns  = False
-		self.inPattern   = False
-		self.isRegExp    = False
-		self.curretVarPos= None
-		self.currentKeys = []
-		self.string     = ""
-	def startElement(self, name, attrs):
-		global crystalAnalyzerInputParam, crystalAnalyzerOutputParam, crystalPatterns, crystalCheckStart, crystalCheckEnd
-		self.string = ""
-		self.currentKeys = []
-		if name == 'analyzer':
-			self.inAnalyzer = True
-		elif name == 'path' and self.inAnalyzer:
-			# store the attributes input and output
-			if 'input' in attrs.keys() and 'output' in attrs.keys():
-				crystalAnalyzerInputParam  = attrs.getValue('input')
-				crystalAnalyzerOutputParam = attrs.getValue('output')
-			else:
-				raise KeyError("CrystalXMLConf: <path> needs 'input' and 'output' attributes")
-		elif name == 'patterns' and self.inAnalyzer:
-			self.inPatterns = True
-			if 'start' in attrs.keys() and 'end' in attrs.keys():
-				crystalCheckStart = attrs.getValue('start')
-				crystalCheckEnd   = attrs.getValue('end')
-			else:
-				raise KeyError("CrystalXMLConf: <patterns> needs 'start' and 'end' attributes")
-			if 'name' in attrs.keys():
-				if attrs.getValue('name').lower() == 'regexp':
-					self.isRegExp = True
-
-		elif self.inPatterns and name == 'pattern':
-			self.inPattern = True
-			if 'module' in attrs.keys():
-				modules = attrs.getValue('module')
-				modules.replace(' ','')
-				self.currentKeys = modules.split(',')
-				if self.isRegExp:
-					if 'varposition' in attrs.keys():
-						curretVarPos = attrs.getValue('varposition')
-			else:
-				raise KeyError("CrystalXMLConf: <pattern > needs 'varposition' attribute")
-
-	def characters(self, ch):
-		self.string = self.string + ch
-
-	def endElement(self, name):
-		global crystalFiles, crystalUrl, crystalExtension, crystalAnalyzerBin, crystalPatterns, crystalRegExpPatterns
-		if name == 'files':
-			crystalFiles = normalize_whitespace(self.string)
-		elif name == 'url':
-			crystalUrl = normalize_whitespace(self.string)
-		elif name == 'extension' and self.inAnalyzer:
-			crystalExtension = normalize_whitespace(self.string)
-		elif name == 'path' and self.inAnalyzer:
-			crystalAnalyzerBin = normalize_whitespace(self.string)
-		elif not self.isRegExp and name == 'pattern' and self.inPattern:
-			tempList = self.string.split('__OR__')
-			for a in self.currentKeys:
-				if a not in crystalPatterns:
-					crystalPatterns[a] = []
-				l = crystalPatterns[a]
-				for t in tempList:
-					l.append(normalize_whitespace(t))
-		elif self.isRegExp and name == 'pattern' and self.inPattern:
-			"""
-			tempList = self.string.split('__OR__')
-			for a in self.currentKeys:
-				if a not in crystalPatterns:
-					crystalRegExpPatterns[a] = []
-				l = crystalRegExpPatterns[a]
-				# build the compiled regexp
-				plop = normalize_whitespace(l)
-				plop = re.compile(plop, re.I)
-				l.append({currentVarPos : plop})
-			"""
-		elif name == "patterns" and self.inPatterns:
-			self.inPatterns = False
-			if self.isRegExp:
-				self.isRegExp = False
-		elif name == "analyzer" and self.inAnalyzer:
-			self.inAnalyzer = False
-
-def copySubTree(src, dst, regFilter):
-	global crystalStorage
-	names = os.listdir(src)
-	try:
-		os.mkdir(dst)
-	except OSError:
-		a = 0
-	try:
-		os.mkdir(dst.replace('crystal/current', 'crystal/analyzed'))
-	except OSError:
-		a = 0
-	for name in names:
-		srcname = os.path.join(src, name)
-		dstname = os.path.join(dst, name)
-		try:
-			if os.path.islink(srcname):
-				linkto = os.readlink(srcname)
-				os.symlink(linkto, dstname)
-			elif os.path.isdir(srcname):
-				copySubTree(srcname, dstname, regFilter)
-			elif regFilter.match(srcname):
-				shutil.copy2(srcname, dstname)
-				crystalStorage.append(dstname)
-		except (IOError, os.error), why:
-			continue
-
-def execCmd(program, args):
-	p = os.popen(program + " " + args)
-	p.close()
-
-
-def generateListOfFiles():
-	"""
-		Create a ghost in ./local/crystal/current and /local/crystal/analyzed
-		And run the SwA tool
-	"""
-	regScripts = re.compile(r'(.*).' + crystalExtension + '$', re.I)
-	copySubTree(crystalFiles, 'local/crystal/current', regScripts)
-	print "Running the static analysis tool..."
-	for file in crystalStorage:
-		fileIn  = os.path.abspath(os.path.join('./', file))
-		fileOut = os.path.abspath(os.path.join('./', file.replace('current', 'analyzed')))
-		cmdLine = crystalAnalyzerInputParam + " " + fileIn + " " + crystalAnalyzerOutputParam + " " + fileOut
-		# execCmd(crystalAnalyzerBin, cmdLine)
-		print crystalAnalyzerBin,cmdLine
-		os.system(crystalAnalyzerBin +" "+ cmdLine)
-
-def stripNoneASCII(output):
-	# should be somepthing to do that.. :/
-	newOutput = ""
-	for s in output:
-		try:
-			s = s.encode()
-			newOutput += s
-		except UnicodeDecodeError:
-			continue
-	return newOutput
-
-def isPatternInFile(fileName):
-	global crystalDatabase
-	file = None
-	try:
-		file = open(fileName, 'r')
-	except IOError:
-		print "Crystal: Cannot open the file [%s]" % fileName
-		return False
-	inZone, inLined = False, False
-	detectPattern = False
-	lineNumber = 0
-	shortName  = fileName[fileName.rfind('analyzed') + 9 : ]
-	vulnName = ""
-	for l in file.readlines():
-		lineNumber += 1
-		l = l.replace('\n','')
-		try:
-			"""
-				Check for the regular expression patterns
-			if len(crystalRegExpPatterns) > 0:
-				for modules in crystalRegExpPatterns:
-					for regexp in crystalRegExpPatterns[modules]:
-						if regexp.match()
-			"""
-			if len(vulnName) > 0 and (detectPattern and not inZone or inLined):
-				# creating the nice structure
-				# { 'index.php' : {'xss' : {'12', 'echo $_GET["plop"]'}}}
-				if shortName not in crystalDatabase:
-					crystalDatabase[shortName] = {}
-				if vulnName not in crystalDatabase[shortName]:
-					crystalDatabase[shortName][vulnName] = {}
-				if str(lineNumber) not in crystalDatabase[shortName][vulnName]:
-					crystalDatabase[shortName][vulnName][str(lineNumber)] = l
-				detectPattern = False
-				inLined = False
-				vulnName = ""
-	
-			if l.count(crystalCheckStart) > 0 and not inZone:
-				b1 = l.find(crystalCheckStart)
-				inZone = True
-				# same line for start and end ?
-				if l.count(crystalCheckEnd) > 0:
-					b2 = l.find(crystalCheckStart)
-					if b1 < b2:
-						inZone = False
-						position = l.lower().find(pattern.lower())
-						if b1 < position and position < b2:
-							detectPattern = True
-							inLined = True
-			elif inZone:
-				# is there any pattern around the corner ?
-				for modules in crystalPatterns:
-					for p in crystalPatterns[modules]:
-						p = p.lower()
-						l = l.lower()
-						# The folowing code is stupid!
-						# I have to change the algorithm for the __AND__ parsing...
-						if '__AND__' in p:
-							listPatterns = p.split('__AND__')
-							isIn = True
-							for patton in listPatterns:
-								if patton not in l:
-									isIn = isIn and False
-							if isIn:
-								detectPattern = True
-								vulnName = modules
-							a=0
-						else:
-							# test if the simple pattern is in the line
-							if p in l:
-								detectPattern = True
-								vulnName = modules
-				if l.count(crystalCheckEnd) > 0:
-					inZone = False
-		except UnicodeDecodeError:
-			continue
-	return True
-
-def buildDatabase():
-	"""
-		Read the analzed files (indirectly with the crystalStorage.replace('current','analyzed') )
-		And look for the patterns
-	"""
-	listOut = []
-	for file in crystalStorage:
-		fileOut = os.path.abspath(os.path.join('./', file.replace('current', 'analyzed')))
-		if not isPatternInFile(fileOut):
-			print "Error with the file [%s]" % file
-
-def createStructure():
-	"""
-		Create the structure in the ./local directory
-	"""
-	try:
-		os.mkdir("local/crystal/")
-	except OSError,e :
-		a=0
-	try:
-		os.mkdir("local/crystal/current")
-	except OSError,e :
-		a=0
-	try:
-		os.mkdir("local/crystal/analyzed")
-	except OSError,e :
-		a=0
-
-"""
-def realLineNumberReverse(fileName, codeStr):
-	print fileName, codeStr
-	try:
-		fN = os.path.abspath(os.path.join('./local/crystal/current/', fileName))
-		file = open(fN, 'r')
-		lineNumber = 0
-		for a in file.readlines():
-			lineNumber += 1
-			if codeStr in a:
-				print a
-				file.close()
-				return lineNumber
-		file.close()
-	except IOError,e:
-		print e
-		return 0
-	return 0
-"""
-
-def generateReport_1():
-	"""
-		Create a first report like:
-
-		* Developer report:
-			# using XSLT...
-			<site>
-				<file name="index.php">
-					<vulnerability line="9">xss</vulnerability>
-					<vulnerability line="25">sql</vulnerability>
-				</file>
-				...
-			</site>
-		
-		* Security report:
-			<site>
-				<vulnerability name="xss">
-					<file name="index.php" line="9" />
-					...
-				</vulnerabilty>
-				<vulnerability name="sql">
-					<file name="index.php" line="25" />
-				</vulnerabilty>
-			</site>
-	"""
-	plop = open('results/crystal_SecurityReport_Grabber.xml','w')
-	plop.write("<crystal>\n")
-	plop.write("<site>\n")
-	plop.write("<!-- The line numbers are from the files in the 'analyzed' directory -->\n")
-	for file in crystalDatabase:
-		plop.write("\t<file name='%s'>\n" % file)
-		for vuln in crystalDatabase[file]:
-			for line in crystalDatabase[file][vuln]:
-				# lineNumber = realLineNumberReverse(file,crystalDatabase[file][vuln][line])
-				localVuln = vuln
-				if localVuln in vulnToDescritiveNames:
-					localVuln = vulnToDescritiveNames[localVuln]
-				plop.write("\t\t<vulnerability name='%s' line='%s' >%s</vulnerability>\n" % (localVuln, line, htmlencode(crystalDatabase[file][vuln][line])))
-		plop.write("\t</file>\n")
-	plop.write("</site>\n")
-	plop.write("</crystal>\n")
-	plop.close()
-
-
-def buildUrlKey(file):
-	fileName = file.replace('\\','/') # on windows...
-	keyUrl = crystalUrl
-	if keyUrl[len(keyUrl)-1] != '/' and fileName[0] != '/':
-		keyUrl += '/'
-	keyUrl += fileName
-	return keyUrl
-
-
-reParamPOST = re.compile(r'(.*)\$_POST\[(.+)\](.*)',re.I)
-reParamGET  = re.compile(r'(.*)\$_GET\[(.+)\](.*)' ,re.I)
-
-def getSimpleParamFromCode_GET(code):
-	"""
-		Using the regular expression above, try to get some parameters name
-	"""
-	params = [] # we can have multiple params...
-	code = code.replace("'",'');
-	code = code.replace('"','');
-	if code.lower().count('get') > 0:
-		# try to match the $_GET
-		if reParamGET.match(code):
-			out = reParamGET.search(code)
-			params.append(out.group(2))
-			params.append(getSimpleParamFromCode_GET(out.group(3)))
-			params = flatten(params)
-	return params
-
-
-def getSimpleParamFromCode_POST(code):
-	"""
-		Using the regular expression above, try to get some parameters name
-	"""
-	params = [] # we can have multiple params...
-	code = code.replace("'",'');
-	code = code.replace('"','');
-	if code.lower().count('post') > 0:
-		# try to match the $_GET
-		if reParamPOST.match(code):
-			out = reParamPOST.search(code)
-			params.append(out.group(2))
-			params.append(getSimpleParamFromCode_POST(out.group(3)))
-			params = flatten(params)
-	return params
-
-
-def createClassicalDatabase(vulnsType, localCrystalDB):
-	"""
-		From the crystalDatabase, generate the same database as in Spider
-		This is generated for calling the differents modules
-
-		ClassicalDB = { url : { 'GET' : { param : value } } }
-	"""
-	classicalDB = {}
-	for file in localCrystalDB:
-		# build the URL
-		keyUrl = buildUrlKey(file)
-		if keyUrl not in classicalDB:
-			classicalDB[keyUrl] = {'GET' : {}, 'POST' : {}}
-		for vuln in localCrystalDB[file]:
-			# only get the kind of vulnerability we want
-			if vuln != vulnsType:
-				continue
-			for line in localCrystalDB[file][vuln]:
-				code = localCrystalDB[file][vuln][line]
-				# try to extract some data...
-				params_GET  = getSimpleParamFromCode_GET (code)
-				params_POST = getSimpleParamFromCode_POST(code)
-				if len(params_GET) > 0:
-					for p in params_GET:
-						lG = classicalDB[keyUrl]['GET']
-						if p not in classicalDB[keyUrl]['GET']:
-							lG = dict_add(lG,{p:''})
-						classicalDB[keyUrl]['GET'] = lG
-				if len(params_POST) > 0:
-					for p in params_POST:
-						lP = classicalDB[keyUrl]['POST']
-						if p not in classicalDB[keyUrl]['POST']:
-							lP = dict_add(lP,{p:''})
-						classicalDB[keyUrl]['POST'] = lP
-	return classicalDB
-
-
-def retrieveVulnList():
-	vulnList = []
-	for file in crystalDatabase:
-		for vuln in crystalDatabase[file]:
-			if vuln not in vulnList:
-				vulnList.append(vuln)
-	return vulnList
-
-
-def process(urlGlobal, localDB, attack_list):
-	"""
-		Crystal Module entry point
-	"""
-	print "Crystal Module Start"
-	try:
-		f = open("crystal.conf.xml", 'r')
-		f.close()
-	except IOError:
-		print "The crystal module needs the 'crystal.conf.xml' configuration file."
-		sys.exit(1)
-	parser = make_parser()
-	crystal_handler = CrystalConfHandler()
-	# Tell the parser to use our handler
-	parser.setContentHandler(crystal_handler)
-	try:
-		parser.parse("crystal.conf.xml")
-	except KeyError, e:
-		print e
-		sys.exit(1)
-
-	#---------- White box testing
-	
-	createStructure()
-	generateListOfFiles()
-
-	buildDatabase()
-	print "Build first report: List of vulneratilities and places in the code"
-	generateReport_1()
-	#---------- Start the Black Box testing
-
-	# need to create a classical database like, so losing information
-	# but for a type of vulnerability
-	listVulns = retrieveVulnList()
-
-	for vulns in listVulns:
-		localDatabase = createClassicalDatabase(vulns, crystalDatabase)
-		setDatabase(localDatabase)
-		print "inProcess Crystal DB = ", localDatabase
-		# print vulns, database
-		# Call the Black Box Module
-		print "Scan for ", vulns
-		investigate(crystalUrl, vulns)
-
-	print "Crystal Module Stop"
-
+#!/usr/bin/env python
+"""
+	Crystal Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys,os,re, string, shutil
+from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
+from grabber import getContent_POST, getContentDirectURL_POST
+from grabber import getContent_GET , getContentDirectURL_GET
+from grabber import single_urlencode, partially_in, unescape
+from grabber import investigate, setDatabase
+from spider  import flatten, htmlencode, dict_add
+from spider  import database
+
+
+vulnToDescritiveNames = {
+	'xss' : 'Cross-Site Scripting',
+	'sql' : 'SQL Injection',
+	'bsql': 'Specific Blind Injection...',
+	'include' : 'PHP Include Vulnerability'
+}
+
+
+"""
+	Crystal Module Cooking Book
+	---------------------------
+
+	Make-ahead Tip: Prepare lots of coffee before starting...
+	Preparation: 24 hours
+	Ingredients:
+		A: PHP-Sat
+		B: Grabber Modules lambda
+	Tools:
+		A: Context editor
+		B: Python 2.4
+		C: Nice music (Opera is not needed but you should listen this)
+
+	Directions:
+		0) Read the configuration file (with boolean operator in patterns)
+		1) Scan the PHP sources with PHP-Sat handler (which copy everything in the
+		'/local/crystal/' directory).
+		2) Make a kind of diff then:
+			If the diff results, check for the patterns (given in the configuration file)
+				Parse the PHP line under the end of the pattern
+				Try to get a variable value
+				<after>
+					If no direct variable... backtrack sequentially or in the AST
+				</after>
+		3) Generate the XML report of "the crystal-static-analysis" module
+		4) Build a database of:
+			>>> transformed_into_URL(hypothetical flawed files) : {list of "flawed" params}
+		5) Run the classical tools
+"""
+
+# Crystal Configuration variables
+crystalFiles      = None
+crystalUrl        = None
+crystalExtension  = None
+
+crystalAnalyzerBin= None
+crystalAnalyzerInputParam = None
+crystalAnalyzerOutputParam = None
+
+crystalCheckStart = None
+crystalCheckEnd   = None
+# example: {'xss' : ['pattern_1 __AND__ pattern_3','pattern_2'], 'sql' : ['pattern_3'], 'bsql' : ['pattern_3']}
+crystalPatterns   = {}
+# example: {'xss' : [{'var-position' : reg1}], 'sql' : [{'var-position' : reg2}]}
+crystalRegExpPatterns = {}
+crystalStorage    = []
+crystalDatabase   = {}
+crystalFinalStorage = {}
+
+
+def normalize_whitespace(text):
+	return ' '.join(text.split())
+
+def clear_whitespace(text):
+	return text.replace(' ','')
+
+# Handle the XML file with a SAX Parser
+class CrystalConfHandler(ContentHandler):
+	def __init__(self):
+		self.inAnalyzer  = False
+		self.inPatterns  = False
+		self.inPattern   = False
+		self.isRegExp    = False
+		self.curretVarPos= None
+		self.currentKeys = []
+		self.string     = ""
+	def startElement(self, name, attrs):
+		global crystalAnalyzerInputParam, crystalAnalyzerOutputParam, crystalPatterns, crystalCheckStart, crystalCheckEnd
+		self.string = ""
+		self.currentKeys = []
+		if name == 'analyzer':
+			self.inAnalyzer = True
+		elif name == 'path' and self.inAnalyzer:
+			# store the attributes input and output
+			if 'input' in attrs.keys() and 'output' in attrs.keys():
+				crystalAnalyzerInputParam  = attrs.getValue('input')
+				crystalAnalyzerOutputParam = attrs.getValue('output')
+			else:
+				raise KeyError("CrystalXMLConf: <path> needs 'input' and 'output' attributes")
+		elif name == 'patterns' and self.inAnalyzer:
+			self.inPatterns = True
+			if 'start' in attrs.keys() and 'end' in attrs.keys():
+				crystalCheckStart = attrs.getValue('start')
+				crystalCheckEnd   = attrs.getValue('end')
+			else:
+				raise KeyError("CrystalXMLConf: <patterns> needs 'start' and 'end' attributes")
+			if 'name' in attrs.keys():
+				if attrs.getValue('name').lower() == 'regexp':
+					self.isRegExp = True
+
+		elif self.inPatterns and name == 'pattern':
+			self.inPattern = True
+			if 'module' in attrs.keys():
+				modules = attrs.getValue('module')
+				modules.replace(' ','')
+				self.currentKeys = modules.split(',')
+				if self.isRegExp:
+					if 'varposition' in attrs.keys():
+						curretVarPos = attrs.getValue('varposition')
+			else:
+				raise KeyError("CrystalXMLConf: <pattern > needs 'varposition' attribute")
+
+	def characters(self, ch):
+		self.string = self.string + ch
+
+	def endElement(self, name):
+		global crystalFiles, crystalUrl, crystalExtension, crystalAnalyzerBin, crystalPatterns, crystalRegExpPatterns
+		if name == 'files':
+			crystalFiles = normalize_whitespace(self.string)
+		elif name == 'url':
+			crystalUrl = normalize_whitespace(self.string)
+		elif name == 'extension' and self.inAnalyzer:
+			crystalExtension = normalize_whitespace(self.string)
+		elif name == 'path' and self.inAnalyzer:
+			crystalAnalyzerBin = normalize_whitespace(self.string)
+		elif not self.isRegExp and name == 'pattern' and self.inPattern:
+			tempList = self.string.split('__OR__')
+			for a in self.currentKeys:
+				if a not in crystalPatterns:
+					crystalPatterns[a] = []
+				l = crystalPatterns[a]
+				for t in tempList:
+					l.append(normalize_whitespace(t))
+		elif self.isRegExp and name == 'pattern' and self.inPattern:
+			"""
+			tempList = self.string.split('__OR__')
+			for a in self.currentKeys:
+				if a not in crystalPatterns:
+					crystalRegExpPatterns[a] = []
+				l = crystalRegExpPatterns[a]
+				# build the compiled regexp
+				plop = normalize_whitespace(l)
+				plop = re.compile(plop, re.I)
+				l.append({currentVarPos : plop})
+			"""
+		elif name == "patterns" and self.inPatterns:
+			self.inPatterns = False
+			if self.isRegExp:
+				self.isRegExp = False
+		elif name == "analyzer" and self.inAnalyzer:
+			self.inAnalyzer = False
+
+def copySubTree(src, dst, regFilter):
+	global crystalStorage
+	names = os.listdir(src)
+	try:
+		os.mkdir(dst)
+	except OSError:
+		a = 0
+	try:
+		os.mkdir(dst.replace('crystal/current', 'crystal/analyzed'))
+	except OSError:
+		a = 0
+	for name in names:
+		srcname = os.path.join(src, name)
+		dstname = os.path.join(dst, name)
+		try:
+			if os.path.islink(srcname):
+				linkto = os.readlink(srcname)
+				os.symlink(linkto, dstname)
+			elif os.path.isdir(srcname):
+				copySubTree(srcname, dstname, regFilter)
+			elif regFilter.match(srcname):
+				shutil.copy2(srcname, dstname)
+				crystalStorage.append(dstname)
+		except (IOError, os.error), why:
+			continue
+
+def execCmd(program, args):
+	p = os.popen(program + " " + args)
+	p.close()
+
+
+def generateListOfFiles():
+	"""
+		Create a ghost in ./local/crystal/current and /local/crystal/analyzed
+		And run the SwA tool
+	"""
+	regScripts = re.compile(r'(.*).' + crystalExtension + '$', re.I)
+	copySubTree(crystalFiles, 'local/crystal/current', regScripts)
+	print "Running the static analysis tool..."
+	for file in crystalStorage:
+		fileIn  = os.path.abspath(os.path.join('./', file))
+		fileOut = os.path.abspath(os.path.join('./', file.replace('current', 'analyzed')))
+		cmdLine = crystalAnalyzerInputParam + " " + fileIn + " " + crystalAnalyzerOutputParam + " " + fileOut
+		# execCmd(crystalAnalyzerBin, cmdLine)
+		print crystalAnalyzerBin,cmdLine
+		os.system(crystalAnalyzerBin +" "+ cmdLine)
+
+def stripNoneASCII(output):
+	# should be somepthing to do that.. :/
+	newOutput = ""
+	for s in output:
+		try:
+			s = s.encode()
+			newOutput += s
+		except UnicodeDecodeError:
+			continue
+	return newOutput
+
+def isPatternInFile(fileName):
+	global crystalDatabase
+	file = None
+	try:
+		file = open(fileName, 'r')
+	except IOError:
+		print "Crystal: Cannot open the file [%s]" % fileName
+		return False
+	inZone, inLined = False, False
+	detectPattern = False
+	lineNumber = 0
+	shortName  = fileName[fileName.rfind('analyzed') + 9 : ]
+	vulnName = ""
+	for l in file.readlines():
+		lineNumber += 1
+		l = l.replace('\n','')
+		try:
+			"""
+				Check for the regular expression patterns
+			if len(crystalRegExpPatterns) > 0:
+				for modules in crystalRegExpPatterns:
+					for regexp in crystalRegExpPatterns[modules]:
+						if regexp.match()
+			"""
+			if len(vulnName) > 0 and (detectPattern and not inZone or inLined):
+				# creating the nice structure
+				# { 'index.php' : {'xss' : {'12', 'echo $_GET["plop"]'}}}
+				if shortName not in crystalDatabase:
+					crystalDatabase[shortName] = {}
+				if vulnName not in crystalDatabase[shortName]:
+					crystalDatabase[shortName][vulnName] = {}
+				if str(lineNumber) not in crystalDatabase[shortName][vulnName]:
+					crystalDatabase[shortName][vulnName][str(lineNumber)] = l
+				detectPattern = False
+				inLined = False
+				vulnName = ""
+	
+			if l.count(crystalCheckStart) > 0 and not inZone:
+				b1 = l.find(crystalCheckStart)
+				inZone = True
+				# same line for start and end ?
+				if l.count(crystalCheckEnd) > 0:
+					b2 = l.find(crystalCheckStart)
+					if b1 < b2:
+						inZone = False
+						position = l.lower().find(pattern.lower())
+						if b1 < position and position < b2:
+							detectPattern = True
+							inLined = True
+			elif inZone:
+				# is there any pattern around the corner ?
+				for modules in crystalPatterns:
+					for p in crystalPatterns[modules]:
+						p = p.lower()
+						l = l.lower()
+						# The folowing code is stupid!
+						# I have to change the algorithm for the __AND__ parsing...
+						if '__AND__' in p:
+							listPatterns = p.split('__AND__')
+							isIn = True
+							for patton in listPatterns:
+								if patton not in l:
+									isIn = isIn and False
+							if isIn:
+								detectPattern = True
+								vulnName = modules
+							a=0
+						else:
+							# test if the simple pattern is in the line
+							if p in l:
+								detectPattern = True
+								vulnName = modules
+				if l.count(crystalCheckEnd) > 0:
+					inZone = False
+		except UnicodeDecodeError:
+			continue
+	return True
+
+def buildDatabase():
+	"""
+		Read the analzed files (indirectly with the crystalStorage.replace('current','analyzed') )
+		And look for the patterns
+	"""
+	listOut = []
+	for file in crystalStorage:
+		fileOut = os.path.abspath(os.path.join('./', file.replace('current', 'analyzed')))
+		if not isPatternInFile(fileOut):
+			print "Error with the file [%s]" % file
+
+def createStructure():
+	"""
+		Create the structure in the ./local directory
+	"""
+	try:
+		os.mkdir("local/crystal/")
+	except OSError,e :
+		a=0
+	try:
+		os.mkdir("local/crystal/current")
+	except OSError,e :
+		a=0
+	try:
+		os.mkdir("local/crystal/analyzed")
+	except OSError,e :
+		a=0
+
+"""
+def realLineNumberReverse(fileName, codeStr):
+	print fileName, codeStr
+	try:
+		fN = os.path.abspath(os.path.join('./local/crystal/current/', fileName))
+		file = open(fN, 'r')
+		lineNumber = 0
+		for a in file.readlines():
+			lineNumber += 1
+			if codeStr in a:
+				print a
+				file.close()
+				return lineNumber
+		file.close()
+	except IOError,e:
+		print e
+		return 0
+	return 0
+"""
+
+def generateReport_1():
+	"""
+		Create a first report like:
+
+		* Developer report:
+			# using XSLT...
+			<site>
+				<file name="index.php">
+					<vulnerability line="9">xss</vulnerability>
+					<vulnerability line="25">sql</vulnerability>
+				</file>
+				...
+			</site>
+		
+		* Security report:
+			<site>
+				<vulnerability name="xss">
+					<file name="index.php" line="9" />
+					...
+				</vulnerabilty>
+				<vulnerability name="sql">
+					<file name="index.php" line="25" />
+				</vulnerabilty>
+			</site>
+	"""
+	plop = open('results/crystal_SecurityReport_Grabber.xml','w')
+	plop.write("<crystal>\n")
+	plop.write("<site>\n")
+	plop.write("<!-- The line numbers are from the files in the 'analyzed' directory -->\n")
+	for file in crystalDatabase:
+		plop.write("\t<file name='%s'>\n" % file)
+		for vuln in crystalDatabase[file]:
+			for line in crystalDatabase[file][vuln]:
+				# lineNumber = realLineNumberReverse(file,crystalDatabase[file][vuln][line])
+				localVuln = vuln
+				if localVuln in vulnToDescritiveNames:
+					localVuln = vulnToDescritiveNames[localVuln]
+				plop.write("\t\t<vulnerability name='%s' line='%s' >%s</vulnerability>\n" % (localVuln, line, htmlencode(crystalDatabase[file][vuln][line])))
+		plop.write("\t</file>\n")
+	plop.write("</site>\n")
+	plop.write("</crystal>\n")
+	plop.close()
+
+
+def buildUrlKey(file):
+	fileName = file.replace('\\','/') # on windows...
+	keyUrl = crystalUrl
+	if keyUrl[len(keyUrl)-1] != '/' and fileName[0] != '/':
+		keyUrl += '/'
+	keyUrl += fileName
+	return keyUrl
+
+
+reParamPOST = re.compile(r'(.*)\$_POST\[(.+)\](.*)',re.I)
+reParamGET  = re.compile(r'(.*)\$_GET\[(.+)\](.*)' ,re.I)
+
+def getSimpleParamFromCode_GET(code):
+	"""
+		Using the regular expression above, try to get some parameters name
+	"""
+	params = [] # we can have multiple params...
+	code = code.replace("'",'');
+	code = code.replace('"','');
+	if code.lower().count('get') > 0:
+		# try to match the $_GET
+		if reParamGET.match(code):
+			out = reParamGET.search(code)
+			params.append(out.group(2))
+			params.append(getSimpleParamFromCode_GET(out.group(3)))
+			params = flatten(params)
+	return params
+
+
+def getSimpleParamFromCode_POST(code):
+	"""
+		Using the regular expression above, try to get some parameters name
+	"""
+	params = [] # we can have multiple params...
+	code = code.replace("'",'');
+	code = code.replace('"','');
+	if code.lower().count('post') > 0:
+		# try to match the $_GET
+		if reParamPOST.match(code):
+			out = reParamPOST.search(code)
+			params.append(out.group(2))
+			params.append(getSimpleParamFromCode_POST(out.group(3)))
+			params = flatten(params)
+	return params
+
+
+def createClassicalDatabase(vulnsType, localCrystalDB):
+	"""
+		From the crystalDatabase, generate the same database as in Spider
+		This is generated for calling the differents modules
+
+		ClassicalDB = { url : { 'GET' : { param : value } } }
+	"""
+	classicalDB = {}
+	for file in localCrystalDB:
+		# build the URL
+		keyUrl = buildUrlKey(file)
+		if keyUrl not in classicalDB:
+			classicalDB[keyUrl] = {'GET' : {}, 'POST' : {}}
+		for vuln in localCrystalDB[file]:
+			# only get the kind of vulnerability we want
+			if vuln != vulnsType:
+				continue
+			for line in localCrystalDB[file][vuln]:
+				code = localCrystalDB[file][vuln][line]
+				# try to extract some data...
+				params_GET  = getSimpleParamFromCode_GET (code)
+				params_POST = getSimpleParamFromCode_POST(code)
+				if len(params_GET) > 0:
+					for p in params_GET:
+						lG = classicalDB[keyUrl]['GET']
+						if p not in classicalDB[keyUrl]['GET']:
+							lG = dict_add(lG,{p:''})
+						classicalDB[keyUrl]['GET'] = lG
+				if len(params_POST) > 0:
+					for p in params_POST:
+						lP = classicalDB[keyUrl]['POST']
+						if p not in classicalDB[keyUrl]['POST']:
+							lP = dict_add(lP,{p:''})
+						classicalDB[keyUrl]['POST'] = lP
+	return classicalDB
+
+
+def retrieveVulnList():
+	vulnList = []
+	for file in crystalDatabase:
+		for vuln in crystalDatabase[file]:
+			if vuln not in vulnList:
+				vulnList.append(vuln)
+	return vulnList
+
+
+def process(urlGlobal, localDB, attack_list):
+	"""
+		Crystal Module entry point
+	"""
+	print "Crystal Module Start"
+	try:
+		f = open("crystal.conf.xml", 'r')
+		f.close()
+	except IOError:
+		print "The crystal module needs the 'crystal.conf.xml' configuration file."
+		sys.exit(1)
+	parser = make_parser()
+	crystal_handler = CrystalConfHandler()
+	# Tell the parser to use our handler
+	parser.setContentHandler(crystal_handler)
+	try:
+		parser.parse("crystal.conf.xml")
+	except KeyError, e:
+		print e
+		sys.exit(1)
+
+	#---------- White box testing
+	
+	createStructure()
+	generateListOfFiles()
+
+	buildDatabase()
+	print "Build first report: List of vulneratilities and places in the code"
+	generateReport_1()
+	#---------- Start the Black Box testing
+
+	# need to create a classical database like, so losing information
+	# but for a type of vulnerability
+	listVulns = retrieveVulnList()
+
+	for vulns in listVulns:
+		localDatabase = createClassicalDatabase(vulns, crystalDatabase)
+		setDatabase(localDatabase)
+		print "inProcess Crystal DB = ", localDatabase
+		# print vulns, database
+		# Call the Black Box Module
+		print "Scan for ", vulns
+		investigate(crystalUrl, vulns)
+
+	print "Crystal Module Stop"
+
diff -rupN grabber-original/files.py grabber-new/files.py
--- grabber-original/files.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/files.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,118 +1,134 @@
-#!/usr/bin/env python
-"""
-	File Inclusion Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys
-from grabber import getContent_POST, getContent_GET
-from grabber import getContentDirectURL_GET, getContentDirectURL_POST
-from grabber import single_urlencode
-
-severity = ["None", "Low", "Medium", "High"]
-
-def detect_file(output, url_get = "http://localhost/?param=false"):
-	listWords = {"root:x:0:0" : 3, "[boot loader]" : 1, "<title>Google</title>" : 3 ,"java.io.FileNotFoundException:" : 1,"fread()" : 1,"include_path" : 1,"Failed opening required" : 1,"file(\"" : 1 ,"file_get_contents(\"" : 1}
-	if "404" in output or "403" in output:
-		# it probabably report an http error
-		return 0
-	if "500" in output:
-		return 1
-	for wrd in listWords:
-		if output.count(wrd) > 0:
-			return listWords[wrd]
-	return 0
-
-def generateOutput(url, gParam, instance,method,type, severityNum = 1):
-	astr = "<file>\n\t<severity>%s</severity>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='Files Injection Type'>%s</type>"  % (severity[severityNum],method,url,gParam,str(instance),type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
-		astr += "\n\t<result>%s</result>" % p
-	astr += "\n</file>\n"
-	return astr
-
-def generateOutputLong(url, urlString ,method,type, severityNum, allParams = {}):
-	astr = "<file>\n\t<severity>%s</severity>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='Files Injection Type'>%s</type>"  % (severity[severityNum], method,url,type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+urlString)
-		astr += "\n\t<result>%s</result>" % (p)
-	else:
-		astr += "\n\t<parameters>"
-		for k in allParams:
-			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
-		astr += "\n\t</parameters>"
-	astr += "\n</file>\n"
-	return astr
-
-def permutations(L):
-	if len(L) == 1:
-		yield [L[0]]
-	elif len(L) >= 2:
-		(a, b) = (L[0:1], L[1:])
-		for p in permutations(b):
-			for i in range(len(p)+1):
-				yield b[:i] + a + b[i:]
-
-def process(url, database, attack_list):
-	plop = open('results/files_GrabberAttacks.xml','w')
-	plop.write("<filesAttacks>\n")
-
-	for u in database.keys():
-		if len(database[u]['GET']):
-			print "Method = GET ", u
-			for gParam in database[u]['GET']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						handle = getContent_GET(u,gParam,instance)
-						if handle != None:
-							output = handle.read()
-							header = handle.info()
-							k = detect_file(output)
-							if k > 0:
-								# generate the info...
-								plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection, k))
-			# see the permutations
-			if len(database[u]['GET'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						url = ""
-						for gParam in database[u]['GET']:
-							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
-						handle = getContentDirectURL_GET(u,url)
-						if handle != None:
-							output = handle.read()
-							k = detect_file(output)
-							if k > 0:
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"GET",typeOfInjection,k))
-		if len(database[u]['POST']):
-			print "Method = POST ", u
-			for gParam in database[u]['POST']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						handle = getContent_POST(u,gParam,instance)
-						if handle != None:
-							output = handle.read()
-							header = handle.info()
-							k = detect_file(output)
-							if k > 0:
-								# generate the info...
-								plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection,k))
-			# see the permutations
-			if len(database[u]['POST'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						allParams = {}
-						for gParam in database[u]['POST']:
-							allParams[gParam] = str(instance)
-						handle = getContentDirectURL_POST(u,allParams)
-						if handle != None:
-							output = handle.read()
-							k = detect_file(output)
-							if k > 0:
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"POST",typeOfInjection,k,allParams))
-	plop.write("\n</filesAttacks>")
-	plop.close()
-	return ""
+#!/usr/bin/env python
+"""
+	File Inclusion Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys
+from grabber import getContent_POST, getContent_GET
+from grabber import getContentDirectURL_GET, getContentDirectURL_POST
+from grabber import single_urlencode
+from report import appendToReport
+
+severity = ["None", "Low", "Medium", "High"]
+
+def detect_file(output, url_get = "http://localhost/?param=false"):
+	listWords = {"root:x:0:0" : 3, "[boot loader]" : 1, "<title>Google</title>" : 3 ,"java.io.FileNotFoundException:" : 1,"fread()" : 1,"include_path" : 1,"Failed opening required" : 1,"file(\"" : 1 ,"file_get_contents(\"" : 1}
+	if "404" in output or "403" in output:
+		# it probabably report an http error
+		return 0
+	if "500" in output:
+		return 1
+	for wrd in listWords:
+		if output.count(wrd) > 0:
+			return listWords[wrd]
+	return 0
+
+def generateOutput(url, gParam, instance,method,type, severityNum = 1):
+	astr = "<file>\n\t<severity>%s</severity>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='Files Injection Type'>%s</type>"  % (severity[severityNum],method,url,gParam,str(instance),type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
+		astr += "\n\t<result>%s</result>" % p
+	astr += "\n</file>\n"
+	return astr
+
+def generateOutputLong(url, urlString ,method,type, severityNum, allParams = {}):
+	astr = "<file>\n\t<severity>%s</severity>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='Files Injection Type'>%s</type>"  % (severity[severityNum], method,url,type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+urlString)
+		astr += "\n\t<result>%s</result>" % (p)
+	else:
+		astr += "\n\t<parameters>"
+		for k in allParams:
+			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
+		astr += "\n\t</parameters>"
+	astr += "\n</file>\n"
+	return astr
+
+def generateHTMLOutput(url, urlString, method, type, instance, allParams={}):
+	message = "<p class='well'><strong>"+ method +"</strong> <i>"+ url +"</i> <br/>"
+	message += "Type: <strong>"+ type +  "</strong> <br/>"
+	if method in ("GET", "get"):
+		message += "Parameter: <strong>"+ urlString + "</strong><br/>  Value: <strong>"+ instance +  "</strong> <br/></p>"
+	
+	return message
+
+def permutations(L):
+	if len(L) == 1:
+		yield [L[0]]
+	elif len(L) >= 2:
+		(a, b) = (L[0:1], L[1:])
+		for p in permutations(b):
+			for i in range(len(p)+1):
+				yield b[:i] + a + b[i:]
+
+def process(url, database, attack_list, txheaders):
+	appendToReport(url, "<div class='panel panel-info'><div class='panel-heading'><h3 class='panel-title'> <a data-toggle='collapse' data-target='#collapseInclude' href='#collapseInclude'>File Injection Attacks </a></h3></div>")
+	plop = open('results/files_GrabberAttacks.xml','w')
+	plop.write("<filesAttacks>\n")
+	appendToReport(url, '<div id="collapseInclude" class="panel-collapse collapse in"><div class="panel-body">');
+	for u in database.keys():
+		appendToReport(u, "<h4><div class='label label-default'><a target='_balnk' href='"+ u +"'>"+ u +"</a></div></h4>")
+		if len(database[u]['GET']):
+			print "Method = GET ", u
+			for gParam in database[u]['GET']:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						handle = getContent_GET(u,gParam,instance, txheaders)
+						if handle != None:
+							output = handle.read()
+							header = handle.info()
+							k = detect_file(output)
+							if k > 0:
+								# generate the info...
+								plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection, k))
+								appendToReport(u, generateHTMLOutput(u, gParam, "GET", typeOfInjection, instance))
+			# see the permutations
+			if len(database[u]['GET'].keys()) > 1:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						url = ""
+						for gParam in database[u]['GET']:
+							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
+						handle = getContentDirectURL_GET(u,url, txheaders)
+						if handle != None:
+							output = handle.read()
+							k = detect_file(output)
+							if k > 0:
+								# generate the info...
+								plop.write(generateOutputLong(u,url,"GET",typeOfInjection,k))
+								appendToReport(u, generateHTMLOutput(u, url, "GET", typeOfInjection, instance))
+		if len(database[u]['POST']):
+			print "Method = POST ", u
+			for gParam in database[u]['POST']:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						handle = getContent_POST(u,gParam,instance, txheaders)
+						if handle != None:
+							output = handle.read()
+							header = handle.info()
+							k = detect_file(output)
+							if k > 0:
+								# generate the info...
+								plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection,k))
+								appendToReport(u, generateHTMLOutput(u, gParam, "POST", typeOfInjection, instance))
+			# see the permutations
+			if len(database[u]['POST'].keys()) > 1:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						allParams = {}
+						for gParam in database[u]['POST']:
+							allParams[gParam] = str(instance)
+						handle = getContentDirectURL_POST(u,allParams, txheaders)
+						if handle != None:
+							output = handle.read()
+							k = detect_file(output)
+							if k > 0:
+								# generate the info...
+								plop.write(generateOutputLong(u,url,"POST",typeOfInjection,k,allParams))
+								appendToReport(u ,generateHTMLOutput(u, url, "POST", typeOfInjection, instance, allparams))
+	plop.write("\n</filesAttacks>")
+	appendToReport(url, "</div></div>");
+	plop.close()
+	return ""
diff -rupN grabber-original/filesAttacks.xml grabber-new/filesAttacks.xml
--- grabber-original/filesAttacks.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/filesAttacks.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,39 +1,39 @@
-<?xml version="1.0"?>
-<files>
-	<attack>
-		<name>HTTP Distant file</name>
-		<code>http://www.google.com</code>
-	</attack>
-	<attack>
-		<name>Windows local file insertion</name>
-		<code>c:\\\\boot.ini</code>
-	</attack>
-	<attack>
-		<name>Unix local file insertion</name>
-		<code>/etc/passwd</code>
-	</attack>
-	<attack>
-		<name>Unix local file insertion</name>
-		<code>/etc/passwd\0</code>
-	</attack>
-	<attack>
-		<name>Windows local file insertion</name>
-		<code>c:\\\\boot.ini\0</code>
-	</attack>
-	<attack>
-		<name>Unix local file insertion</name>
-		<code>../../../../../../../../../../etc/passwd</code>
-	</attack>
-	<attack>
-		<name>Unix local file insertion</name>
-		<code>../../../../../../../../../../etc/passwd\0</code>
-	</attack>
-	<attack>
-		<name>Server boot local file insertion</name>
-		<code>../../../../../../../../../../boot.ini</code>
-	</attack>	
-	<attack>
-		<name>Server boot file insertion</name>
-		<code>../../../../../../../../../../boot.ini\0</code>
-	</attack>
-</files>
+<?xml version="1.0"?>
+<files>
+	<attack>
+		<name>HTTP Distant file</name>
+		<code>http://www.google.com</code>
+	</attack>
+	<attack>
+		<name>Windows local file insertion</name>
+		<code>c:\\\\boot.ini</code>
+	</attack>
+	<attack>
+		<name>Unix local file insertion</name>
+		<code>/etc/passwd</code>
+	</attack>
+	<attack>
+		<name>Unix local file insertion</name>
+		<code>/etc/passwd\0</code>
+	</attack>
+	<attack>
+		<name>Windows local file insertion</name>
+		<code>c:\\\\boot.ini\0</code>
+	</attack>
+	<attack>
+		<name>Unix local file insertion</name>
+		<code>../../../../../../../../../../etc/passwd</code>
+	</attack>
+	<attack>
+		<name>Unix local file insertion</name>
+		<code>../../../../../../../../../../etc/passwd\0</code>
+	</attack>
+	<attack>
+		<name>Server boot local file insertion</name>
+		<code>../../../../../../../../../../boot.ini</code>
+	</attack>	
+	<attack>
+		<name>Server boot file insertion</name>
+		<code>../../../../../../../../../../boot.ini\0</code>
+	</attack>
+</files>
diff -rupN grabber-original/grabber.conf.xml grabber-new/grabber.conf.xml
--- grabber-original/grabber.conf.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/grabber.conf.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,23 +1,26 @@
-<?xml version="1.0"?>
-<!-- Grabber configuration file -->
-<grabber version="0.1">
-	<site>
-		<url>http://192.168.1.2/bank</url>
-		<spider>0</spider> <!-- Depth of the spider -->
-		<scan>
-			<!--
-				If the action is in the list... it will be done...
-				tags:
-					session
-					javascript
-					crystal
-					xss
-					sql
-					bsql
-					include
-					backup
-			-->
-			<crystal />
-		</scan>
-	</site>
-</grabber>
+<?xml version="1.0"?>
+<!-- Grabber configuration file -->
+<grabber version="0.1">
+	<site>
+		<url>https://129.219.253.30:80/~level03/cgi-bin/login.php</url>
+		<spider>0</spider> <!-- Depth of the spider -->
+
+		<scan>
+            <xss/>
+            <sql/>
+			<!--
+				If the action is in the list... it will be done...
+				tags:
+					session
+					javascript
+					crystal
+					xss
+					sql
+					bsql
+					include
+					backup
+			-->
+			<crystal />
+		</scan>
+	</site>
+</grabber>
diff -rupN grabber-original/grabber.py grabber-new/grabber.py
--- grabber-original/grabber.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/grabber.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,558 +1,600 @@
-#!/usr/bin/env python
-"""
-	Grabber Core v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-from BeautifulSoup import BeautifulSoup
-from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
-from optparse import OptionParser
-from urllib2 import URLError, HTTPError
-import urllib
-import time
-import re,sys,os
-
-# Personal libraries
-from spider import database, database_css, database_js
-from spider import spider, cj, allowedExtensions
-
-COOKIEFILE = 'cookies.lwp'          # the path and filename that you want to use to save your cookies in
-import os.path
-txdata = None
-refererUrl = "http://google.com/?q=grabber"
-txheaders = {'User-agent' : 'Grabber/0.1 (X11; U; Linux i686; en-US; rv:1.7)', 'Referer' : refererUrl}
-
-import cookielib
-import urllib2
-urlopen = urllib2.urlopen
-Request = urllib2.Request
-
-def normalize_whitespace(text):
-	return ' '.join(text.split())
-
-def clear_whitespace(text):
-	return text.replace(' ','')
-
-# Configuration variables
-confFile = False
-confUrl  = ""
-confSpider = False
-confActions = []
-confInfos   = {}
-
-# Handle the XML file with a SAX Parser
-class ConfHandler(ContentHandler):
-	def __init__(self):
-		global confFile
-		confFile = True
-		self.inSite    = False
-		self.inScan    = False
-		self.inSpider  = False
-		self.inUrl     = False
-		self.inAction  = False
-		self.string = ""
-		self.listActions = ["crystal", "sql","bsql","xss","include","backup","javascript", "session"]
-	def startElement(self, name, attrs):
-		global confUrl,confInfos
-		self.string = ""
-		if name == 'site':
-			self.inSite = True
-		if name == 'spider' and self.inSite:
-			self.inSpider = True
-		if name == 'scan' and self.inSite:
-			self.inScan = True
-		elif self.inSite and name == 'url':
-			self.inUrl = True
-			confUrl = ""
-		elif self.inScan and name in self.listActions:
-			self.inAction = True
-			if 'info' in attrs.keys():
-				confInfos[name] = attrs.getValue('info')
-	def characters(self, ch):
-		if self.inSite:
-			self.string = self.string + ch
-	def endElement(self, name):
-		global confUrl,confActions,confSpider
-		if name == 'url' and self.inUrl:
-			self.inUrl = False
-			confUrl = normalize_whitespace(self.string)
-		if name == 'spider' and self.inSpider:
-			self.inSpider = False
-			confSpider = clear_whitespace(self.string)
-		if name in self.listActions and self.inScan and not name in confActions:
-			confActions.append(name)
-		if name == 'site' and self.inSite:
-			self.inSite = False
-
-attack_list = { }
-
-# Handle the XML file with a SAX Parser
-class AttackHandler(ContentHandler):
-	def __init__(self):
-		global attack_list
-		attack_list = {}
-		self.inElmt = False
-		self.inCode = False
-		self.inName = False
-		self.sName   = ""
-		self.code   = ""
-	def startElement(self, name, attrs):
-		if name == 'attack':
-			self.inElmt = True
-		elif name == 'code':
-			self.inCode = True
-			self.code = ""
-		elif name == "name":
-			self.inName = True
-			self.sName = ""
-	def characters(self, ch):
-		if self.inCode:
-			self.code = self.code + ch
-		elif self.inName:
-			self.sName = self.sName + ch
-	def endElement(self, name):
-		global attack_list
-		if name == 'code':
-			self.inCode = False
-			self.code = normalize_whitespace(self.code)
-		if name == 'name':
-			self.inName = False
-			self.sName = normalize_whitespace(self.sName)
-		if name == 'attack':
-			self.inElmt = False
-			# send the plop in the dictionnary
-			if not (self.sName in attack_list.keys()):
-				attack_list[self.sName] = []
-			attack_list[self.sName].append(self.code)
-
-class LogHandler:
-	def __init__(self, fileName):
-		self.stream = None
-		try:
-			self.stream = open(fileName, 'w')
-		except IOError:
-			print "Error during the construction of the log system"
-			return
-		self.stream.write("# Log from Grabber.py\n")
-	def __le__(self, string):
-		self.stream.write(string + '\n')
-		self.stream.flush()
-	def __del__(self):
-		self.stream.close()
-
-log = LogHandler('grabber.log')
-
-def unescape(s):
-	"""
-		Unescaping the HTML special characters
-	"""
-	s = s.replace("&lt;", "<")
-	s = s.replace("&gt;", ">")
-	s = s.replace("&quot;", "\"")
-	s = s.replace("&apos;","'")
-	s = s.replace("&amp;", "&")
-	return s
-
-
-def single_urlencode(text):
-   """single URL-encode a given 'text'.  Do not return the 'variablename=' portion."""
-   blah = urllib.urlencode({'blahblahblah':text})
-   #we know the length of the 'blahblahblah=' is equal to 13.  This lets us avoid any messy string matches
-   blah = blah[13:]
-   blah = blah.replace('%5C0','%00')
-   return blah
-
-def getContent_GET(url,param,injection):
-	global log
-	"""
-		Get the content of the url by GET method
-	"""
-	newUrl = url
-	ret = None
-	if url.find('?') < 0:
-		if url[len(url)-1] != '/' and not allowedExtensions(url):
-			url += '/'
-		newUrl = url + '?' + param + '=' + single_urlencode(str(injection))
-	else:
-		newUrl = url + '&' + param + '=' + single_urlencode(str(injection))
-	try:
-		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
-		urllib2.install_opener(opener)
-		log <= ( newUrl)
-		req = Request(newUrl, None, txheaders) # create a request object
-		ret = urlopen(req)                     # and open it to return a handle on the url
-		ret = urlopen(req)                     # and open it to return a handle on the url
-	except HTTPError, e:
-		log <= ( 'The server couldn\'t fulfill the request.')
-		log <= ( 'Error code: %s' % e.code)
-		return None
-	except URLError, e:
-		log <= ( 'We failed to reach a server.')
-		log <= ( 'Reason: %s' % e.reason)
-		return None
-	except IOError:
-		log <= ( "Cannot open: %s" % url)
-		return None
-	return ret
-
-
-def getContentDirectURL_GET(url, string):
-	global log
-	"""
-		Get the content of the url by GET method
-	"""
-	ret = ""
-	try:
-		if len(string) > 0:
-			if url[len(url)-1] != '/' and url.find('?') < 0  and not allowedExtensions(url):
-				url += '/'
-			url = url + "?" + (string)
-		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
-		urllib2.install_opener(opener)
-		log <= ( url)
-		req = Request(url, None, txheaders) # create a request object
-		ret = urlopen(req)                     # and open it to return a handle on the url
-	except HTTPError, e:
-		log <= ( 'The server couldn\'t fulfill the request.')
-		log <= ( 'Error code: %s' % e.code)
-		return None
-	except URLError, e:
-		log <= ( 'We failed to reach a server.')
-		log <= ( 'Reason: %s' % e.reason)
-		return None
-	except IOError:
-		log <= ( "Cannot open: %s" % url)
-		return None
-	return ret
-
-
-def getContent_POST(url,param,injection):
-	global log
-	"""
-		Get the content of the url by POST method
-	"""
-	txdata = urllib.urlencode({param: injection})
-	ret = None
-	try:
-		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
-		urllib2.install_opener(opener)
-		log <= ( url)
-		log <= ( txdata)
-		req = Request(url, txdata, txheaders)  # create a request object
-		ret = urlopen(req)                     # and open it to return a handle on the url
-		ret = urlopen(req)                     # and open it to return a handle on the url
-	except HTTPError, e:
-		log <= ( 'The server couldn\'t fulfill the request.')
-		log <= ( 'Error code: %s' % e.code)
-		return None
-	except URLError, e:
-		log <= ( 'We failed to reach a server.')
-		log <= ( 'Reason: %s' % e.reason)
-		return None
-	except IOError:
-		log <= ( "Cannot open: %s" % url)
-		return None
-	return ret
-
-
-def getContentDirectURL_POST(url,allParams):
-	global log
-	"""
-		Get the content of the url by POST method
-	"""
-	txdata = urllib.urlencode(allParams)
-	ret = None
-	try:
-		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
-		urllib2.install_opener(opener)
-		log <= ( url)
-		log <= ( txdata)
-		req = Request(url, txdata, txheaders)  # create a request object
-		ret = urlopen(req)                     # and open it to return a handle on the url
-		ret = urlopen(req)                     # and open it to return a handle on the url
-	except HTTPError, e:
-		log <= ( 'The server couldn\'t fulfill the request.')
-		log <= ( 'Error code: %s' % e.code)
-		return None
-	except URLError, e:
-		log <= ( 'We failed to reach a server.')
-		log <= ( 'Reason: %s' % e.reason)
-		return None
-	except IOError:
-		log <= ( "Cannot open: %s" % url)
-		return None
-	return ret
-
-# Levenstein distance
-def ld(a, b): # stolen from m.l. hetland
-	n, m = len(a), len(b)
-	if n > m:
-		# Make sure n <= m, to use O(min(n,m)) space
-		a,b = b,a
-		n,m = m,n
-	current = xrange(n+1)
-	for i in xrange(1,m+1):
-		previous, current = current, [i]+[0] * m
-		for j in xrange(1, n+1):
-			add, delete = previous[j] + 1, current[j-1] + 1
-			change = previous[j-1]
-			if a[j-1] != b[i-1]:
-				change +=1
-			current[j] = min(add, delete, change)
-	return current[n]
-
-
-def partially_in(object, container, url = "IMPOSSIBLE_GRABBER_URL", two_long = False):
-	"""
-		Crappy decision function
-			Is an object partially in a text ?
-	"""
-	try:
-		if object in container and url not in container:
-			return True
-	except TypeError:
-		return False
-	if not two_long:
-		# load the big engine...
-		dist = ld(object, container)
-		# espilon ~ len(object) / 4
-		b1 = int(len(object) - len(object) / 4)
-		b2 = int(len(object) + len(object) / 4)
-		# diff of the original text and the given text
-		length = abs(len(container)- dist)
-		if b1 < length and length < b2:
-			return True
-	else:
-		# load the big engine...
-		dist = ld(object, container)
-		# espilon ~ len(object) / (len(object) + 1)
-		b1 = int(len(object) - len(object) / (len(object) + 1))
-		b2 = int(len(object) + len(object) / (len(object) + 1))
-		# diff of the original text and the given text
-		length = abs(len(container)- dist)
-		if b1 < length and length < b2:
-			return True
-	return False
-
-
-def load_definition(fileName):
-	"""
-		Load the XML of definition
-	"""
-	global attack_list
-	attack_list = {}
-	parser = make_parser()
-	xss_handler = AttackHandler()
-	# Tell the parser to use our handler
-	parser.setContentHandler(xss_handler)
-	parser.parse(fileName)
-
-
-def	setDatabase(localDatabase):
-	global database
-	database = {}
-	database = localDatabase
-
-
-def investigate(url, what = "xss"):
-	global attack_list
-	"""
-		Cross-Site Scripting Checking
-		Injection
-		Blind Injection
-	"""
-	localDB = None
-	if what == "xss":
-		from xss import process
-		load_definition('xssAttacks.xml')
-		localDB = database
-	elif what == "sql":
-		from sql import process
-		load_definition('sqlAttacks.xml')
-		localDB = database
-	elif what == "bsql":
-		from bsql import process
-		load_definition('bsqlAttacks.xml')
-		localDB = database
-	elif what == "backup":
-		from backup import process
-		localDB = database
-	elif what == "include":
-		from files import process
-		load_definition('filesAttacks.xml')
-		localDB = database
-	elif what == "javascript":
-		from javascript import process
-		localDB = database_js
-	elif what == "crystal":
-		from crystal import process
-		localDB = database
-	elif what == "session":
-		if 'session' in confInfos:
-			attack_list = confInfos['session']
-			localDB = None
-			from session import process
-		else:
-			raise AtrributeError("You need to give the session id storage key e.g. PHPSESSID, sid etc. ")
-
-	process(url, localDB, attack_list)
-
-	# look at teh cookies returned
-	for index, cookie in enumerate(cj):
-		print '[Cookie]\t', index, '\t:\t', cookie
-	cj.save(COOKIEFILE)
-
-# put a link
-def active_link(s):
-	pos = s.find('http://')
-	if pos < 1:
-		return s
-	else:
-		print pos, len(s), s[pos:len(s)]
-		url = s[pos:len(s)]
-		newStr = s[0:pos-1] + "<a href='" +url + "'>" + urllib.unquote(url) + "</a>"
-		return newStr
-	return s
-
-def createStructure():
-	try:
-		os.mkdir("results")
-	except OSError,e :
-		a=0
-	try:
-		os.mkdir("local")
-	except OSError,e :
-		a=0
-	try:
-		os.mkdir("local/js")
-	except OSError,e :
-		a=0
-	try:
-		os.mkdir("local/css")
-	except OSError,e :
-		a=0
-
-if __name__ == '__main__':
-	option_url = ""
-	option_sql = False
-	option_bsql = False
-	option_xss = False
-	option_backup = False
-	option_include = False
-	option_spider = False
-	option_js = False
-	option_crystal = False
-	option_session = False
-
-	if len(sys.argv) > 1:
-		parser = OptionParser()
-		parser.add_option("-u", "--url", dest="archives_url", help="Adress to investigate")
-		parser.add_option("-s", "--sql", dest="sql", action="store_true",default=False, help="Look for the SQL Injection")
-		parser.add_option("-x", "--xss", dest="xss", action="store_true",default=False, help="Perform XSS attacks")
-		parser.add_option("-b", "--bsql", dest="bsql", action="store_true",default=False, help="Look for blind SQL Injection")
-		parser.add_option("-z", "--backup", dest="backup", action="store_true",default=False, help="Look for backup files")
-		parser.add_option("-d", "--spider", dest="spider", help="Look for every files")
-		parser.add_option("-i", "--include", dest="include", action="store_true",default=False, help="Perform File Insertion attacks")
-		parser.add_option("-j", "--javascript", dest="javascript", action="store_true",default=False, help="Test the javascript code ?")
-		parser.add_option("-c", "--crystal", dest="crystal", action="store_true",default=False, help="Simple crystal ball test.")
-		parser.add_option("-e", "--session", dest="session", action="store_true",default=False, help="Session evaluations")
-
-		(options, args) = parser.parse_args()
-
-		option_url = options.archives_url
-		option_sql = options.sql
-		option_bsql = options.bsql
-		option_xss = options.xss
-		option_backup = options.backup
-		option_include = options.include
-		option_spider = options.spider
-		option_js = options.javascript
-		option_crystal = options.crystal
-		option_session = options.session
-	else:
-		try:
-			f = open("grabber.conf.xml", 'r')
-		except IOError:
-			print "No arguments ? You need to setup the XML configuration file or using the inline arguments"
-			print "Look at the doc to start..."
-			sys.exit(1)
-		parser = make_parser()
-		conf_handler = ConfHandler()
-		# Tell the parser to use our handler
-		parser.setContentHandler(conf_handler)
-		parser.parse("grabber.conf.xml")
-
-		option_url    = confUrl
-		option_spider = confSpider
-		option_sql    = "sql" in confActions
-		option_bsql   = "bsql" in confActions
-		option_xss    = "xss" in confActions
-		option_backup = "backup" in confActions
-		option_include= "include" in confActions
-		option_js     = "javascript" in confActions
-		option_crystal= "crystal" in confActions
-		option_session= "session" in confActions
-
-	# default to localhost ?
-	archives_url = "http://localhost"
-	if option_url:
-		archives_url = option_url
-	root = archives_url
-
-	createStructure()
-	depth = 1
-	try:
-		depth = int(option_spider.strip().split()[0])
-	except (ValueError, IndexError,AttributeError):
-		depth = 0
-
-	try:
-		try:
-			spider(archives_url, depth)
-		except IOError,e :
-			print "Cannot open the url = %s" % archives_url
-			print e.strerror
-			sys.exit(1)
-		if len(database.keys()) < 1:
-			print "No information found!"
-			sys.exit(1)
-		else:
-			print "Start investigation..."
-
-		if option_sql:
-			investigate(archives_url, "sql")
-		if option_xss:
-			investigate(archives_url)
-		if option_bsql:
-			investigate(archives_url,"bsql")
-		if option_backup:
-			investigate(archives_url, "backup")
-		if option_include:
-			investigate(archives_url, "include")
-		if option_js:
-			investigate(archives_url, "javascript")
-		if option_crystal:
-			investigate(archives_url, "crystal")
-		if option_session:
-			investigate(archives_url, "session")
-	except KeyboardInterrupt:
-		print "Plouf!"
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
+#!/usr/bin/env python
+"""
+	Grabber Core v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+from BeautifulSoup import BeautifulSoup
+from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
+from optparse import OptionParser
+from urllib2 import URLError, HTTPError
+import urllib
+import time
+import re,sys,os
+import ssl
+import webbrowser
+import paramiko
+import socks
+import socket
+
+# Personal libraries
+from spider import database, database_css, database_js
+from spider import spider, cj, allowedExtensions
+from report import generateReport, appendToReport, setOptions
+
+COOKIEFILE = 'cookies.lwp'          # the path and filename that you want to use to save your cookies in
+import os.path
+txdata = None
+txheaders = {}
+refererUrl = "http://google.com/?q=grabber"
+#txheaders = {'User-agent' : 'Grabber/0.1 (X11; U; Linux i686; en-US; rv:1.7)', 'Referer' : refererUrl, 'Cookie': cookie}
+
+import cookielib
+import urllib2
+
+socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, "localhost", 8080)
+socket.socket = socks.socksocket
+
+urlopen = urllib2.urlopen
+Request = urllib2.Request
+
+def normalize_whitespace(text):
+	return ' '.join(text.split())
+
+def clear_whitespace(text):
+	return text.replace(' ','')
+
+def definition_headers(cookie):
+    global txheaders
+    txheaders = {'User-agent' : 'Grabber/0.1 (X11; U; Linux i686; en-US; rv:1.7)', 'Referer' : refererUrl, 'Cookie': cookie}
+
+# Configuration variables
+confFile = False
+confUrl  = ""
+confSpider = False
+confActions = []
+confInfos   = {}
+
+# Handle the XML file with a SAX Parser
+class ConfHandler(ContentHandler):
+	def __init__(self):
+		global confFile
+		confFile = True
+		self.inSite    = False
+		self.inScan    = False
+		self.inSpider  = False
+		self.inUrl     = False
+		self.inAction  = False
+		self.string = ""
+		self.listActions = ["crystal", "sql","bsql","xss","include","backup","javascript", "session"]
+	def startElement(self, name, attrs):
+		global confUrl,confInfos
+		self.string = ""
+		if name == 'site':
+			self.inSite = True
+		if name == 'spider' and self.inSite:
+			self.inSpider = True
+		if name == 'scan' and self.inSite:
+			self.inScan = True
+		elif self.inSite and name == 'url':
+			self.inUrl = True
+			confUrl = ""
+		elif self.inScan and name in self.listActions:
+			self.inAction = True
+			if 'info' in attrs.keys():
+				confInfos[name] = attrs.getValue('info')
+	def characters(self, ch):
+		if self.inSite:
+			self.string = self.string + ch
+	def endElement(self, name):
+		global confUrl,confActions,confSpider
+		if name == 'url' and self.inUrl:
+			self.inUrl = False
+			confUrl = normalize_whitespace(self.string)
+		if name == 'spider' and self.inSpider:
+			self.inSpider = False
+			confSpider = clear_whitespace(self.string)
+		if name in self.listActions and self.inScan and not name in confActions:
+			confActions.append(name)
+		if name == 'site' and self.inSite:
+			self.inSite = False
+
+attack_list = { }
+
+# Handle the XML file with a SAX Parser
+class AttackHandler(ContentHandler):
+	def __init__(self):
+		global attack_list
+		attack_list = {}
+		self.inElmt = False
+		self.inCode = False
+		self.inName = False
+		self.sName   = ""
+		self.code   = ""
+	def startElement(self, name, attrs):
+		if name == 'attack':
+			self.inElmt = True
+		elif name == 'code':
+			self.inCode = True
+			self.code = ""
+		elif name == "name":
+			self.inName = True
+			self.sName = ""
+	def characters(self, ch):
+		if self.inCode:
+			self.code = self.code + ch
+		elif self.inName:
+			self.sName = self.sName + ch
+	def endElement(self, name):
+		global attack_list
+		if name == 'code':
+			self.inCode = False
+			self.code = normalize_whitespace(self.code)
+		if name == 'name':
+			self.inName = False
+			self.sName = normalize_whitespace(self.sName)
+		if name == 'attack':
+			self.inElmt = False
+			# send the plop in the dictionnary
+			if not (self.sName in attack_list.keys()):
+				attack_list[self.sName] = []
+			attack_list[self.sName].append(self.code)
+
+class LogHandler:
+	def __init__(self, fileName):
+		self.stream = None
+		try:
+			self.stream = open(fileName, 'w')
+		except IOError:
+			print "Error during the construction of the log system"
+			return
+		self.stream.write("# Log from Grabber.py\n")
+	def __le__(self, string):
+		self.stream.write(string + '\n')
+		self.stream.flush()
+	def __del__(self):
+		self.stream.close()
+
+class ParamHandler:
+	def __init__(self, cookie):
+		self.cookie = cookie
+
+log = LogHandler('grabber.log')
+
+def unescape(s):
+	"""
+		Unescaping the HTML special characters
+	"""
+	s = s.replace("&lt;", "<")
+	s = s.replace("&gt;", ">")
+	s = s.replace("&quot;", "\"")
+	s = s.replace("&apos;","'")
+	s = s.replace("&amp;", "&")
+	return s
+
+def escape(s):
+	"""
+		Escaping the HTML special characters
+	"""
+	s = s.replace("<", "&lt;")
+	s = s.replace(">", "&gt;")
+	s = s.replace("\"", "&quot;")
+	s = s.replace("'","&apos;")
+	s = s.replace("&", "&apos;")
+	return s
+
+def single_urlencode(text):
+   """single URL-encode a given 'text'.  Do not return the 'variablename=' portion."""
+   blah = urllib.urlencode({'blahblahblah':text})
+   #we know the length of the 'blahblahblah=' is equal to 13.  This lets us avoid any messy string matches
+   blah = blah[13:]
+   blah = blah.replace('%5C0','%00')
+   return blah
+
+def getContent_GET(url,param,injection, txheaders):
+	global log
+	"""
+		Get the content of the url by GET method
+	"""
+	newUrl = url
+	ret = None
+	if url.find('?') < 0:
+		if url[len(url)-1] != '/' and not allowedExtensions(url):
+			url += '/'
+		newUrl = url + '?' + param + '=' + single_urlencode(str(injection))
+	else:
+		newUrl = url + '&' + param + '=' + single_urlencode(str(injection))
+	try:
+		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
+		urllib2.install_opener(opener)
+		log <= ( newUrl)
+		context = ssl._create_unverified_context()
+		req = Request(newUrl, None, txheaders) # create a request object
+		ret = urlopen(req ,context=context)                     # and open it to return a handle on the url
+	except HTTPError, e:
+		log <= ( 'The server couldn\'t fulfill the request.')
+		log <= ( 'Error code: %s' % e.code)
+		return None
+	except URLError, e:
+		log <= ( 'We failed to reach a server.')
+		log <= ( 'Reason: %s' % e.reason)
+		return None
+	except IOError:
+		log <= ( "Cannot open: %s" % url)
+		return None
+	return ret
+
+
+def getContentDirectURL_GET(url, string, txheaders):
+	global log
+	"""
+		Get the content of the url by GET method
+	"""
+	ret = ""
+	try:
+		if len(string) > 0:
+			if url[len(url)-1] != '/' and url.find('?') < 0  and not allowedExtensions(url):
+				url += '/'
+			url = url + "?" + (string)
+		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
+		urllib2.install_opener(opener)
+		log <= ( url)
+		context = ssl._create_unverified_context()
+		req = Request(url, None, txheaders) # create a request object
+		ret = urlopen(req, context=context)                     # and open it to return a handle on the url
+	except HTTPError, e:
+		log <= ( 'The server couldn\'t fulfill the request.')
+		log <= ( 'Error code: %s' % e.code)
+		return None
+	except URLError, e:
+		log <= ( 'We failed to reach a server.')
+		log <= ( 'Reason: %s' % e.reason)
+		return None
+	except IOError:
+		log <= ( "Cannot open: %s" % url)
+		return None
+	return ret
+
+
+def getContent_POST(url,param,injection, txheaders):
+	global log
+	"""
+		Get the content of the url by POST method
+	"""
+	txdata = urllib.urlencode({param: injection})
+	ret = None
+	try:
+		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
+		urllib2.install_opener(opener)
+		log <= ( url)
+		log <= ( txdata)
+		context = ssl._create_unverified_context()
+		req = Request(url, txdata, txheaders)  # create a request object
+		ret = urlopen(req, context=context)                     # and open it to return a handle on the url
+	except HTTPError, e:
+		print e
+		log <= ( 'The server couldn\'t fulfill the request.')
+		log <= ( 'Error code: %s' % e.code)
+		return None
+	except URLError, e:
+		print e
+		log <= ( 'We failed to reach a server.')
+		log <= ( 'Reason: %s' % e.reason)
+		return None
+	except IOError:
+		log <= ( "Cannot open: %s" % url)
+		return None
+	return ret
+
+
+def getContentDirectURL_POST(url, allParams, txheaders):
+	global log
+	"""
+		Get the content of the url by POST method
+	"""
+	txdata = urllib.urlencode(allParams)
+	ret = None
+	try:
+		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
+		urllib2.install_opener(opener)
+		log <= ( url)
+		log <= ( txdata)
+		req = Request(url, txdata, txheaders)  # create a request object
+		context = ssl._create_unverified_context()
+		ret = urlopen(req, context=context)                     # and open it to return a handle on the url
+	except HTTPError, e:
+		log <= ( 'The server couldn\'t fulfill the request.')
+		log <= ( 'Error code: %s' % e.code)
+		return None
+	except URLError, e:
+		log <= ( 'We failed to reach a server.')
+		log <= ( 'Reason: %s' % e.reason)
+		return None
+	except IOError:
+		log <= ( "Cannot open: %s" % url)
+		return None
+	return ret
+
+# Levenstein distance
+def ld(a, b): # stolen from m.l. hetland
+	n, m = len(a), len(b)
+	if n > m:
+		# Make sure n <= m, to use O(min(n,m)) space
+		a,b = b,a
+		n,m = m,n
+	current = xrange(n+1)
+	for i in xrange(1,m+1):
+		previous, current = current, [i]+[0] * m
+		for j in xrange(1, n+1):
+			add, delete = previous[j] + 1, current[j-1] + 1
+			change = previous[j-1]
+			if a[j-1] != b[i-1]:
+				change +=1
+			current[j] = min(add, delete, change)
+	return current[n]
+
+
+def partially_in(object, container, url = "IMPOSSIBLE_GRABBER_URL", two_long = False):
+	"""
+		Crappy decision function
+			Is an object partially in a text ?
+	"""
+	try:
+		if object in container and url not in container:
+			return True
+	except TypeError:
+		return False
+	if not two_long:
+		# load the big engine...
+		dist = ld(object, container)
+		# espilon ~ len(object) / 4
+		b1 = int(len(object) - len(object) / 4)
+		b2 = int(len(object) + len(object) / 4)
+		# diff of the original text and the given text
+		length = abs(len(container)- dist)
+		if b1 < length and length < b2:
+			return True
+	else:
+		# load the big engine...
+		dist = ld(object, container)
+		# espilon ~ len(object) / (len(object) + 1)
+		b1 = int(len(object) - len(object) / (len(object) + 1))
+		b2 = int(len(object) + len(object) / (len(object) + 1))
+		# diff of the original text and the given text
+		length = abs(len(container)- dist)
+		if b1 < length and length < b2:
+			return True
+	return False
+
+
+def load_definition(fileName):
+	"""
+		Load the XML of definition
+	"""
+	global attack_list
+	attack_list = {}
+	parser = make_parser()
+	xss_handler = AttackHandler()
+	# Tell the parser to use our handler
+	parser.setContentHandler(xss_handler)
+	parser.parse(fileName)
+
+
+def	setDatabase(localDatabase):
+	global database
+	database = {}
+	database = localDatabase
+
+
+def investigate(url, txheaders, what = "xss"):
+	global attack_list
+	"""
+		Cross-Site Scripting Checking
+		Injection
+		Blind Injection
+	"""
+	localDB = None
+	if what == "xss":
+		from xss import process
+		load_definition('xssAttacks.xml')
+		localDB = database
+	elif what == "sql":
+		from sql import process
+		load_definition('sqlAttacks.xml')
+		localDB = database
+	elif what == "bsql":
+		from bsql import process
+		load_definition('bsqlAttacks.xml')
+		localDB = database
+	elif what == "backup":
+		from backup import process
+		localDB = database
+	elif what == "include":
+		from files import process
+		load_definition('filesAttacks.xml')
+		localDB = database
+	elif what == "javascript":
+		from javascript import process
+		localDB = database_js
+	elif what == "crystal":
+		from crystal import process
+		localDB = database
+	elif what == "session":
+		if 'session' in confInfos:
+			attack_list = confInfos['session']
+			localDB = None
+			from session import process
+		else:
+			raise AtrributeError("You need to give the session id storage key e.g. PHPSESSID, sid etc. ")
+
+	process(url, localDB, attack_list, txheaders)
+
+	# look at teh cookies returned
+	for index, cookie in enumerate(cj):
+		print '[Cookie]\t', index, '\t:\t', cookie
+	cj.save(COOKIEFILE)
+
+# put a link
+def active_link(s):
+	pos = s.find('http://')
+	if pos < 1:
+		return s
+	else:
+		print pos, len(s), s[pos:len(s)]
+		url = s[pos:len(s)]
+		newStr = s[0:pos-1] + "<a href='" +url + "'>" + urllib.unquote(url) + "</a>"
+		return newStr
+	return s
+
+def createStructure():
+	try:
+		os.mkdir("results")
+	except OSError,e :
+		a=0
+	try:
+		os.mkdir("local")
+	except OSError,e :
+		a=0
+	try:
+		os.mkdir("local/js")
+	except OSError,e :
+		a=0
+	try:
+		os.mkdir("local/css")
+	except OSError,e :
+		a=0
+
+if __name__ == '__main__':
+	stopped = 0
+	option_url = ""
+	option_sql = False
+	option_bsql = False
+	option_xss = False
+	option_backup = False
+	option_include = False
+	option_spider = False
+	option_js = False
+	option_crystal = False
+	option_session = False
+	option_cookie = False
+
+	if len(sys.argv) > 1:
+		parser = OptionParser()
+		parser.add_option("-u", "--url", dest="archives_url", help="Adress to investigate")
+		parser.add_option("-s", "--sql", dest="sql", action="store_true",default=False, help="Look for the SQL Injection")
+		parser.add_option("-x", "--xss", dest="xss", action="store_true",default=False, help="Perform XSS attacks")
+		parser.add_option("-b", "--bsql", dest="bsql", action="store_true",default=False, help="Look for blind SQL Injection")
+		parser.add_option("-z", "--backup", dest="backup", action="store_true",default=False, help="Look for backup files")
+		parser.add_option("-d", "--spider", dest="spider", help="Look for every files")
+		parser.add_option("-i", "--include", dest="include", action="store_true",default=False, help="Perform File Insertion attacks")
+		parser.add_option("-j", "--javascript", dest="javascript", action="store_true",default=False, help="Test the javascript code ?")
+		parser.add_option("-c", "--crystal", dest="crystal", action="store_true",default=False, help="Simple crystal ball test.")
+		parser.add_option("-e", "--session", dest="session", action="store_true",default=False, help="Session evaluations")
+		parser.add_option("-o", "--cookie", dest="cookie", help="Pass HTTP cookies")
+
+		(options, args) = parser.parse_args()
+
+		option_url = options.archives_url
+		option_sql = options.sql
+		option_bsql = options.bsql
+		option_xss = options.xss
+		option_backup = options.backup
+		option_include = options.include
+		option_spider = options.spider
+		option_js = options.javascript
+		option_crystal = options.crystal
+		option_session = options.session
+		option_cookie = options.cookie
+	else:
+		try:
+			f = open("grabber.conf.xml", 'r')
+		except IOError:
+			print "No arguments ? You need to setup the XML configuration file or using the inline arguments"
+			print "Look at the doc to start..."
+			sys.exit(1)
+		parser = make_parser()
+		conf_handler = ConfHandler()
+		# Tell the parser to use our handler
+		parser.setContentHandler(conf_handler)
+		parser.parse("grabber.conf.xml")
+
+		option_url    = confUrl
+		option_spider = confSpider
+		option_sql    = "sql" in confActions
+		option_bsql   = "bsql" in confActions
+		option_xss    = "xss" in confActions
+		option_backup = "backup" in confActions
+		option_include= "include" in confActions
+		option_js     = "javascript" in confActions
+		option_crystal= "crystal" in confActions
+		option_session= "session" in confActions
+
+	# default to localhost ?
+	archives_url = "http://localhost"
+	if option_url:
+		archives_url = option_url
+	root = archives_url
+	createStructure()
+	depth = 1
+	setOptions(options)
+
+	generateReport(archives_url, False);
+	filename = "file:///Applications/XAMPP/xamppfiles/htdocs/grabber/results/report.html"
+	webbrowser.get('macosx').open(filename, 0, False)
+
+	definition_headers(option_cookie)
+	if option_cookie != None:
+		appendToReport(archives_url, "<h4><div class='label label-default'>Cookie: "+ option_cookie +"</div></h4>")
+	try:
+		depth = int(option_spider.strip().split()[0])
+	except (ValueError, IndexError,AttributeError):
+		depth = 0
+
+	try:
+		try:
+			spider(archives_url, txheaders, depth)
+		except IOError,e :
+			print "Cannot open the url = %s" % archives_url
+			print e.strerror
+			sys.exit(1)
+		if len(database.keys()) < 1:
+			print "No information found!"
+			sys.exit(1)
+		else:
+			print "Start investigation..."
+
+		if option_sql:
+			investigate(archives_url, txheaders, "sql")
+		if option_xss:
+			investigate(archives_url, txheaders)
+		if option_bsql:
+			investigate(archives_url, txheaders, "bsql")
+		if option_backup:
+			investigate(archives_url, txheaders, "backup")
+		if option_include:
+			investigate(archives_url, txheaders, "include")
+		if option_js:
+			investigate(archives_url, txheaders, "javascript")
+		if option_crystal:
+			investigate(archives_url, txheaders, "crystal")
+		if option_session:
+			investigate(archives_url, txheaders, "session")
+	except KeyboardInterrupt:
+		stopped = 1
+		print "Plouf!"
+	if stopped == 0:
+		appendToReport("Completed", "", True)
+	else:
+		appendToReport("Stopped", "", True)
+
+
+
+
+
+
+
+
+
+
+
+
+
diff -rupN grabber-original/index.html grabber-new/index.html
--- grabber-original/index.html	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/index.html	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,24 @@
+<!Doctype html>
+<html>
+<head>
+</head>
+
+<body>
+	<h1>Welcome to Grabber!</h1>
+
+	<h3>Scan the website</h3>
+	<form action="startGrabber.php" method = "GET">
+		Url: <input name="url" type="text"><br/>
+		<br/>
+		Depth of Spider : <input name="depth" type="number"><br/>
+		
+		Check For:<br/>
+		<input type="checkbox" name="xss" value="xss">XSS <br/>
+		<input type="checkbox" name="sql" value="sql">SQL Injection<br/>
+		<input type="checkbox" name="javascript" value="javascript">Javascript<br/>
+		<input type="checkbox" name="bsql" value="bsql">Blind SQL Injection<br/>
+
+		<button type="submit"> Submit</button>
+	</form>
+</body>
+</html>
\ No newline at end of file
diff -rupN grabber-original/javascript.conf.xml grabber-new/javascript.conf.xml
--- grabber-original/javascript.conf.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/javascript.conf.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,14 +1,14 @@
-<?xml version="1.0"?>
-<!-- JavaScript Source Code Analyzer configuration file -->
-<javascript version="0.1">
-	<!-- 
-		Analyzer information, here JavaScript Lint by Matthias Miller
-		http://www.JavaScriptLint.com
-	-->
-	<analyzer>
-		<path input="-nologo -process" output="">C:\server\jsl-0.3.0\jsl.exe</path>
-		<configuration param="-conf">C:\work\Samate\Grabber\jsl.grabber.conf</configuration>
-		<extension>js</extension>
-		<pattern>__FILENAME__(__LINE__): __ERROR__</pattern>
-	</analyzer>
-</javascript>
+<?xml version="1.0"?>
+<!-- JavaScript Source Code Analyzer configuration file -->
+<javascript version="0.1">
+	<!-- 
+		Analyzer information, here JavaScript Lint by Matthias Miller
+		http://www.JavaScriptLint.com
+	-->
+	<analyzer>
+		<path input="-nologo -process" output="">C:\server\jsl-0.3.0\jsl.exe</path>
+		<configuration param="-conf">C:\work\Samate\Grabber\jsl.grabber.conf</configuration>
+		<extension>js</extension>
+		<pattern>__FILENAME__(__LINE__): __ERROR__</pattern>
+	</analyzer>
+</javascript>
diff -rupN grabber-original/javascript.py grabber-new/javascript.py
--- grabber-original/javascript.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/javascript.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,182 +1,182 @@
-#!/usr/bin/env python
-"""
-	Simple JavaScript Checker Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-	
-	- Look at the JavaScript Source... 
-
-"""
-import sys, re, os
-from spider  import htmlencode
-from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
-
-# JavaScript Configuration variables
-jsAnalyzerBin= None
-jsAnalyzerInputParam = None
-jsAnalyzerOutputParam = None
-jsAnalyzerConfParam = None
-jsAnalyzerConfFile= None
-jsAnalyzerExtension = None
-jsAnalyzerPattern = None
-
-# { 'FILENAME' : { 'line' : ['error 1', 'error 2']  } }
-jsDatabase = {}
-
-"""
-<?xml version="1.0"?>
-<!-- JavaScript Source Code Analyzer configuration file -->
-<javascript version="0.1">
-	<!--
-		Analyzer information, here JavaScript Lint by Matthias Miller
-		http://www.JavaScriptLint.com
-	-->
-	<analyzer>
-		<path input="-process" output="">C:\server\jsl-0.3.0\jsl.exe</path>
-		<configuration param="-conf">C:\server\jsl-0.3.0\jsl.grabber.conf</configuration>
-		<extension>js</extension>
-	</analyzer>
-</javascript>
-"""
-
-def normalize_whitespace(text):
-	return ' '.join(text.split())
-
-def clear_whitespace(text):
-	return text.replace(' ','')
-
-# Handle the XML file with a SAX Parser
-class JavaScriptConfHandler(ContentHandler):
-	def __init__(self):
-		self.inAnalyzer  = False
-		self.string      = ""
-
-	def startElement(self, name, attrs):
-		global jsAnalyzerInputParam, jsAnalyzerOutputParam, jsAnalyzerConfParam
-		self.string = ""
-		self.currentKeys = []
-		if name == 'analyzer':
-			self.inAnalyzer = True
-		elif name == 'path' and self.inAnalyzer:
-			# store the attributes input and output
-			if 'input' in attrs.keys() and 'output' in attrs.keys():
-				jsAnalyzerInputParam  = attrs.getValue('input')
-				jsAnalyzerOutputParam = attrs.getValue('output')
-			else:
-				raise KeyError("JavaScriptXMLConf: <path> needs 'input' and 'output' attributes")
-		elif name == 'configuration' and self.inAnalyzer:
-			# store the attribute 'param'
-			if 'param' in attrs.keys():
-				jsAnalyzerConfParam  = attrs.getValue('param')
-			else:
-				raise KeyError("JavaScriptXMLConf: <configuration> needs 'param' attribute")
-
-	def characters(self, ch):
-		self.string = self.string + ch
-
-	def endElement(self, name):
-		global jsAnalyzerBin, jsAnalyzerConfFile, jsAnalyzerExtension,jsAnalyzerPattern
-		if name == 'configuration':
-			jsAnalyzerConfFile = normalize_whitespace(self.string)
-		elif name == 'extension' and self.inAnalyzer:
-			jsAnalyzerExtension = normalize_whitespace(self.string)
-		elif name == 'path' and self.inAnalyzer:
-			jsAnalyzerBin = normalize_whitespace(self.string)
-		elif name == "analyzer":
-			self.inAnalyzer = False
-		elif name == "pattern":
-			jsAnalyzerPattern = normalize_whitespace(self.string)
-
-def execCmd(program, args):
-	buff = []
-	p = os.popen(program + " " + args)
-	buff = p.readlines()
-	p.close()
-	return buff
-
-
-def generateListOfFiles(localDB, urlGlobal):
-	global jsDatabase
-	"""
-		Create a ghost in ./local/crystal/current and /local/crystal/analyzed
-		And run the SwA tool
-	"""
-	regScripts = re.compile(r'(.*).' + jsAnalyzerExtension + '$', re.I)
-	# escape () and []
-	localRegOutput = jsAnalyzerPattern
-	localRegOutput = localRegOutput.replace('(', '\(')
-	localRegOutput = localRegOutput.replace(')', '\)')
-	localRegOutput = localRegOutput.replace('[', '\[')
-	localRegOutput = localRegOutput.replace(']', '\]')
-	localRegOutput = localRegOutput.replace(':', '\:')
-	localRegOutput = localRegOutput.replace('__LINE__', '(\d+)')
-	localRegOutput = localRegOutput.replace('__FILENAME__', '(.+)')
-	localRegOutput = localRegOutput.replace('__ERROR__', '(.+)')
-	regOutput = re.compile('^'+localRegOutput+'$', re.I)
-	
-	print "Running the static analysis tool..."
-	for file in localDB:
-		print file
-		file = file.replace(urlGlobal + '/', '')
-		fileIn  = os.path.abspath(os.path.join('./local', file))
-		cmdLine = jsAnalyzerConfParam + " " +jsAnalyzerConfFile + " " + jsAnalyzerInputParam + " " + fileIn
-		if jsAnalyzerOutputParam != "":
-			cmdLine += " " + jsAnalyzerOutputParam + " " + fileIn+'.jslint'
-		output  = execCmd(jsAnalyzerBin, cmdLine)
-		# Analyze the output
-		for o in output:
-			lO = o.replace('\n','')
-			if regOutput.match(lO):
-				out = regOutput.search(lO)
-				if file not in jsDatabase:
-					jsDatabase[file] = {}
-				line = clear_whitespace(out.group(2))
-				if line not in jsDatabase[file]:
-					jsDatabase[file][line] = []
-				jsDatabase[file][line].append(normalize_whitespace(out.group(3)))
-	# sort the dictionary
-	# + file
-	#   + lines
-
-
-def process(urlGlobal, localDB, attack_list):
-	"""
-		Crystal Module entry point
-	"""
-	print "JavaScript Module Start"
-	try:
-		f = open("javascript.conf.xml", 'r')
-		f.close()
-	except IOError:
-		print "The javascript module needs the 'javascript.conf.xml' configuration file."
-		sys.exit(1)
-	parser = make_parser()
-	js_handler = JavaScriptConfHandler()
-	# Tell the parser to use our handler
-	parser.setContentHandler(js_handler)
-	try:
-		parser.parse("javascript.conf.xml")
-	except KeyError, e:
-		print e
-		sys.exit(1)
-
-	# only a white box testing...
-	generateListOfFiles(localDB,urlGlobal)
-	# create the report
-	plop = open('results/javascript_Grabber.xml','w')
-	plop.write("<javascript>\n")
-	plop.write("<site>\n")
-	for file in jsDatabase:
-		plop.write("\t<file name='%s'>\n" % file)
-		for line in jsDatabase[file]:
-			if len(jsDatabase[file][line]) > 1:
-				plop.write("\t\t<line number='%s'>\n" % line)
-				for error in jsDatabase[file][line]:
-					plop.write("\t\t\t<error>%s</error>\n" % htmlencode(error))
-				plop.write("\t\t</line>\n")
-			else:
-				plop.write("\t\t<line number='%s'>%s</line>\n" % (line, htmlencode(jsDatabase[file][line][0])))
-		plop.write("\t</file>\n")
-	plop.write("</site>\n")
-	plop.write("</javascript>\n")
-	plop.close()
-
+#!/usr/bin/env python
+"""
+	Simple JavaScript Checker Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+	
+	- Look at the JavaScript Source... 
+
+"""
+import sys, re, os
+from spider  import htmlencode
+from xml.sax import *   # Need PyXML [http://pyxml.sourceforge.net/]
+
+# JavaScript Configuration variables
+jsAnalyzerBin= None
+jsAnalyzerInputParam = None
+jsAnalyzerOutputParam = None
+jsAnalyzerConfParam = None
+jsAnalyzerConfFile= None
+jsAnalyzerExtension = None
+jsAnalyzerPattern = None
+
+# { 'FILENAME' : { 'line' : ['error 1', 'error 2']  } }
+jsDatabase = {}
+
+"""
+<?xml version="1.0"?>
+<!-- JavaScript Source Code Analyzer configuration file -->
+<javascript version="0.1">
+	<!--
+		Analyzer information, here JavaScript Lint by Matthias Miller
+		http://www.JavaScriptLint.com
+	-->
+	<analyzer>
+		<path input="-process" output="">C:\server\jsl-0.3.0\jsl.exe</path>
+		<configuration param="-conf">C:\server\jsl-0.3.0\jsl.grabber.conf</configuration>
+		<extension>js</extension>
+	</analyzer>
+</javascript>
+"""
+
+def normalize_whitespace(text):
+	return ' '.join(text.split())
+
+def clear_whitespace(text):
+	return text.replace(' ','')
+
+# Handle the XML file with a SAX Parser
+class JavaScriptConfHandler(ContentHandler):
+	def __init__(self):
+		self.inAnalyzer  = False
+		self.string      = ""
+
+	def startElement(self, name, attrs):
+		global jsAnalyzerInputParam, jsAnalyzerOutputParam, jsAnalyzerConfParam
+		self.string = ""
+		self.currentKeys = []
+		if name == 'analyzer':
+			self.inAnalyzer = True
+		elif name == 'path' and self.inAnalyzer:
+			# store the attributes input and output
+			if 'input' in attrs.keys() and 'output' in attrs.keys():
+				jsAnalyzerInputParam  = attrs.getValue('input')
+				jsAnalyzerOutputParam = attrs.getValue('output')
+			else:
+				raise KeyError("JavaScriptXMLConf: <path> needs 'input' and 'output' attributes")
+		elif name == 'configuration' and self.inAnalyzer:
+			# store the attribute 'param'
+			if 'param' in attrs.keys():
+				jsAnalyzerConfParam  = attrs.getValue('param')
+			else:
+				raise KeyError("JavaScriptXMLConf: <configuration> needs 'param' attribute")
+
+	def characters(self, ch):
+		self.string = self.string + ch
+
+	def endElement(self, name):
+		global jsAnalyzerBin, jsAnalyzerConfFile, jsAnalyzerExtension,jsAnalyzerPattern
+		if name == 'configuration':
+			jsAnalyzerConfFile = normalize_whitespace(self.string)
+		elif name == 'extension' and self.inAnalyzer:
+			jsAnalyzerExtension = normalize_whitespace(self.string)
+		elif name == 'path' and self.inAnalyzer:
+			jsAnalyzerBin = normalize_whitespace(self.string)
+		elif name == "analyzer":
+			self.inAnalyzer = False
+		elif name == "pattern":
+			jsAnalyzerPattern = normalize_whitespace(self.string)
+
+def execCmd(program, args):
+	buff = []
+	p = os.popen(program + " " + args)
+	buff = p.readlines()
+	p.close()
+	return buff
+
+
+def generateListOfFiles(localDB, urlGlobal):
+	global jsDatabase
+	"""
+		Create a ghost in ./local/crystal/current and /local/crystal/analyzed
+		And run the SwA tool
+	"""
+	regScripts = re.compile(r'(.*).' + jsAnalyzerExtension + '$', re.I)
+	# escape () and []
+	localRegOutput = jsAnalyzerPattern
+	localRegOutput = localRegOutput.replace('(', '\(')
+	localRegOutput = localRegOutput.replace(')', '\)')
+	localRegOutput = localRegOutput.replace('[', '\[')
+	localRegOutput = localRegOutput.replace(']', '\]')
+	localRegOutput = localRegOutput.replace(':', '\:')
+	localRegOutput = localRegOutput.replace('__LINE__', '(\d+)')
+	localRegOutput = localRegOutput.replace('__FILENAME__', '(.+)')
+	localRegOutput = localRegOutput.replace('__ERROR__', '(.+)')
+	regOutput = re.compile('^'+localRegOutput+'$', re.I)
+	
+	print "Running the static analysis tool..."
+	for file in localDB:
+		print file
+		file = file.replace(urlGlobal + '/', '')
+		fileIn  = os.path.abspath(os.path.join('./local', file))
+		cmdLine = jsAnalyzerConfParam + " " +jsAnalyzerConfFile + " " + jsAnalyzerInputParam + " " + fileIn
+		if jsAnalyzerOutputParam != "":
+			cmdLine += " " + jsAnalyzerOutputParam + " " + fileIn+'.jslint'
+		output  = execCmd(jsAnalyzerBin, cmdLine)
+		# Analyze the output
+		for o in output:
+			lO = o.replace('\n','')
+			if regOutput.match(lO):
+				out = regOutput.search(lO)
+				if file not in jsDatabase:
+					jsDatabase[file] = {}
+				line = clear_whitespace(out.group(2))
+				if line not in jsDatabase[file]:
+					jsDatabase[file][line] = []
+				jsDatabase[file][line].append(normalize_whitespace(out.group(3)))
+	# sort the dictionary
+	# + file
+	#   + lines
+
+
+def process(urlGlobal, localDB, attack_list):
+	"""
+		Crystal Module entry point
+	"""
+	print "JavaScript Module Start"
+	try:
+		f = open("javascript.conf.xml", 'r')
+		f.close()
+	except IOError:
+		print "The javascript module needs the 'javascript.conf.xml' configuration file."
+		sys.exit(1)
+	parser = make_parser()
+	js_handler = JavaScriptConfHandler()
+	# Tell the parser to use our handler
+	parser.setContentHandler(js_handler)
+	try:
+		parser.parse("javascript.conf.xml")
+	except KeyError, e:
+		print e
+		sys.exit(1)
+
+	# only a white box testing...
+	generateListOfFiles(localDB,urlGlobal)
+	# create the report
+	plop = open('results/javascript_Grabber.xml','w')
+	plop.write("<javascript>\n")
+	plop.write("<site>\n")
+	for file in jsDatabase:
+		plop.write("\t<file name='%s'>\n" % file)
+		for line in jsDatabase[file]:
+			if len(jsDatabase[file][line]) > 1:
+				plop.write("\t\t<line number='%s'>\n" % line)
+				for error in jsDatabase[file][line]:
+					plop.write("\t\t\t<error>%s</error>\n" % htmlencode(error))
+				plop.write("\t\t</line>\n")
+			else:
+				plop.write("\t\t<line number='%s'>%s</line>\n" % (line, htmlencode(jsDatabase[file][line][0])))
+		plop.write("\t</file>\n")
+	plop.write("</site>\n")
+	plop.write("</javascript>\n")
+	plop.close()
+
diff -rupN grabber-original/report.py grabber-new/report.py
--- grabber-original/report.py	1969-12-31 17:00:00.000000000 -0700
+++ grabber-new/report.py	2015-05-06 16:35:54.000000000 -0700
@@ -0,0 +1,108 @@
+output = "<p></p>"
+optionsMessage = ""
+def setOptions(options):
+	global optionsMessage
+	print options
+	optionsMessage = "<br/><div> Looking for: ";
+	if options.sql != False:
+		optionsMessage += "<span class='label label-warning'>SQL Injection</span>"
+	if options.xss != False:
+		optionsMessage += "<span class='label label-info'>XSS Attacks</span>"
+	if options.bsql != False:
+		optionsMessage += "<span class='label label-success'>Blind SQL Injection</span>"
+	if options.include != False:
+		optionsMessage += "<span class='label label-danger'>File Injection</span>"
+	optionsMessage+= "</div>"
+
+def generateHeader(url , isFinal=False):
+	meta = '<meta http-equiv="refresh" content="2">'
+	processing = showProcessing()
+	if isFinal == True:
+		meta = ""
+		processing = ""
+
+	if "Completed" in url:
+		url = """Status: <div class='label label-success'>"""+ url +"""</div>"""
+	elif "Stopped" in url:
+		url = """Status: <div class='label label-danger'>"""+ url +"""</div>"""
+	elif "Indexing" in url:
+		url = """Generating Index: <div class='label label-primary'>"""+ url +"""</div>"""
+	else:
+		url = """Processing <div class='label label-primary'>"""+ url +"""</div>"""
+	header = """<html>
+		<head>"""+meta+"""
+			<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
+			<link rel="stylesheet" type="text/css" href='http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/css/bootstrap.min.css'>
+			<script src='http://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js'></script>	
+			<style>
+				.glyphicon-refresh-animate {
+				    -animation: spin .7s infinite linear;
+				    -ms-animation: spin .7s infinite linear;
+				    -webkit-animation: spinw .7s infinite linear;
+				    -moz-animation: spinm .7s infinite linear;
+				}
+				 
+				@keyframes spin {
+				    from { transform: scale(1) rotate(0deg);}
+				    to { transform: scale(1) rotate(360deg);}
+				}
+				  
+				@-webkit-keyframes spinw {
+				    from { -webkit-transform: rotate(0deg);}
+				    to { -webkit-transform: rotate(360deg);}
+				}
+				 
+				@-moz-keyframes spinm {
+				    from { -moz-transform: rotate(0deg);}
+				    to { -moz-transform: rotate(360deg);}
+				}
+				span.label {
+					margin: 0 5px;
+				}
+
+				.label a {
+					color: white !important
+				}
+
+
+			</style>
+		</head>
+		<body>
+		<div class="container">
+		<div class="jumbotron">
+			<h1 class='h1'>Grabber</h1>
+			<h4>Plain ol' web vulnerability scanner</h4>
+			<hr/>
+			<h4>"""+ optionsMessage +"""</h4>
+			<h2>"""+ url +"""
+			"""+ processing +"""
+			</h2>
+		</div>
+		<div class="panel-group" id="accordion">"""
+	return header
+
+def generateReport(url, isFinal=False):
+	try:
+		f = open("results/report.html", 'w')
+		header = generateHeader(url, isFinal)
+
+		body = header + """
+		"""+ output +"""</div></div></body>
+		</html>"""
+
+		f.write(body)
+		f.close()
+
+	except IOError:
+		print "Failed to create report file"
+		sys.exit(1)
+
+def appendToReport(url, message, isFinal=False):
+	global output
+	output += message
+	generateReport(url, isFinal)
+
+
+def showProcessing():
+	message = '<span class="label label-warning"><span class="glyphicon glyphicon-refresh glyphicon-refresh-animate"></span></span>'
+	return message
\ No newline at end of file
diff -rupN grabber-original/session.py grabber-new/session.py
--- grabber-original/session.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/session.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,73 +1,73 @@
-#!/usr/bin/env python
-"""
-	Session Analyzer Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys,re,time,datetime
-from grabber import getContentDirectURL_GET
-sessions = {}
-
-
-def normalize_whitespace(text):
-	return ' '.join(text.split())
-
-
-def getDirectSessionID(currentURL, sid):
-	k = currentURL.find(sid)
-	if k > 0:
-		return currentURL[k+10:]
-	return None
-
-def stripNoneASCII(output):
-	# should be somepthing to do that.. :/
-	newOutput = ""
-	for s in output:
-		try:
-			s = s.encode()
-			newOutput += s
-		except UnicodeDecodeError:
-			continue
-	return newOutput
-
-regDate = re.compile(r'^Date: (.*)$', re.I)
-
-def lookAtSessionID(url, sidName, regSession):
-	global sessions
-	handle = getContentDirectURL_GET(url,"")
-	if handle != None:
-		output = handle.read()
-		header = str(handle.info()).split('\n')
-		for h in header:
-			# extract date header information
-			if regDate.match(h):
-				out = regDate.search(h)
-				date = out.group(1)
-				# convert this date into the good GMT number
-				# ie time in seconds since 01/01/1970 00:00:00
-				gi = time.strptime(normalize_whitespace(date.replace('GMT','')), "%a, %d %b %Y %H:%M:%S")
-				gi = time.mktime(gi) - time.mktime(time.gmtime(0))
-
-		output = output.replace('\n','')
-		output = output.replace('\t','')
-		# print output[790:821]
-		output = stripNoneASCII(output)
-		if output.find(sidName) > 0:
-			if regSession.match(output):
-				out = regSession.search(output)
-				ssn = out.group(2)
-				if ssn != None:
-					if gi != None:
-						sessions[ssn] = gi
-					else:
-						sessions[ssn] = ''
-
-def process(url, database, sidName):
-	regString  = "(.*)" + sidName + "=([a-z|A-Z|0-9]+)(.*)"
-	regSession = re.compile(regString,re.I)
-	print url, sidName, regString
-	for k in range(0,1000):
-		lookAtSessionID(url, sidName, regSession)
-	o = open('results/sessions.txt','w')
-	for s in sessions:
-		o.write("%s, %s\n" % (s, sessions[s]))
-	o.close()
+#!/usr/bin/env python
+"""
+	Session Analyzer Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys,re,time,datetime
+from grabber import getContentDirectURL_GET
+sessions = {}
+
+
+def normalize_whitespace(text):
+	return ' '.join(text.split())
+
+
+def getDirectSessionID(currentURL, sid):
+	k = currentURL.find(sid)
+	if k > 0:
+		return currentURL[k+10:]
+	return None
+
+def stripNoneASCII(output):
+	# should be somepthing to do that.. :/
+	newOutput = ""
+	for s in output:
+		try:
+			s = s.encode()
+			newOutput += s
+		except UnicodeDecodeError:
+			continue
+	return newOutput
+
+regDate = re.compile(r'^Date: (.*)$', re.I)
+
+def lookAtSessionID(url, sidName, regSession):
+	global sessions
+	handle = getContentDirectURL_GET(url,"")
+	if handle != None:
+		output = handle.read()
+		header = str(handle.info()).split('\n')
+		for h in header:
+			# extract date header information
+			if regDate.match(h):
+				out = regDate.search(h)
+				date = out.group(1)
+				# convert this date into the good GMT number
+				# ie time in seconds since 01/01/1970 00:00:00
+				gi = time.strptime(normalize_whitespace(date.replace('GMT','')), "%a, %d %b %Y %H:%M:%S")
+				gi = time.mktime(gi) - time.mktime(time.gmtime(0))
+
+		output = output.replace('\n','')
+		output = output.replace('\t','')
+		# print output[790:821]
+		output = stripNoneASCII(output)
+		if output.find(sidName) > 0:
+			if regSession.match(output):
+				out = regSession.search(output)
+				ssn = out.group(2)
+				if ssn != None:
+					if gi != None:
+						sessions[ssn] = gi
+					else:
+						sessions[ssn] = ''
+
+def process(url, database, sidName):
+	regString  = "(.*)" + sidName + "=([a-z|A-Z|0-9]+)(.*)"
+	regSession = re.compile(regString,re.I)
+	print url, sidName, regString
+	for k in range(0,1000):
+		lookAtSessionID(url, sidName, regSession)
+	o = open('results/sessions.txt','w')
+	for s in sessions:
+		o.write("%s, %s\n" % (s, sessions[s]))
+	o.close()
diff -rupN grabber-original/setup.py grabber-new/setup.py
--- grabber-original/setup.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/setup.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,4 +1,4 @@
-from distutils.core import setup
-import py2exe
-
+from distutils.core import setup
+import py2exe
+
 setup(console=['grabber.py'])
\ No newline at end of file
diff -rupN grabber-original/spider.py grabber-new/spider.py
--- grabber-original/spider.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/spider.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,578 +1,628 @@
-#!/usr/bin/env python
-"""
-	Spider Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import urllib
-import time
-import re,sys,os,string
-from BeautifulSoup import BeautifulSoup,SoupStrainer
-from urllib2 import URLError, HTTPError
-COOKIEFILE = 'cookies.lwp'          # the path and filename that you want to use to save your cookies in
-import os.path
-cj = None
-ClientCookie = None
-cookielib = None
-
-import cookielib
-import urllib2
-urlopen = urllib2.urlopen
-cj = cookielib.LWPCookieJar()       # This is a subclass of FileCookieJar that has useful load and save methods
-Request = urllib2.Request
-txdata = None
-refererUrl = "http://google.com/?q=you!"
-txheaders = {'User-agent' : 'Grabber/0.1 (X11; U; Linux i686; en-US; rv:1.7)', 'Referer' : refererUrl}
-
-allowed=['php','html','htm','xml','xhtml','xht','xhtm',
-         'asp','aspx','msp','mspx','php3','php4','php5','txt','shtm',
-	    'shtml','phtm','phtml','jhtml','pl','jsp','cfm','cfml','do','py',
-		'js', 'css']
-database     = {}
-database_url = []
-database_css = []
-database_js  = []
-database_ext = [] # database of unsecure external links
-local_url    = []
-dumb_params  = [] # if there is no parameters associated with a given URL, associate this list of "whatever looks like"
-root = "http://localhost"
-
-
-outSpiderFile = None
-
-"""
-	database = {
-	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }},
-	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }},
-	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }}
-	}
-"""
-_urlEncode = {}
-for i in range(256):
-	_urlEncode[chr(i)] = '%%%02x' % i
-for c in string.letters + string.digits + '_,.-/':
-	_urlEncode[c] = c
-_urlEncode[' '] = '+'
-
-
-def urlEncode(s):
-	""" 
-		Returns the encoded version of the given string, safe for using as a URL. 
-	"""
-	return string.join(map(lambda c: _urlEncode[c], list(s)), '')
-
-
-
-def urlDecode(s):
-	""" 
-		Returns the decoded version of the given string. Note that invalid URLs will throw exceptons. 
-		For example, a URL whose % coding is incorrect. 
-	"""
-	mychr = chr
-	atoi = string.atoi
-	parts = string.split(string.replace(s, '+', ' '), '%')
-	for i in range(1, len(parts)):
-		part = parts[i]
-		parts[i] = mychr(atoi(part[:2], 16)) + part[2:]
-	return string.join(parts, '')
-
-
-
-def htmlencode(s):
-	"""
-		Escaping the HTML special characters
-	"""
- 	s = s.replace("&", "&amp;")
-	s = s.replace("<", "&lt;")
-	s = s.replace(">", "&gt;")
-	s = s.replace("\"","&quot;")
-	s = s.replace("'", "&apos;")
-	return s
-
-
-
-def htmldecode(s):
-	"""
-		Unescaping the HTML special characters
-	"""
-	s = s.replace("&lt;", "<")
-	s = s.replace("&gt;", ">")
-	s = s.replace("&quot;", "\"")
-	s = s.replace("&apos;","'")
-	s = s.replace("&amp;", "&")
-	return s
-
-
-
-def getContentDirectURL_GET(url, string):
-	"""
-		Get the content of the url by GET method
-	"""
-	ret = ""
-	try:
-		if len(string) > 0:
-			url = url + "?" + (string)
-		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
-		urllib2.install_opener(opener)
-		req = Request(url, None, txheaders) # create a request object
-		ret = urlopen(req)                     # and open it to return a handle on the url
-	except HTTPError, e:
-		return
-	except URLError, e:
-		return
-	except IOError:
-		return
-	return ret
-
-
-
-def scan(currentURL):
-	"""
-		The Scanner is the first part of Grabber.
-		It retrieves every information of the HTML page
-		TODO:
-			Reading in every href='' element for CSS and src='' for JavaScript / Image
-	"""
-	try:
-		archives_hDl = getContentDirectURL_GET(currentURL,'')
-	except IOError:
-		log <= ("IOError @ %s" % currentURL)
-	try:
-		htmlContent= archives_hDl.read()
-	except IOError, e:
-		print "Cannot open the file,",(e.strerror)
-		return
-	except AttributeError:
-		print ("Grabber cannot retrieve the given url: %s" % currentURL)
-		return
-	parseHtmlLinks (currentURL,htmlContent)
-	parseHtmlParams(currentURL,htmlContent)
-
-def allowedExtensions(plop):
-	for e in allowed:
-		if '.'+e in plop:
-			return True
-	return False
-
-
-
-def makeRoot(urlLocal):
-	if allowedExtensions(urlLocal):
-		return urlLocal[0:urlLocal.rfind('/')+1]
-	return urlLocal
-
-
-
-def giveGoodURL(href, urlLocal):
-	"""
-		It should return a good url...
-		href = argument retrieven from the href...
-	"""
-	if 'javascript' in href:
-		return htmldecode(urlLocal)
-	if 'http://' in href or 'https://' in href:
-		if urlLocal in href:
-			return htmldecode(href)
-		else:
-			return urlLocal
-	if len(href) < 1:
-		return htmldecode(urlLocal)
-	if href[0] == '?' and '?' not in urlLocal and not allowedExtensions(urlLocal):
-		for e in allowed:
-			if '.'+e in urlLocal:
-				return htmldecode(urlLocal + href)
-		return htmldecode(urlLocal + '/' + href)
-	else:
-		# simple name
-		if allowedExtensions(urlLocal) or '?' in urlLocal:
-			return htmldecode(urlLocal[0:urlLocal.rfind('/')+1] + href)
-		else:
-			return htmldecode(urlLocal + '/' + href)
-	return htmldecode(href)
-
-
-def dl(fileAdress, destFile):
-	"""
-		Download the file
-	"""
-	try:
-		f =  urllib.urlopen(fileAdress)
-		g = f.read()
-		file = open(os.path.join('./', destFile), "wb")
-	except IOError:
-		return False
-	file.write(g)
-	file.close()
-	return True
-
-
-def removeSESSID(urlssid):
-	"""
-		Remove the phpsessid information... don't care about it now
-	"""
-	k = urlssid.find('PHPSESSID')
-	if k > 0:
-		return urlssid[0:k-1]
-	k = urlssid.find('sid')
-	if k > 0:
-		return urlssid[0:k-1]
-	return urlssid
-
-def parseHtmlLinks(currentURL,htmlContent):
-	global database_url,database_js,database_css
-	"""
-		Parse the HTML/XHTML code to get JS, CSS, links etc.
-	"""
-	links = SoupStrainer('a')
-	# listAnchors = [tag['href'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=links)]
-	listAnchors = []
-	for tag in BeautifulSoup(htmlContent, parseOnlyThese=links):
-		try:
-			string = str(tag).lower()
-			if string.count("href") > 0:
-				listAnchors.append(tag['href'])
-		except TypeError:
-			continue
-		except KeyError:
-			continue
-
-	for a in listAnchors:
-		goodA = giveGoodURL(a,currentURL)
-		goodA = removeSESSID(goodA)
-		if (root in goodA) and (goodA not in database_url):
-			database_url.append(goodA)
-
-	# parse the CSS and the JavaScript
-	script = SoupStrainer('script')
-	#listScripts = [tag['src'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=script)]
-	listScripts = []
-	for tag in BeautifulSoup(htmlContent, parseOnlyThese=script):
-		try:
-			string = str(tag).lower()
-			if string.count("src") > 0 and string.count(".src") < 1:
-				listScripts.append(tag['src'])
-		except TypeError:
-			continue
-		except KeyError:
-			continue
-
-	for a in listScripts:
-		sc = giveGoodURL(a,currentURL)
-		if sc not in database_js:
-			database_js.append(sc)
-		if sc == currentURL:
-			# remote script
-			database_ext.append(sc)
-	parseJavaScriptCalls()
-
-	link = SoupStrainer('link')
-	# listLinks = [tag['href'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=link)]
-	listLinks = []
-	for tag in BeautifulSoup(htmlContent, parseOnlyThese=link):
-		try:
-			string = str(tag).lower()
-			if string.count("href") > 0:
-				listLinks.append(tag['href'])
-		except TypeError:
-			continue
-		except KeyError:
-			continue
-
-	for a in listLinks:
-		sc = giveGoodURL(a,currentURL)
-		if sc not in database_css:
-			database_css.append(sc)
-	return True
-
-jsChars = ["'",'"']
-
-def rfindFirstJSChars(string):
-	b = [string.rfind(k) for k in jsChars]
-	return max(b)
-
-regDumbParam = re.compile(r'(\w+)')
-regDumbParamNumber = re.compile(r'(\d+)')
-
-jsParams = ["'",'"','=','+','%','\\',')','(','^','*','-']
-
-def cleanListDumbParams(listDumb):
-	newDumbList = []
-	for w in listDumb:
-		w = w.replace(' ','')
-		w = w.replace('\n','')
-		#l = [c for c in jsParams if c in w] # no jsParams
-		if len(w) > 0 and regDumbParam.match(w) and not regDumbParamNumber.match(w):
-			newDumbList.append(w)
-	return newDumbList
-
-def unique(L):
-	noDupli=[]
-	[noDupli.append(i) for i in L if not noDupli.count(i)]
-	return noDupli
-
-def flatten(L):
-	if type(L) != type([]):
-		return [L]
-	if L == []:
-		return L
-	return reduce(lambda L1,L2:L1+L2,map(flatten,L))
-
-
-def parseJavaScriptContent(jsContent):
-	global database_url, database_ext, dumb_params
-	"""
-		Parse the content of a JavaScript file
-	"""
-	for l in jsContent.readlines():
-		for e in allowed:
-			if l.count('.'+e) > 0:
-				# we found an external a call
-				if l.count('http://') > 0 and l.count(root) < 1:
-					# External link
-					et= '.'+e
-					b1 = l.find('http://')
-					b2 = l.find(et) + len(et)
-					database_ext.append(l[b1:b2])
-				else:
-					# Internal link
-					et= '.'+e
-					b2 = l.find(et) + len(et)
-					b1 = rfindFirstJSChars(l[:b2])+1
-					database_url.append(giveGoodURL(l[b1:b2],root))
-		# try to get a parameter
-		k = l.find('?')
-		if k > 0:
-			results = l[k:].split('?')
-			plop = []
-			for a in results:
-				plop.append(cleanListDumbParams(regDumbParam.split(a)))
-			dumb_params.append(flatten(plop))
-		k = l.find('&')
-		if k > 0:
-			results = l[k:].split('&')
-			plop = []
-			for a in results:
-				plop.append(cleanListDumbParams(regDumbParam.split(a)))
-			plop = flatten(plop)
-			dumb_params.append(flatten(plop))
-	dumb_params = unique(flatten(dumb_params))
-
-def parseJavaScriptCalls():
-	global database_js
-	"""
-		Parse the JavaScript and download the files
-	"""
-	for j in database_js:
-		jsName = j[j.rfind('/')+1:]
-		if not os.path.exists('local/js/' + jsName):
-			# first download the file
-			dl(j,'local/js/' + jsName)
-			try:
-				jsContent = open('local/js/' + jsName, 'r')
-			except IOError:
-				continue
-			parseJavaScriptContent(jsContent)
-			jsContent.close()
-
-def splitQuery(query_string):
-	"""
-		Split the num=plop&truc=kikoo&o=42 into
-		a dictionary
-	"""
-	try:
-		d = dict([x.split('=') for x in query_string.split('&') ])
-	except ValueError:
-		d = {}
-	return d
-
-def dict_add(d1,d2):
-	"""
-		Flatten 2 dictionaries
-	"""
-	d={}
-	if len(d1):
-		for s in d1.keys():
-			d[s] = d1[s]
-	if len(d2):
-		for s in d2.keys():
-			d[s] = d2[s]
-	return d
-
-def dict_add_list(d1,l1):
-	d={}
-	if len(d1):
-		for s in d1.keys():
-			d[s] = d1[s]
-	if len(l1):
-		for s in l1:
-			d[s] = 'bar'
-	return d
-
-def parseHtmlParams(currentURL, htmlContent):
-	global database, database_css, database_js
-	"""
-		Parse html to get args
-	"""
-	for url in database_url:
-		k = url.find('?')
-		if k > 0:
-			keyUrl = url[0:k-1]
-			query = url[k+1:]
-			if not keyUrl in database:
-				database[keyUrl] = {}
-				database[keyUrl]['GET']  = {}
-				database[keyUrl]['POST'] = {}
-			lG = database[keyUrl]['GET']
-			lG = dict_add(lG,splitQuery(query))
-			database[keyUrl]['GET']  = lG
-		elif len(dumb_params) > 0:
-			keyUrl = url
-			# no params in the URL... let's assign the dumb_params
-			if not keyUrl in database:
-				database[keyUrl] = {}
-				database[keyUrl]['GET']  = {}
-				database[keyUrl]['POST'] = {}
-			lG = database[keyUrl]['GET']
-			lP = database[keyUrl]['POST']
-			lG = dict_add_list(lG,dumb_params)
-			lP = dict_add_list(lP,dumb_params)
-			database[keyUrl]['GET']  = lG
-			database[keyUrl]['POST'] = lP
-
-	# then, parse the forms
-	forms = SoupStrainer('form')
-	input = SoupStrainer('input')
-	listForm = [tag for tag in BeautifulSoup(htmlContent, parseOnlyThese=forms)]
-	for f in listForm:
-		method = 'GET'
-		if 'method' in f or 'METHOD' in f:
-			method = f['method'].upper()
-		action = currentURL
-		if 'action' in f or 'ACTION' in f:
-			action = f['action']
-		keyUrl = giveGoodURL(action,currentURL)
-		listInput = [tag for tag in BeautifulSoup(str(f), parseOnlyThese=input)]
-		for i in listInput:
-			if not keyUrl in database:
-				database[keyUrl] = {}
-				database[keyUrl]['GET']  = {}
-				database[keyUrl]['POST'] = {}
-			try:
-				value = i['value']
-			except KeyError:
-				value = '42'
-			try:
-				name = i['name']
-			except KeyError:
-				name = 'foo'
-				value= 'bar'
-				continue
-			lGP = database[keyUrl][method]
-			lGP = dict_add(lGP,{name : value})
-			database[keyUrl][method] = lGP
-	return True
-
-
-def runSpiderScan(entryUrl, depth = 0):
-	global outSpiderFile
-	print "runSpiderScan @ ", entryUrl, " |   #",depth
-	if outSpiderFile:
-		outSpiderFile.write("\t\t<entryURL>%s</entryURL>\n" % entryUrl)
-	scan(entryUrl)
-	if depth > 0 and len(database_url) > 0:
-		for a in database_url:
-			runSpiderScan(a, depth-1)
-		return False
-	return True
-
-
-def spider(entryUrl, depth = 0):
-	global root,outSpiderFile
-	"""
-		Retrieve every links
-	"""
-	if depth > 0:
-		root = makeRoot(entryUrl)
-	else:
-		root = entryUrl
-	
-	# test if the spider has already be done on this website
-	try:
-		f = open("local/spiderSite.xml", 'r')
-		firstLine = f.readline()
-		f.close()
-		if firstLine.count(root) > 0:
-			alreadyScanned = True
-		else:
-			alreadyScanned = False
-	except IOError:
-		alreadyScanned = False
-
-	print "Start scanning...", root
-	if depth == 0:
-		scan(root)
-	else:
-		if not alreadyScanned:
-			outSpiderFile = open("local/spiderSite.xml","w")
-			outSpiderFile.write("<spider root='%s' depth='%d'>\n" % (root,depth) )
-			runSpiderScan(root, depth)
-			if len(dumb_params) > 0:
-				outSpiderFile.write("<dumb_parameters>\n")
-				for d in dumb_params:
-					outSpiderFile.write("\t<dumb>%s</dumb>\n" % (d))
-				outSpiderFile.write("</dumb_parameters>\n")
-			outSpiderFile.write("\n</spider>")
-			outSpiderFile.close()
-		else:
-			print "Loading the previous spider results from 'local/spiderSite.xml'"
-			# load the XML file
-			regUrl = re.compile(r'(.*)<entryURL>(.*)</entryURL>(.*)',re.I)
-			regDmb = re.compile(r'(.*)<dumb>(.*)</dumb>(.*)',re.I)
-
-			f = open("local/spiderSite.xml", 'r')
-			for l in f.readlines():
-				if regUrl.match(l):
-					out = regUrl.search(l)
-					url = out.group(2)
-					database_url.append(url)
-				if regDmb.match(l):
-					out = regDmb.search(l)
-					param = out.group(2)
-					dumb_params.append(param)
-			f.close()
-
-			# scan every url
-			for currentURL in database_url:
-				try:
-					archives_hDl = getContentDirectURL_GET(currentURL,'')
-				except IOError:
-					log <= ("IOError @ %s" % currentURL)
-					continue
-				try:
-					htmlContent= archives_hDl.read()
-				except IOError, e:
-					continue
-				except AttributeError, e:
-					continue
-				parseHtmlParams(currentURL,htmlContent)
-
-
-	outSpiderFile = open("results/touchFiles.xml","w")
-	outSpiderFile.write("<spider root='%s'>\n" % root)
-	for i in database_url:
-		outSpiderFile.write("\t<url type='anchor'>%s</url>\n" % i)
-	for i in database_js:
-		outSpiderFile.write("\t<url type='JavaScript'>%s</url>\n" % i)
-	for i in database_css:
-		outSpiderFile.write("\t<url type='MetaLink'>%s</url>\n" % i)
-	outSpiderFile.write("</spider>")
-	outSpiderFile.close()
-
-	if len(database_ext) > 0:
-		# alert of External calls
-		outSpiderFile = open("results/externalCalls.xml","w")
-		outSpiderFile.write("<external>\n")
-		for i in database_ext:
-			outSpiderFile.write("\t<call severity='high'>%s</call>\n" % i)
-		outSpiderFile.write("</external>")
-		outSpiderFile.close()
-
+#!/usr/bin/env python
+"""
+	Spider Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import urllib
+import time
+import re,sys,os,string
+import ssl
+from BeautifulSoup import BeautifulSoup,SoupStrainer
+from urllib2 import URLError, HTTPError
+from urlparse import urljoin
+import os.path
+from report import appendToReport
+
+COOKIEFILE = 'cookies.lwp'          # the path and filename that you want to use to save your cookies in
+cj = None
+ClientCookie = None
+cookielib = None
+
+import cookielib
+import urllib2
+
+urlopen = urllib2.urlopen
+cj = cookielib.LWPCookieJar()       # This is a subclass of FileCookieJar that has useful load and save methods
+Request = urllib2.Request
+txdata = None
+refererUrl = "http://google.com/?q=you!"
+
+
+allowed=['php','html','htm','xml','xhtml','xht','xhtm',
+         'asp','aspx','msp','mspx','php3','php4','php5','txt','shtm',
+	    'shtml','phtm','phtml','jhtml','pl','jsp','cfm','cfml','do','py',
+		'js', 'css']
+database     = {}
+database_url = []
+database_css = []
+database_js  = []
+database_ext = [] # database of unsecure external links
+local_url    = []
+dumb_params  = [] # if there is no parameters associated with a given URL, associate this list of "whatever looks like"
+root = "http://localhost"
+
+
+outSpiderFile = None
+
+"""
+	database = {
+	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }},
+	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }},
+	 u"URL" : {'GET' : {'param1':value}, 'POST' : { 'param2' : value }}
+	}
+"""
+_urlEncode = {}
+for i in range(256):
+	_urlEncode[chr(i)] = '%%%02x' % i
+for c in string.letters + string.digits + '_,.-/':
+	_urlEncode[c] = c
+_urlEncode[' '] = '+'
+
+
+def urlEncode(s):
+	""" 
+		Returns the encoded version of the given string, safe for using as a URL. 
+	"""
+	return string.join(map(lambda c: _urlEncode[c], list(s)), '')
+
+
+
+def urlDecode(s):
+	""" 
+		Returns the decoded version of the given string. Note that invalid URLs will throw exceptons. 
+		For example, a URL whose % coding is incorrect. 
+	"""
+	mychr = chr
+	atoi = string.atoi
+	parts = string.split(string.replace(s, '+', ' '), '%')
+	for i in range(1, len(parts)):
+		part = parts[i]
+		parts[i] = mychr(atoi(part[:2], 16)) + part[2:]
+	return string.join(parts, '')
+
+
+
+def htmlencode(s):
+	"""
+		Escaping the HTML special characters
+	"""
+ 	s = s.replace("&", "&amp;")
+	s = s.replace("<", "&lt;")
+	s = s.replace(">", "&gt;")
+	s = s.replace("\"","&quot;")
+	s = s.replace("'", "&apos;")
+	return s
+
+
+
+def htmldecode(s):
+	"""
+		Unescaping the HTML special characters
+	"""
+	s = s.replace("&lt;", "<")
+	s = s.replace("&gt;", ">")
+	s = s.replace("&quot;", "\"")
+	s = s.replace("&apos;","'")
+	s = s.replace("&amp;", "&")
+	return s
+
+
+
+def getContentDirectURL_GET(url, string):
+	"""
+		Get the content of the url by GET method
+	"""
+	ret = ""
+	try:
+		if len(string) > 0:
+			url = url + "?" + (string)
+		opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
+		urllib2.install_opener(opener)
+		context = ssl._create_unverified_context()
+		req = Request(url, None, txheaders) # create a request object
+		ret = urlopen(req, context=context)                     # and open it to return a handle on the url
+	except HTTPError, e:
+		print e
+		return
+	except URLError, e:
+		print e
+		return
+	except IOError:
+		return
+	return ret
+
+
+
+def scan(currentURL):
+	"""
+		The Scanner is the first part of Grabber.
+		It retrieves every information of the HTML page
+		TODO:
+			Reading in every href='' element for CSS and src='' for JavaScript / Image
+	"""
+	try:
+		archives_hDl = getContentDirectURL_GET(currentURL,'')
+	except IOError:
+		log <= ("IOError @ %s" % currentURL)
+	try:
+		htmlContent= archives_hDl.read()
+		#print archives_hDl.info()
+	except IOError, e:
+		print "Cannot open the file,",(e.strerror)
+		return
+	except AttributeError:
+		print ("Grabber cannot retrieve the given url: %s" % currentURL)
+		return
+ 	print currentURL
+	parseHtmlLinks(currentURL,htmlContent)
+	parseHtmlParams(currentURL,htmlContent)
+
+def allowedExtensions(plop):
+	for e in allowed:
+		if '.'+e in plop:
+			return True
+	return False
+
+
+
+def makeRoot(urlLocal):
+	if allowedExtensions(urlLocal):
+		return urlLocal[0:urlLocal.rfind('/')+1]
+	return urlLocal
+
+
+
+def giveGoodURL(href, urlLocal):
+	
+	"""
+		It should return a good url...
+		href = argument retrieven from the href...
+	"""
+	if 'javascript' in href:
+		return htmldecode(urlLocal)
+	if 'http://' in href or 'https://' in href:
+		if urlLocal in href:
+			return htmldecode(href)
+		else:
+			return urlLocal
+	if len(href) < 1:
+		return htmldecode(urlLocal)
+	if href[0] == '?' and '?' not in urlLocal and not allowedExtensions(urlLocal):
+		for e in allowed:
+			if '.'+e in urlLocal:
+				return htmldecode(urlLocal + href)
+		if urlLocal[len(urlLocal)-1] != '/':
+			urlLocal += '/'
+		return htmldecode(urlLocal + href)
+	if href[0] == '/':
+		 temp_url = urljoin(urlLocal, href)
+		 return htmldecode(temp_url)
+	else:
+		# simple name
+		if allowedExtensions(urlLocal) or '?' in urlLocal:
+			return htmldecode(urlLocal[0:urlLocal.rfind('/')+1] + href)
+		else:
+			if urlLocal[len(urlLocal)-1] != '/':
+				urlLocal += '/'
+			return htmldecode(urlLocal + href)
+	return htmldecode(href)
+
+def dl(fileAdress, destFile):
+	"""
+		Download the file
+	"""
+	try:
+		f =  urllib.urlopen(fileAdress)
+		g = f.read()
+		file = open(os.path.join('./', destFile), "wb")
+	except IOError:
+		return False
+	file.write(g)
+	file.close()
+	return True
+
+
+def removeSESSID(urlssid):
+	"""
+		Remove the phpsessid information... don't care about it now
+	"""
+	k = urlssid.find('PHPSESSID')
+	if k > 0:
+		return urlssid[0:k-1]
+	k = urlssid.find('sid')
+	if k > 0:
+		return urlssid[0:k-1]
+	return urlssid
+
+def parseHtmlLinks(currentURL,htmlContent):
+	global database_url,database_js,database_css
+	"""
+		Parse the HTML/XHTML code to get JS, CSS, links etc.
+	"""
+	links = SoupStrainer('a')
+	# listAnchors = [tag['href'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=links)]
+	listAnchors = []
+
+	for tag in BeautifulSoup(htmlContent, parseOnlyThese=links):
+		try:
+			string = str(tag).lower()
+			if string.count("href") > 0:
+				listAnchors.append(tag.get('href'))
+		except TypeError:
+			continue
+		except KeyError:
+			continue
+
+	for a in listAnchors:
+		goodA = giveGoodURL(a,currentURL)
+		goodA = removeSESSID(goodA)
+		if (goodA not in database_url):
+			database_url.append(goodA)
+
+	# parse the CSS and the JavaScript
+	script = SoupStrainer('script')
+	#listScripts = [tag['src'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=script)]
+	listScripts = []
+	for tag in BeautifulSoup(htmlContent, parseOnlyThese=script):
+		try:
+			string = str(tag).lower()
+			if string.count("src") > 0 and string.count(".src") < 1:
+				listScripts.append(tag.get('src'))
+		except TypeError:
+			continue
+		except KeyError:
+			continue
+
+	for a in listScripts:
+		sc = giveGoodURL(a,currentURL)
+		if sc not in database_js:
+			database_js.append(sc)
+		if sc == currentURL:
+			# remote script
+			database_ext.append(sc)
+	parseJavaScriptCalls()
+
+	link = SoupStrainer('link')
+	# listLinks = [tag['href'] for tag in BeautifulSoup(htmlContent, parseOnlyThese=link)]
+	listLinks = []
+	for tag in BeautifulSoup(htmlContent, parseOnlyThese=link):
+		try:
+			string = str(tag).lower()
+			if string.count("href") > 0:
+				listLinks.append(tag.get('href'))
+		except TypeError:
+			continue
+		except KeyError:
+			continue
+
+	for a in listLinks:
+		sc = giveGoodURL(a,currentURL)
+		if sc not in database_css:
+			database_css.append(sc)
+	return True
+
+jsChars = ["'",'"']
+
+def rfindFirstJSChars(string):
+	b = [string.rfind(k) for k in jsChars]
+	return max(b)
+
+regDumbParam = re.compile(r'(\w+)')
+regDumbParamNumber = re.compile(r'(\d+)')
+
+jsParams = ["'",'"','=','+','%','\\',')','(','^','*','-']
+
+def cleanListDumbParams(listDumb):
+	newDumbList = []
+	for w in listDumb:
+		w = w.replace(' ','')
+		w = w.replace('\n','')
+		#l = [c for c in jsParams if c in w] # no jsParams
+		if len(w) > 0 and regDumbParam.match(w) and not regDumbParamNumber.match(w):
+			newDumbList.append(w)
+	return newDumbList
+
+def unique(L):
+	noDupli=[]
+	[noDupli.append(i) for i in L if not noDupli.count(i)]
+	return noDupli
+
+def flatten(L):
+	if type(L) != type([]):
+		return [L]
+	if L == []:
+		return L
+	return reduce(lambda L1,L2:L1+L2,map(flatten,L))
+
+
+def parseJavaScriptContent(jsContent):
+	global database_url, database_ext, dumb_params
+	"""
+		Parse the content of a JavaScript file
+	"""
+	for l in jsContent.readlines():
+		for e in allowed:
+			if l.count('.'+e) > 0:
+				# we found an external a call
+				if l.count('http://') > 0 and l.count(root) < 1:
+					# External link
+					et= '.'+e
+					b1 = l.find('http://')
+					b2 = l.find(et) + len(et)
+					database_ext.append(l[b1:b2])
+				else:
+					# Internal link
+					et= '.'+e
+					b2 = l.find(et) + len(et)
+					b1 = rfindFirstJSChars(l[:b2])+1
+					database_url.append(giveGoodURL(l[b1:b2],root))
+		# try to get a parameter
+		k = l.find('?')
+		if k > 0:
+			results = l[k:].split('?')
+			plop = []
+			for a in results:
+				plop.append(cleanListDumbParams(regDumbParam.split(a)))
+			dumb_params.append(flatten(plop))
+		k = l.find('&')
+		if k > 0:
+			results = l[k:].split('&')
+			plop = []
+			for a in results:
+				plop.append(cleanListDumbParams(regDumbParam.split(a)))
+			plop = flatten(plop)
+			dumb_params.append(flatten(plop))
+	dumb_params = unique(flatten(dumb_params))
+
+def parseJavaScriptCalls():
+	global database_js
+	"""
+		Parse the JavaScript and download the files
+	"""
+	for j in database_js:
+		jsName = j[j.rfind('/')+1:]
+		if not os.path.exists('local/js/' + jsName):
+			# first download the file
+			dl(j,'local/js/' + jsName)
+			try:
+				jsContent = open('local/js/' + jsName, 'r')
+			except IOError:
+				continue
+			parseJavaScriptContent(jsContent)
+			jsContent.close()
+
+def splitQuery(query_string):
+	"""
+		Split the num=plop&truc=kikoo&o=42 into
+		a dictionary
+	"""
+	try:
+		d = dict([x.split('=') for x in query_string.split('&') ])
+	except ValueError:
+		d = {}
+	return d
+
+def dict_add(d1,d2):
+	"""
+		Flatten 2 dictionaries
+	"""
+	d={}
+	if len(d1):
+		for s in d1.keys():
+			d[s] = d1[s]
+	if len(d2):
+		for s in d2.keys():
+			d[s] = d2[s]
+
+	return d
+
+def dict_add_list(d1,l1):
+	d={}
+	if len(d1):
+		for s in d1.keys():
+			d[s] = d1[s]
+	if len(l1):
+		for s in l1:
+			d[s] = 'bar'
+	return d
+
+def parseHtmlParams(currentURL, htmlContent):
+	global database, database_css, database_js
+	"""
+		Parse html to get args
+	"""
+	for url in database_url:
+		k = url.find('?')
+		if k > 0:
+			keyUrl = url[0:k]
+			query = url[k+1:]
+			if not keyUrl in database:
+				database[keyUrl] = {}
+				database[keyUrl]['GET']  = {}
+				database[keyUrl]['POST'] = {}
+			lG = database[keyUrl]['GET']
+			lG = dict_add(lG,splitQuery(query))
+			database[keyUrl]['GET']  = lG
+		elif len(dumb_params) > 0:
+			keyUrl = url
+			# no params in the URL... let's assign the dumb_params
+			if not keyUrl in database:
+				database[keyUrl] = {}
+				database[keyUrl]['GET']  = {}
+				database[keyUrl]['POST'] = {}
+			lG = database[keyUrl]['GET']
+			lP = database[keyUrl]['POST']
+			lG = dict_add_list(lG,dumb_params)
+			lP = dict_add_list(lP,dumb_params)
+			database[keyUrl]['GET']  = lG
+			database[keyUrl]['POST'] = lP
+
+	# then, parse the forms
+	forms = SoupStrainer('form')
+	input = SoupStrainer('input')
+	textarea = SoupStrainer('textarea')
+
+	listForm = [tag for tag in BeautifulSoup(htmlContent, parseOnlyThese=forms)]
+	
+	for f in listForm:
+		method = 'GET'
+		if f.get('method'):
+			method = f.get('method').upper()
+		action = currentURL
+		if f.get('action'):
+			action = f.get('action')
+		keyUrl = giveGoodURL(action,currentURL)
+
+		listInput = [tag for tag in BeautifulSoup(str(f), parseOnlyThese=input)]
+		for i in listInput:
+			if not keyUrl in database:
+				database[keyUrl] = {}
+				database[keyUrl]['GET']  = {}
+				database[keyUrl]['POST'] = {}
+			try:
+				value = i.get('value')
+			except KeyError:
+				value = '42'
+			try:
+				name = i.get('name')
+			except KeyError:
+				name = 'foo'
+				value= 'bar'
+				continue
+			lGP = database[keyUrl][method]
+			d2  = { str(name) : value }
+			lGP = dict_add(lGP, d2)
+			database[keyUrl][method] = lGP
+
+		textarea = [tag for tag in BeautifulSoup(str(f), parseOnlyThese=textarea)]
+		for t in textarea:
+			if not keyUrl in database:
+				database[keyUrl] = {}
+				database[keyUrl]['GET'] = {}
+				database[keyUrl]['POST'] = {}
+			try:
+				value = t.get('value')
+			except KeyError:
+				value = "test"
+			try:
+				name = t.get('name')
+			except KeyError:
+				name = "comment"
+				continue
+
+			lGP = database[keyUrl][method]
+			d2  = { str(name) : value }
+			lGP = dict_add(lGP, d2)
+			database[keyUrl][method] = lGP
+
+	return True
+
+
+def runSpiderScan(entryUrl, depth = 0):
+	global outSpiderFile
+	print "runSpiderScan @ ", entryUrl, " |   #",depth
+	if outSpiderFile:
+		outSpiderFile.write("\t\t<entryURL>%s</entryURL>\n" % entryUrl)
+	scan(entryUrl)
+	if depth > 0 and len(database_url) > 0:
+		for a in database_url:
+			runSpiderScan(a, depth-1)
+		return False
+	return True
+
+
+def spider(entryUrl, headers, depth = 0):
+	print entryUrl
+	global root,outSpiderFile
+	global txheaders
+	txheaders = headers
+	"""
+		Retrieve every links
+	"""
+	if depth > 0:
+		root = makeRoot(entryUrl)
+	else:
+		root = entryUrl
+	
+	# test if the spider has already be done on this website
+	try:
+		f = open("local/spiderSite.xml", 'r')
+		firstLine = f.readline()
+		f.close()
+		if firstLine.count(root) > 0:
+			alreadyScanned = True
+		else:
+			alreadyScanned = False
+	except IOError:
+		alreadyScanned = False
+
+	print "Start scanning...", root
+	appendToReport("Indexing - " + entryUrl, "", False)
+	if depth == 0:
+		scan(root)
+	else:
+		if not alreadyScanned:
+			outSpiderFile = open("local/spiderSite.xml","w")
+			outSpiderFile.write("<spider root='%s' depth='%d'>\n" % (root,depth) )
+			runSpiderScan(root, depth)
+			if len(dumb_params) > 0:
+				outSpiderFile.write("<dumb_parameters>\n")
+				for d in dumb_params:
+					outSpiderFile.write("\t<dumb>%s</dumb>\n" % (d))
+				outSpiderFile.write("</dumb_parameters>\n")
+			outSpiderFile.write("\n</spider>")
+			outSpiderFile.close()
+		else:
+			print "Loading the previous spider results from 'local/spiderSite.xml'"
+			# load the XML file
+			regUrl = re.compile(r'(.*)<entryURL>(.*)</entryURL>(.*)',re.I)
+			regDmb = re.compile(r'(.*)<dumb>(.*)</dumb>(.*)',re.I)
+
+			f = open("local/spiderSite.xml", 'r')
+			for l in f.readlines():
+				if regUrl.match(l):
+					out = regUrl.search(l)
+					url = out.group(2)
+					database_url.append(url)
+				if regDmb.match(l):
+					out = regDmb.search(l)
+					param = out.group(2)
+					dumb_params.append(param)
+			f.close()
+
+			# scan every url
+			for currentURL in database_url:
+				try:
+					archives_hDl = getContentDirectURL_GET(currentURL,'')
+				except IOError:
+					log <= ("IOError @ %s" % currentURL)
+					continue
+				try:
+					htmlContent= archives_hDl.read()
+				except IOError, e:
+					continue
+				except AttributeError, e:
+					continue
+				parseHtmlParams(currentURL,htmlContent)
+
+
+	outSpiderFile = open("results/touchFiles.xml","w")
+	outSpiderFile.write("<spider root='%s'>\n" % root)
+	for i in database_url:
+		outSpiderFile.write("\t<url type='anchor'>%s</url>\n" % i)
+	for i in database_js:
+		outSpiderFile.write("\t<url type='JavaScript'>%s</url>\n" % i)
+	for i in database_css:
+		outSpiderFile.write("\t<url type='MetaLink'>%s</url>\n" % i)
+	outSpiderFile.write("</spider>")
+	outSpiderFile.close()
+
+	if len(database_ext) > 0:
+		# alert of External calls
+		outSpiderFile = open("results/externalCalls.xml","w")
+		outSpiderFile.write("<external>\n")
+		for i in database_ext:
+			outSpiderFile.write("\t<call severity='high'>%s</call>\n" % i)
+		outSpiderFile.write("</external>")
+		outSpiderFile.close()
+
diff -rupN grabber-original/sql.py grabber-new/sql.py
--- grabber-original/sql.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/sql.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,112 +1,133 @@
-#!/usr/bin/env python
-"""
-	SQL Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys
-from grabber import getContent_POST, getContent_GET
-from grabber import getContentDirectURL_GET, getContentDirectURL_POST
-from grabber import single_urlencode
-
-
-def detect_sql(output, url_get = "http://localhost/?param=false"):
-	listWords = ["SQL syntax","valid MySQL","ODBC Microsoft Access Driver","java.sql.SQLException","XPathException","valid ldap","javax.naming.NameNotFoundException"]
-	for wrd in listWords:
-		if output.count(wrd) > 0:
-			return True
-	return False
-
-
-def generateOutput(url, gParam, instance,method,type):
-	astr = "<sql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='SQL Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
-		astr += "\n\t<result>%s</result>" % p
-	astr += "\n</sql>\n"
-	return astr
-
-
-def generateOutputLong(url, urlString ,method,type, allParams = {}):
-	astr = "<sql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='SQL Injection Type'>%s</type>"  % (method,url,type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+urlString)
-		astr += "\n\t<result>%s</result>" % (p)
-	else:
-		astr += "\n\t<parameters>"
-		for k in allParams:
-			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
-		astr += "\n\t</parameters>"
-	astr += "\n</sql>\n"
-	return astr
-
-
-def permutations(L):
-	if len(L) == 1:
-		yield [L[0]]
-	elif len(L) >= 2:
-		(a, b) = (L[0:1], L[1:])
-		for p in permutations(b):
-			for i in range(len(p)+1):
-				yield b[:i] + a + b[i:]
-
-
-def process(url, database, attack_list):
-	plop = open('results/sql_GrabberAttacks.xml','w')
-	plop.write("<sqlAttacks>\n")
-
-	for u in database.keys():
-		if len(database[u]['GET']):
-			print "Method = GET ", u
-			for gParam in database[u]['GET']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						handle = getContent_GET(u,gParam,instance)
-						if handle != None:
-							output = handle.read()
-							header = handle.info()
-							if detect_sql(output):
-								# generate the info...
-								plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection))
-			# see the permutations
-			if len(database[u]['GET'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						url = ""
-						for gParam in database[u]['GET']:
-							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
-						handle = getContentDirectURL_GET(u,url)
-						if handle != None:
-							output = handle.read()
-							if detect_sql(output):
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
-		if len(database[u]['POST']):
-			print "Method = POST ", u
-			for gParam in database[u]['POST']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						handle = getContent_POST(u,gParam,instance)
-						if handle != None:
-							output = handle.read()
-							header = handle.info()
-							if detect_sql(output):
-								# generate the info...
-								plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection))
-			# see the permutations
-			if len(database[u]['POST'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						allParams = {}
-						for gParam in database[u]['POST']:
-							allParams[gParam] = str(instance)
-						handle = getContentDirectURL_POST(u,allParams)
-						if handle != None:
-							output = handle.read()
-							if detect_sql(output):
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"POST",typeOfInjection, allParams))
-	plop.write("\n</sqlAttacks>\n")
-	plop.close()
-	return ""
+#!/usr/bin/env python
+"""
+	SQL Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys
+from grabber import getContent_POST, getContent_GET
+from grabber import getContentDirectURL_GET, getContentDirectURL_POST
+from grabber import single_urlencode
+from report import appendToReport
+
+def detect_sql(output, url_get = "http://localhost/?param=false"):
+	listWords = ["query","Query", "at line", "SQL syntax", "syntax","valid MySQL","ODBC Microsoft Access Driver","java.sql.SQLException","XPathException","valid ldap","javax.naming.NameNotFoundException", "SQLite3"]
+	for wrd in listWords:
+		if output.count(wrd) > 0:
+			print wrd
+			return True
+	return False
+
+
+def generateOutput(url, gParam, instance,method,type):
+	astr = "<sql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='SQL Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
+		astr += "\n\t<result>%s</result>" % p
+	astr += "\n</sql>\n"
+	return astr
+
+
+def generateOutputLong(url, urlString ,method,type, allParams = {}):
+	astr = "<sql>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='SQL Injection Type'>%s</type>"  % (method,url,type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+urlString)
+		astr += "\n\t<result>%s</result>" % (p)
+	else:
+		astr += "\n\t<parameters>"
+		for k in allParams:
+			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
+		astr += "\n\t</parameters>"
+	astr += "\n</sql>\n"
+	return astr
+
+def generateHTMLOutput(url, gParam, instance, method, typeofInjection):
+	message = "<p class='well'><strong>"+ method +"</strong> <i>"+ url +"</i> <br/>"
+	message += "Type: <strong>"+ typeofInjection +  "</strong> <br/>"
+	message += "Parameter: <strong>"+ gParam + "</strong><br/>  Value: <strong>"+ instance +  "</strong> <br/></p>"
+	# message += "Parameters"+ gParam +"<br/><br/>";
+	return message
+
+def permutations(L):
+	if len(L) == 1:
+		yield [L[0]]
+	elif len(L) >= 2:
+		(a, b) = (L[0:1], L[1:])
+		for p in permutations(b):
+			for i in range(len(p)+1):
+				yield b[:i] + a + b[i:]
+
+
+def process(url, database, attack_list, txheaders):
+	appendToReport(url, "<div class='panel panel-info'><div class='panel-heading'><h3 class='panel-title'> <a data-toggle='collapse' data-target='#collapseSql' href='#collapseSql'>SQL Injection Attacks </a></h3></div>")
+	plop = open('results/sql_GrabberAttacks.xml','w')
+	plop.write("<sqlAttacks>\n")
+	
+	appendToReport(url, '<div id="collapseSql" class="panel-collapse collapse in"><div class="panel-body">');
+	for u in database.keys():
+		appendToReport(u, "<h4><div class='label label-default'><a target='_balnk' href='"+ u +"'>"+ u +"</a></div></h4>")
+		if len(database[u]['GET']):
+			print "Method = GET ", u
+			for gParam in database[u]['GET']:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						handle = getContent_GET(u,gParam,instance, txheaders)
+						if handle != None:
+							output = handle.read()
+							header = handle.info()
+							if detect_sql(output):
+								# generate the info...
+								plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection))
+								appendToReport(u, generateHTMLOutput(u, gParam, instance, "GET", typeOfInjection))
+		#see the permutations
+		if len(database[u]['GET'].keys()) > 1:
+			for typeOfInjection in attack_list:
+				for instance in attack_list[typeOfInjection]:
+					url = ""
+					for gParam in database[u]['GET']:
+						url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
+					handle = getContentDirectURL_GET(u,url,txheaders)
+					if handle != None:
+						output = handle.read()
+						if detect_sql(output):
+							# generate the info...
+							plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
+							appendToReport(u, generateHTMLOutput(u, url, "GET", typeOfInjection))
+		if len(database[u]['POST']):
+			print "Method = POST ", u
+			for gParam in database[u]['POST']:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						allParams = {}
+						for param in database[u]['POST']:
+							if param != gParam:
+								allParams[param] = 'abc'
+						allParams[gParam] =  str(instance)
+						handle = getContentDirectURL_POST(u,allParams, txheaders)
+						if handle != None:
+							output = handle.read()
+							header = handle.info()
+							if detect_sql(output):
+								# generate the info...
+								plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection))
+								appendToReport(u, generateHTMLOutput(u, gParam, instance, "POST", typeOfInjection))
+		# see the permutations
+		if len(database[u]['POST'].keys()) > 1:
+			for typeOfInjection in attack_list:
+				for instance in attack_list[typeOfInjection]:
+					allParams = {}
+					print "Attack: "+ instance
+					for gParam in database[u]['POST']:
+						allParams[gParam] = str(instance)
+					handle = getContentDirectURL_POST(u,allParams, txheaders)
+					if handle != None:
+						output = handle.read()
+						if detect_sql(output):
+							# generate the info...
+							plop.write(generateOutputLong(u,url,"POST",typeOfInjection, allParams))
+							appendToReport(u, generateHTMLOutput(u, "All", instance, "POST", typeOfInjection))
+	plop.write("\n</sqlAttacks>\n")
+	appendToReport(url, "</div></div>")
+	plop.close()
+	return ""
diff -rupN grabber-original/sqlAttacks.xml grabber-new/sqlAttacks.xml
--- grabber-original/sqlAttacks.xml	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/sqlAttacks.xml	2015-05-06 16:35:54.000000000 -0700
@@ -1,44 +1,54 @@
-<?xml version="1.0"?>
-<!-- Some of theses SQL Injection are from ha.ckers.org web security lab -->
-<sql>
-	<attack>
-		<name>Basic SQL Injection</name>
-		<code>1&apos;GRABBER_SQL_INJECTION</code>
-		<label>Basic SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Logical OR SQL Injection</name>
-		<code>1&apos; OR 1 OR 1=&apos;</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Logical OR SQL Injection</name>
-		<code>OR 1=1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Logical OR SQL Injection</name>
-		<code>"OR &quot;1&quot;=&quot;1</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Logical SQL Injection</name>
-		<code>1&apos; OR 1 OR 1=&apos;</code>
-		<label>Logicial SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Comment SQL Injection</name>
-		<code>&apos;;--</code>
-		<label>Command SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Escape SQL Injection</name>
-		<code>\&apos;; GRABBER_SQL_STATEMENT; --</code>
-		<label>Escape SQL Injection Attacks</label>
-	</attack>
-	<attack>
-		<name>Evasion SQL Injection</name>
-		<code>1 UNI/**/ON SELECT ALL FROM WHERE</code>
-		<label>Evasion SQL Injection Attacks</label>
-	</attack>
+<?xml version="1.0"?>
+<!-- Some of theses SQL Injection are from ha.ckers.org web security lab -->
+<sql>
+	<attack>
+		<name>Simple SQL Injection</name>
+		<code>&apos;</code>
+		<label>Simple SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Simple SQL Injection</name>
+		<code>&apos; OR 1 = 1 -- </code>
+		<label>Simple SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Basic SQL Injection</name>
+		<code>1&apos;GRABBER_SQL_INJECTION</code>
+		<label>Basic SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Logical OR SQL Injection</name>
+		<code>1&apos; OR 1 OR 1=&apos;</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Logical OR SQL Injection</name>
+		<code>OR 1=1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Logical OR SQL Injection</name>
+		<code>"OR &quot;1&quot;=&quot;1</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Logical SQL Injection</name>
+		<code>1&apos; OR 1 OR 1=&apos;</code>
+		<label>Logicial SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Comment SQL Injection</name>
+		<code>&apos;;--</code>
+		<label>Command SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Escape SQL Injection</name>
+		<code>\&apos;; GRABBER_SQL_STATEMENT; --</code>
+		<label>Escape SQL Injection Attacks</label>
+	</attack>
+	<attack>
+		<name>Evasion SQL Injection</name>
+		<code>1 UNI/**/ON SELECT ALL FROM WHERE</code>
+		<label>Evasion SQL Injection Attacks</label>
+	</attack>
 </sql>
\ No newline at end of file
diff -rupN grabber-original/xss.py grabber-new/xss.py
--- grabber-original/xss.py	2012-03-24 14:34:28.000000000 -0700
+++ grabber-new/xss.py	2015-05-06 16:35:54.000000000 -0700
@@ -1,111 +1,128 @@
-#!/usr/bin/env python
-"""
-	Cross-Site Scripting Module for Grabber v0.1
-	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
-"""
-import sys
-from grabber import getContent_POST, getContent_GET
-from grabber import getContentDirectURL_GET, getContentDirectURL_POST
-
-from grabber import single_urlencode, partially_in, unescape
-
-def detect_xss(instance, output):
-	if unescape(instance) in output:
-		return True
-	elif partially_in(unescape(instance), output):
-		return True
-	return False
-
-def generateOutput(url, gParam, instance,method,type):
-	astr = "<xss>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='XSS Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
-		astr += "\n\t<result>%s</result>" % p
-	astr += "\n</xss>\n"
-	return astr
-
-def generateOutputLong(url, urlString ,method,type, allParams = {}):
-	astr = "<xss>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='XSS Injection Type'>%s</type>"  % (method,url,type)
-	if method in ("get","GET"):
-		# print the real URL
-		p = (url+"?"+urlString)
-		astr += "\n\t<result>%s</result>" % (p)
-	else:
-		astr += "\n\t<parameters>"
-		for k in allParams:
-			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
-		astr += "\n\t</parameters>"
-	astr += "\n</xss>\n"
-	return astr
-
-
-def permutations(L):
-	if len(L) == 1:
-		yield [L[0]]
-	elif len(L) >= 2:
-		(a, b) = (L[0:1], L[1:])
-		for p in permutations(b):
-			for i in range(len(p)+1):
-				yield b[:i] + a + b[i:]
-
-def process(urlGlobal, database, attack_list):
-	plop = open('results/xss_GrabberAttacks.xml','w')
-	plop.write("<xssAttacks>\n")
-
-	for u in database.keys():
-		if len(database[u]['GET']):
-			print "Method = GET ", u
-			for gParam in database[u]['GET']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						if instance != "See Below":
-							handle = getContent_GET(u,gParam,instance)
-							if handle != None:
-								output = handle.read()
-								header = handle.info()
-								if detect_xss(str(instance),output):
-									# generate the info...
-									plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection))
-			# see the permutations
-			if len(database[u]['GET'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						url = ""
-						for gParam in database[u]['GET']:
-							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
-						handle = getContentDirectURL_GET(u,url)
-						if handle != None:
-							output = handle.read()
-							if detect_xss(str(instance),output):
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
-		if len(database[u]['POST']):
-			print "Method = POST ", u
-			for gParam in database[u]['POST']:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						if instance != "See Below":
-							handle = getContent_POST(u,gParam,instance)
-							if handle != None:
-								output = handle.read()
-								header = handle.info()
-								if detect_xss(str(instance),output):
-									# generate the info...
-									plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection))
-			# see the permutations
-			if len(database[u]['POST'].keys()) > 1:
-				for typeOfInjection in attack_list:
-					for instance in attack_list[typeOfInjection]:
-						allParams = {}
-						for gParam in database[u]['POST']:
-							allParams[gParam] = str(instance)
-						handle = getContentDirectURL_POST(u,allParams)
-						if handle != None:
-							output = handle.read()
-							if detect_xss(str(instance), output):
-								# generate the info...
-								plop.write(generateOutputLong(u,url,"POST",typeOfInjection, allParams))
-	plop.write("\n</xssAttacks>\n")	
-	plop.close()
+#!/usr/bin/env python
+"""
+	Cross-Site Scripting Module for Grabber v0.1
+	Copyright (C) 2006 - Romain Gaucher - http://rgaucher.info
+"""
+import sys
+from grabber import getContent_POST, getContent_GET
+from grabber import getContentDirectURL_GET, getContentDirectURL_POST
+
+from grabber import single_urlencode, partially_in, unescape, escape
+from report import appendToReport
+
+def detect_xss(instance, output):
+	if unescape(instance) in output:
+		return True
+	return False
+
+def generateOutput(url, gParam, instance,method,type):
+	astr = "<xss>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<parameter name='%s'>%s</parameter>\n\t<type name='XSS Injection Type'>%s</type>"  % (method,url,gParam,str(instance),type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+gParam+"="+single_urlencode(str(instance)))
+		astr += "\n\t<result>%s</result>" % p
+	astr += "\n</xss>\n"
+	return astr
+
+def generateOutputLong(url, urlString ,method,type, allParams = {}):
+	astr = "<xss>\n\t<method>%s</method>\n\t<url>%s</url>\n\t<type name='XSS Injection Type'>%s</type>"  % (method,url,type)
+	if method in ("get","GET"):
+		# print the real URL
+		p = (url+"?"+urlString)
+		astr += "\n\t<result>%s</result>" % (p)
+	else:
+		astr += "\n\t<parameters>"
+		for k in allParams:
+			astr += "\n\t\t<parameter name='%s'>%s</parameter>" % (k, allParams[k])
+		astr += "\n\t</parameters>"
+	astr += "\n</xss>\n"
+	return astr
+
+def generateHTMLOutput(url, gParam, instance, method, typeofInjection):
+	message = "<p class='well'><strong>"+ method +"</strong> "+ url +" <br/>"
+	message += "Type: <strong>"+ typeofInjection +  "</strong> <br/>"
+	message += "Parameter: <strong>"+ gParam + "</strong><br/>  Value: <strong>"+ escape(instance) +  "</strong> <br/></p>"
+	# message += "Parameters"+ gParam +"<br/><br/>";
+	return message
+
+def permutations(L):
+	if len(L) == 1:
+		yield [L[0]]
+	elif len(L) >= 2:
+		(a, b) = (L[0:1], L[1:])
+		for p in permutations(b):
+			for i in range(len(p)+1):
+				yield b[:i] + a + b[i:]
+
+def process(urlGlobal, database, attack_list ,txheaders):
+	appendToReport(urlGlobal, "<div class='panel panel-info'><div class='panel-heading'><h3 class='panel-title'> <a data-toggle='collapse' data-target='#collapseXss' href='#collapseXss'>XSS Attacks </a></h3></div>")
+	plop = open('results/xss_GrabberAttacks.xml','w')
+	plop.write("<xssAttacks>\n")
+	appendToReport(urlGlobal, '<div id="collapseXss" class="panel-collapse collapse in"><div class="panel-body">')
+
+	for u in database.keys():
+		appendToReport(u, "<h4><div class='label label-default'><a target='_balnk' href='"+ u +"'>"+ u +"</a></div></h4>")
+		if len(database[u]['GET']):
+			print "Method = GET ", u
+			for gParam in database[u]['GET']:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						if instance != "See Below":
+							handle = getContent_GET(u,gParam,instance, txheaders)
+							if handle != None:
+								output = handle.read()
+								header = handle.info()
+								if detect_xss(str(instance),output):
+									# generate the info...
+									plop.write(generateOutput(u,gParam,instance,"GET",typeOfInjection))
+			# see the permutations
+			if len(database[u]['GET'].keys()) > 1:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						url = ""
+						for gParam in database[u]['GET']:
+							url += ("%s=%s&" % (gParam, single_urlencode(str(instance))))
+						handle = getContentDirectURL_GET(u,url, txheaders)
+						if handle != None:
+							output = handle.read()
+							if detect_xss(str(instance),output):
+								# generate the info...
+								plop.write(generateOutputLong(u,url,"GET",typeOfInjection))
+		if len(database[u]['POST']):
+			print "Method = POST ", u
+			for gParam in database[u]['POST']:
+				if gParam != "None":
+					for typeOfInjection in attack_list:
+						for instance in attack_list[typeOfInjection]:
+							allParams = {}
+							for param in database[u]['POST']:
+								if param != gParam:
+									allParams[param] = 'abc'
+							allParams[gParam] = str(instance)
+							if instance != "See Below":
+								handle = getContentDirectURL_POST(u,allParams, txheaders)
+								if handle != None:
+									output = handle.read()
+									header = handle.info()
+									if detect_xss(str(instance),output):
+										# generate the info...
+										plop.write(generateOutput(u,gParam,instance,"POST",typeOfInjection))
+										appendToReport(u, generateHTMLOutput(u, gParam, instance, "POST", typeOfInjection))
+			# see the permutations
+			if len(database[u]['POST'].keys()) > 1:
+				for typeOfInjection in attack_list:
+					for instance in attack_list[typeOfInjection]:
+						allParams = {}
+						for gParam in database[u]['POST']:
+							allParams[gParam] = str(instance)
+						handle = getContentDirectURL_POST(u,allParams, txheaders)
+						if handle != None:
+							output = handle.read()
+							if detect_xss(str(instance), output):
+								# generate the info...
+								plop.write(generateOutputLong(u,gParam,"POST",typeOfInjection, allParams))
+								appendToReport(u, generateHTMLOutput(u, "ALL", instance, "POST", typeOfInjection))
+	plop.write("\n</xssAttacks>\n")	
+	plop.close()
+	appendToReport(urlGlobal, "</div></div>")
 	return ""
\ No newline at end of file
